{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/user/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd83f66-0acd-4695-9851-37fbc36acbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_task_dict = {\n",
    "    'c02': ['1S', '1pS'],\n",
    "    'c03': ['1S', '1SMe', '3SMe'],\n",
    "    'c04': ['1S', '1SAc', '3SAc'],\n",
    "    'c05': ['1S', '1Soct', '3Soct'],\n",
    "    'c06': ['1S', '1pS', '1SMe', '1SAc', '1Soct'],\n",
    "    'c07': ['1S', '3SMe', '3SAc', '3Soct'],\n",
    "    'c08': ['1I', '1L'],\n",
    "    'c09': ['3I', '3L'],\n",
    "    'c10': ['3dI', '3dL'],\n",
    "    'c11': ['3I', '3dI'],\n",
    "    'c12': ['3L', '3dL'],\n",
    "    'c15': ['1D', '1D02'],\n",
    "    'c16': ['1R', '1R02'],\n",
    "    'c17': ['1Y', '1Y02'],\n",
    "    'c18': ['1W', '1W02'],\n",
    "}\n",
    "\n",
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df\n",
    "\n",
    "def train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name='clean_data', train_sample_size=14000):\n",
    "    # 读取pkl文件，生成readid，X，y组成的df\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=42)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=42, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=3000, random_state=42)\n",
    "\n",
    "    # 通过data_df构建dataloader\n",
    "    batch_size = 64\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    nps.ml.seed_everything(42)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(all_peps), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    # pred\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    test_all_reads_s = pred_df['true'].value_counts()\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=all_peps)\n",
    "    cm_df.to_csv(f\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/clean/{train_name}_cm.csv\")\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/clean/{train_name}_pred_proba.csv\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.4862 train_acc: 0.8883 val_loss: 0.3389 val_acc: 0.9123 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3148 train_acc: 0.9289 val_loss: 0.3296 val_acc: 0.9155 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.2974 train_acc: 0.9408 val_loss: 0.2990 val_acc: 0.9396 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.2930 train_acc: 0.9424 val_loss: 0.3761 val_acc: 0.8798 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.2859 train_acc: 0.9471 val_loss: 0.3306 val_acc: 0.9171 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.2840 train_acc: 0.9488 val_loss: 0.3012 val_acc: 0.9407 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.2825 train_acc: 0.9483 val_loss: 0.3263 val_acc: 0.9199 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.2781 train_acc: 0.9496 val_loss: 0.3597 val_acc: 0.8992 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.2745 train_acc: 0.9533 val_loss: 0.2796 val_acc: 0.9480 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.2703 train_acc: 0.9551 val_loss: 0.3929 val_acc: 0.8745 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.2691 train_acc: 0.9559 val_loss: 0.2766 val_acc: 0.9506 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.2660 train_acc: 0.9582 val_loss: 0.3595 val_acc: 0.8993 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.2669 train_acc: 0.9568 val_loss: 0.7902 val_acc: 0.6101 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.2663 train_acc: 0.9588 val_loss: 0.3173 val_acc: 0.9279 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.2611 train_acc: 0.9610 val_loss: 0.2916 val_acc: 0.9447 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2509 train_acc: 0.9666 val_loss: 0.2749 val_acc: 0.9515 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.2466 train_acc: 0.9688 val_loss: 0.2804 val_acc: 0.9507 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.2436 train_acc: 0.9725 val_loss: 0.2792 val_acc: 0.9530 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.2422 train_acc: 0.9736 val_loss: 0.2798 val_acc: 0.9492 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.2396 train_acc: 0.9750 val_loss: 0.2792 val_acc: 0.9549 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.2372 train_acc: 0.9774 val_loss: 0.2708 val_acc: 0.9544 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.2364 train_acc: 0.9775 val_loss: 0.3266 val_acc: 0.9245 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.2327 train_acc: 0.9805 val_loss: 0.3348 val_acc: 0.9190 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.2317 train_acc: 0.9809 val_loss: 0.4126 val_acc: 0.8753 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.2218 train_acc: 0.9879 val_loss: 0.2795 val_acc: 0.9551 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.2176 train_acc: 0.9903 val_loss: 0.2693 val_acc: 0.9607 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.2149 train_acc: 0.9925 val_loss: 0.2726 val_acc: 0.9560 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.2125 train_acc: 0.9940 val_loss: 0.2764 val_acc: 0.9588 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.2104 train_acc: 0.9951 val_loss: 0.2828 val_acc: 0.9520 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.2095 train_acc: 0.9958 val_loss: 0.2808 val_acc: 0.9546 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.2064 train_acc: 0.9978 val_loss: 0.2768 val_acc: 0.9576 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.2053 train_acc: 0.9982 val_loss: 0.2767 val_acc: 0.9563 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.2043 train_acc: 0.9986 val_loss: 0.2815 val_acc: 0.9574 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.2036 train_acc: 0.9988 val_loss: 0.2767 val_acc: 0.9591 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2026 train_acc: 0.9995 val_loss: 0.2742 val_acc: 0.9597 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2026 train_acc: 0.9991 val_loss: 0.2750 val_acc: 0.9598 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2025 train_acc: 0.9991 val_loss: 0.2757 val_acc: 0.9584 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2022 train_acc: 0.9993 val_loss: 0.2755 val_acc: 0.9602 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2018 train_acc: 0.9995 val_loss: 0.2766 val_acc: 0.9588 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2017 train_acc: 0.9995 val_loss: 0.2756 val_acc: 0.9588 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2017 train_acc: 0.9996 val_loss: 0.2764 val_acc: 0.9584 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2016 train_acc: 0.9995 val_loss: 0.2770 val_acc: 0.9587 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2013 train_acc: 0.9997 val_loss: 0.2751 val_acc: 0.9595 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.2014 train_acc: 0.9996 val_loss: 0.2755 val_acc: 0.9599 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.2013 train_acc: 0.9996 val_loss: 0.2748 val_acc: 0.9602 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.2012 train_acc: 0.9996 val_loss: 0.2748 val_acc: 0.9598 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2011 train_acc: 0.9998 val_loss: 0.2753 val_acc: 0.9610 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2011 train_acc: 0.9996 val_loss: 0.2750 val_acc: 0.9605 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2009 train_acc: 0.9998 val_loss: 0.2751 val_acc: 0.9591 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2012 train_acc: 0.9996 val_loss: 0.2762 val_acc: 0.9590 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2010 train_acc: 0.9997 val_loss: 0.2761 val_acc: 0.9605 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2010 train_acc: 0.9996 val_loss: 0.2759 val_acc: 0.9603 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2009 train_acc: 0.9997 val_loss: 0.2759 val_acc: 0.9593 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2009 train_acc: 0.9997 val_loss: 0.2752 val_acc: 0.9607 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2010 train_acc: 0.9998 val_loss: 0.2756 val_acc: 0.9596 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2010 train_acc: 0.9997 val_loss: 0.2752 val_acc: 0.9597 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2010 train_acc: 0.9997 val_loss: 0.2748 val_acc: 0.9602 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2009 train_acc: 0.9998 val_loss: 0.2762 val_acc: 0.9584 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2007 train_acc: 0.9997 val_loss: 0.2756 val_acc: 0.9603 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2007 train_acc: 0.9998 val_loss: 0.2769 val_acc: 0.9608 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2007 train_acc: 0.9997 val_loss: 0.2757 val_acc: 0.9587 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2006 train_acc: 0.9999 val_loss: 0.2758 val_acc: 0.9598 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2007 train_acc: 0.9998 val_loss: 0.2765 val_acc: 0.9595 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2007 train_acc: 0.9998 val_loss: 0.2765 val_acc: 0.9597 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2007 train_acc: 0.9999 val_loss: 0.2755 val_acc: 0.9601 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2007 train_acc: 0.9998 val_loss: 0.2761 val_acc: 0.9585 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2008 train_acc: 0.9997 val_loss: 0.2761 val_acc: 0.9580 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2007 train_acc: 0.9999 val_loss: 0.2767 val_acc: 0.9591 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.2007 train_acc: 0.9997 val_loss: 0.2756 val_acc: 0.9601 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2005 train_acc: 0.9999 val_loss: 0.2757 val_acc: 0.9594 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2005 train_acc: 0.9999 val_loss: 0.2760 val_acc: 0.9588 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2004 train_acc: 0.9999 val_loss: 0.2775 val_acc: 0.9575 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2005 train_acc: 0.9998 val_loss: 0.2769 val_acc: 0.9579 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.2004 train_acc: 0.9999 val_loss: 0.2758 val_acc: 0.9592 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.2003 train_acc: 1.0000 val_loss: 0.2767 val_acc: 0.9596 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.2003 train_acc: 1.0000 val_loss: 0.2767 val_acc: 0.9594 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.2004 train_acc: 1.0000 val_loss: 0.2768 val_acc: 0.9587 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9578\n",
      "14000: 0.9578333333333333\n",
      " test_acc: 0.9578\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.9973 train_acc: 0.5691 val_loss: 0.9351 val_acc: 0.5904 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.8783 train_acc: 0.6282 val_loss: 0.9668 val_acc: 0.5721 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8410 train_acc: 0.6557 val_loss: 0.9333 val_acc: 0.5774 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8211 train_acc: 0.6673 val_loss: 0.9859 val_acc: 0.5523 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8064 train_acc: 0.6813 val_loss: 0.8864 val_acc: 0.6220 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7901 train_acc: 0.6936 val_loss: 0.7893 val_acc: 0.6909 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7756 train_acc: 0.7023 val_loss: 0.9482 val_acc: 0.5721 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7624 train_acc: 0.7114 val_loss: 0.7727 val_acc: 0.7044 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7497 train_acc: 0.7193 val_loss: 0.8232 val_acc: 0.6736 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7407 train_acc: 0.7271 val_loss: 0.8354 val_acc: 0.6534 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7268 train_acc: 0.7356 val_loss: 0.8988 val_acc: 0.6122 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7149 train_acc: 0.7441 val_loss: 0.9369 val_acc: 0.5911 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6770 train_acc: 0.7692 val_loss: 0.9733 val_acc: 0.5958 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6603 train_acc: 0.7818 val_loss: 0.7306 val_acc: 0.7336 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6488 train_acc: 0.7875 val_loss: 0.8776 val_acc: 0.6521 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6328 train_acc: 0.7992 val_loss: 0.9704 val_acc: 0.6156 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6173 train_acc: 0.8114 val_loss: 0.8153 val_acc: 0.6847 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6062 train_acc: 0.8183 val_loss: 0.7686 val_acc: 0.7242 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.5626 train_acc: 0.8480 val_loss: 0.7411 val_acc: 0.7336 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5426 train_acc: 0.8576 val_loss: 0.7196 val_acc: 0.7581 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5265 train_acc: 0.8708 val_loss: 0.7740 val_acc: 0.7363 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.5124 train_acc: 0.8781 val_loss: 0.7866 val_acc: 0.7297 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.4994 train_acc: 0.8866 val_loss: 0.7459 val_acc: 0.7543 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.4863 train_acc: 0.8993 val_loss: 0.8568 val_acc: 0.7043 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.4603 train_acc: 0.9149 val_loss: 0.7755 val_acc: 0.7496 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.4479 train_acc: 0.9220 val_loss: 0.7522 val_acc: 0.7545 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.4402 train_acc: 0.9270 val_loss: 0.7520 val_acc: 0.7538 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.4334 train_acc: 0.9326 val_loss: 0.7554 val_acc: 0.7521 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.4211 train_acc: 0.9413 val_loss: 0.7495 val_acc: 0.7604 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.4125 train_acc: 0.9469 val_loss: 0.7675 val_acc: 0.7579 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.4113 train_acc: 0.9483 val_loss: 0.7634 val_acc: 0.7502 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.4064 train_acc: 0.9510 val_loss: 0.7624 val_acc: 0.7536 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.4013 train_acc: 0.9550 val_loss: 0.7601 val_acc: 0.7562 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3937 train_acc: 0.9591 val_loss: 0.7591 val_acc: 0.7615 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3929 train_acc: 0.9598 val_loss: 0.7729 val_acc: 0.7553 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.3915 train_acc: 0.9594 val_loss: 0.7616 val_acc: 0.7492 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.3905 train_acc: 0.9601 val_loss: 0.7738 val_acc: 0.7555 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.3881 train_acc: 0.9616 val_loss: 0.7577 val_acc: 0.7579 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3836 train_acc: 0.9648 val_loss: 0.7597 val_acc: 0.7637 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.3843 train_acc: 0.9651 val_loss: 0.7605 val_acc: 0.7560 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.3807 train_acc: 0.9666 val_loss: 0.7597 val_acc: 0.7585 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.3820 train_acc: 0.9665 val_loss: 0.7635 val_acc: 0.7491 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.3798 train_acc: 0.9666 val_loss: 0.7596 val_acc: 0.7590 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3804 train_acc: 0.9671 val_loss: 0.7580 val_acc: 0.7551 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3803 train_acc: 0.9673 val_loss: 0.7633 val_acc: 0.7553 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3801 train_acc: 0.9680 val_loss: 0.7642 val_acc: 0.7553 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3782 train_acc: 0.9681 val_loss: 0.7632 val_acc: 0.7564 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3761 train_acc: 0.9702 val_loss: 0.7617 val_acc: 0.7572 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3764 train_acc: 0.9706 val_loss: 0.7775 val_acc: 0.7502 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3765 train_acc: 0.9696 val_loss: 0.7779 val_acc: 0.7541 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3744 train_acc: 0.9699 val_loss: 0.7667 val_acc: 0.7494 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3746 train_acc: 0.9705 val_loss: 0.7666 val_acc: 0.7573 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3737 train_acc: 0.9712 val_loss: 0.7605 val_acc: 0.7592 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3749 train_acc: 0.9702 val_loss: 0.7642 val_acc: 0.7583 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3756 train_acc: 0.9705 val_loss: 0.7786 val_acc: 0.7472 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3725 train_acc: 0.9723 val_loss: 0.7678 val_acc: 0.7551 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3713 train_acc: 0.9726 val_loss: 0.7659 val_acc: 0.7587 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3722 train_acc: 0.9728 val_loss: 0.7844 val_acc: 0.7496 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3717 train_acc: 0.9720 val_loss: 0.7699 val_acc: 0.7605 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3710 train_acc: 0.9728 val_loss: 0.7654 val_acc: 0.7581 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3723 train_acc: 0.9722 val_loss: 0.7788 val_acc: 0.7444 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3723 train_acc: 0.9724 val_loss: 0.7705 val_acc: 0.7549 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3693 train_acc: 0.9739 val_loss: 0.7746 val_acc: 0.7528 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3696 train_acc: 0.9727 val_loss: 0.7646 val_acc: 0.7564 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3686 train_acc: 0.9744 val_loss: 0.7692 val_acc: 0.7508 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3688 train_acc: 0.9738 val_loss: 0.7871 val_acc: 0.7536 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3669 train_acc: 0.9753 val_loss: 0.7861 val_acc: 0.7442 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3647 train_acc: 0.9768 val_loss: 0.7635 val_acc: 0.7562 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3658 train_acc: 0.9758 val_loss: 0.7733 val_acc: 0.7536 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7573\n",
      "14000: 0.7573333333333333\n",
      " test_acc: 0.7573\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.9635 train_acc: 0.5924 val_loss: 1.1942 val_acc: 0.4586 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.8091 train_acc: 0.6746 val_loss: 0.8167 val_acc: 0.6532 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.7704 train_acc: 0.7062 val_loss: 0.8998 val_acc: 0.5990 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.7465 train_acc: 0.7197 val_loss: 1.2667 val_acc: 0.4187 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.7301 train_acc: 0.7336 val_loss: 0.9293 val_acc: 0.5943 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7126 train_acc: 0.7429 val_loss: 1.6035 val_acc: 0.5041 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.6806 train_acc: 0.7635 val_loss: 0.7035 val_acc: 0.7536 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.6678 train_acc: 0.7750 val_loss: 0.8917 val_acc: 0.6316 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.6576 train_acc: 0.7812 val_loss: 0.7605 val_acc: 0.7061 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.6491 train_acc: 0.7883 val_loss: 1.2873 val_acc: 0.5744 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.6418 train_acc: 0.7898 val_loss: 0.6537 val_acc: 0.7811 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.6333 train_acc: 0.7950 val_loss: 0.7499 val_acc: 0.7193 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6197 train_acc: 0.8063 val_loss: 0.9656 val_acc: 0.6137 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6103 train_acc: 0.8123 val_loss: 0.7125 val_acc: 0.7477 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.5991 train_acc: 0.8215 val_loss: 0.9822 val_acc: 0.6212 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.5623 train_acc: 0.8453 val_loss: 0.6667 val_acc: 0.7854 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.5505 train_acc: 0.8512 val_loss: 0.8791 val_acc: 0.6807 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.5346 train_acc: 0.8618 val_loss: 0.8080 val_acc: 0.7084 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.5227 train_acc: 0.8703 val_loss: 0.9495 val_acc: 0.6583 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5089 train_acc: 0.8814 val_loss: 0.6367 val_acc: 0.8044 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.4959 train_acc: 0.8898 val_loss: 0.6962 val_acc: 0.7694 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.4862 train_acc: 0.8974 val_loss: 0.7859 val_acc: 0.7357 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.4737 train_acc: 0.9070 val_loss: 0.6536 val_acc: 0.7989 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.4637 train_acc: 0.9112 val_loss: 1.0027 val_acc: 0.6318 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.4402 train_acc: 0.9281 val_loss: 0.8436 val_acc: 0.7180 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.4286 train_acc: 0.9355 val_loss: 0.6705 val_acc: 0.7954 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.4212 train_acc: 0.9406 val_loss: 0.7151 val_acc: 0.7767 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.4111 train_acc: 0.9470 val_loss: 0.6999 val_acc: 0.7897 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.4004 train_acc: 0.9538 val_loss: 0.6644 val_acc: 0.8074 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.3973 train_acc: 0.9565 val_loss: 0.6679 val_acc: 0.8012 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.3915 train_acc: 0.9602 val_loss: 0.7314 val_acc: 0.7803 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.3894 train_acc: 0.9615 val_loss: 0.6692 val_acc: 0.7984 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.3842 train_acc: 0.9653 val_loss: 0.7057 val_acc: 0.7792 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3799 train_acc: 0.9679 val_loss: 0.6729 val_acc: 0.8035 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3786 train_acc: 0.9686 val_loss: 0.7063 val_acc: 0.7850 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.3738 train_acc: 0.9719 val_loss: 0.6944 val_acc: 0.7886 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.3745 train_acc: 0.9714 val_loss: 0.6701 val_acc: 0.8074 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.3719 train_acc: 0.9726 val_loss: 0.6728 val_acc: 0.8065 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3706 train_acc: 0.9730 val_loss: 0.6697 val_acc: 0.8020 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.3671 train_acc: 0.9752 val_loss: 0.6688 val_acc: 0.8048 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.3676 train_acc: 0.9744 val_loss: 0.6767 val_acc: 0.7967 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3675 train_acc: 0.9757 val_loss: 0.6717 val_acc: 0.8008 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3691 train_acc: 0.9740 val_loss: 0.6687 val_acc: 0.8078 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3669 train_acc: 0.9762 val_loss: 0.6698 val_acc: 0.8037 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3669 train_acc: 0.9759 val_loss: 0.6774 val_acc: 0.7965 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3659 train_acc: 0.9773 val_loss: 0.6665 val_acc: 0.8059 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3663 train_acc: 0.9758 val_loss: 0.6708 val_acc: 0.8001 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3650 train_acc: 0.9774 val_loss: 0.6679 val_acc: 0.8035 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3643 train_acc: 0.9774 val_loss: 0.6718 val_acc: 0.7999 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3648 train_acc: 0.9776 val_loss: 0.6752 val_acc: 0.8006 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3638 train_acc: 0.9778 val_loss: 0.6692 val_acc: 0.8042 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3631 train_acc: 0.9776 val_loss: 0.6684 val_acc: 0.8055 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3609 train_acc: 0.9795 val_loss: 0.6721 val_acc: 0.8042 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3620 train_acc: 0.9782 val_loss: 0.6772 val_acc: 0.7927 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3615 train_acc: 0.9782 val_loss: 0.6817 val_acc: 0.7995 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3623 train_acc: 0.9779 val_loss: 0.6697 val_acc: 0.7997 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3611 train_acc: 0.9797 val_loss: 0.6690 val_acc: 0.8023 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3585 train_acc: 0.9807 val_loss: 0.6659 val_acc: 0.8057 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3575 train_acc: 0.9813 val_loss: 0.6724 val_acc: 0.8023 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3591 train_acc: 0.9802 val_loss: 0.6672 val_acc: 0.8038 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3602 train_acc: 0.9792 val_loss: 0.6706 val_acc: 0.8027 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3584 train_acc: 0.9809 val_loss: 0.6717 val_acc: 0.8033 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3581 train_acc: 0.9801 val_loss: 0.6759 val_acc: 0.8001 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3576 train_acc: 0.9814 val_loss: 0.6690 val_acc: 0.8085 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3578 train_acc: 0.9809 val_loss: 0.6676 val_acc: 0.8061 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3567 train_acc: 0.9818 val_loss: 0.6683 val_acc: 0.8067 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3569 train_acc: 0.9812 val_loss: 0.6836 val_acc: 0.8003 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3552 train_acc: 0.9819 val_loss: 0.6692 val_acc: 0.8046 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3579 train_acc: 0.9808 val_loss: 0.6751 val_acc: 0.8010 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.3557 train_acc: 0.9820 val_loss: 0.6711 val_acc: 0.8038 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.3546 train_acc: 0.9825 val_loss: 0.6782 val_acc: 0.7971 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.3563 train_acc: 0.9806 val_loss: 0.6760 val_acc: 0.7984 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.3534 train_acc: 0.9831 val_loss: 0.6738 val_acc: 0.7971 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.3529 train_acc: 0.9838 val_loss: 0.6700 val_acc: 0.8053 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.3536 train_acc: 0.9837 val_loss: 0.6684 val_acc: 0.8070 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.3526 train_acc: 0.9835 val_loss: 0.6680 val_acc: 0.8067 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.3526 train_acc: 0.9836 val_loss: 0.6679 val_acc: 0.8052 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.3518 train_acc: 0.9839 val_loss: 0.6676 val_acc: 0.8070 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.3527 train_acc: 0.9834 val_loss: 0.6763 val_acc: 0.7991 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.3529 train_acc: 0.9835 val_loss: 0.6731 val_acc: 0.8050 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.3513 train_acc: 0.9847 val_loss: 0.6702 val_acc: 0.8055 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.3512 train_acc: 0.9842 val_loss: 0.6692 val_acc: 0.8089 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.3533 train_acc: 0.9830 val_loss: 0.6794 val_acc: 0.7920 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.3521 train_acc: 0.9837 val_loss: 0.6699 val_acc: 0.8042 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.3499 train_acc: 0.9850 val_loss: 0.6802 val_acc: 0.7997 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.3496 train_acc: 0.9851 val_loss: 0.6898 val_acc: 0.8005 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.3476 train_acc: 0.9867 val_loss: 0.6682 val_acc: 0.8084 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.3500 train_acc: 0.9851 val_loss: 0.6707 val_acc: 0.8027 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.3483 train_acc: 0.9862 val_loss: 0.6772 val_acc: 0.8005 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.3484 train_acc: 0.9853 val_loss: 0.6717 val_acc: 0.8042 lr: 5e-05\n",
      "Epoch  90 / 200 train_loss: 0.3485 train_acc: 0.9857 val_loss: 0.6686 val_acc: 0.8080 lr: 5e-05\n",
      "Epoch  91 / 200 train_loss: 0.3472 train_acc: 0.9864 val_loss: 0.6717 val_acc: 0.8014 lr: 5e-05\n",
      "Epoch  92 / 200 train_loss: 0.3485 train_acc: 0.9859 val_loss: 0.6786 val_acc: 0.7924 lr: 5e-05\n",
      "Epoch  93 / 200 train_loss: 0.3474 train_acc: 0.9866 val_loss: 0.6714 val_acc: 0.8046 lr: 5e-05\n",
      "Epoch  94 / 200 train_loss: 0.3462 train_acc: 0.9877 val_loss: 0.6707 val_acc: 0.8057 lr: 5e-05\n",
      "Epoch  95 / 200 train_loss: 0.3460 train_acc: 0.9871 val_loss: 0.6758 val_acc: 0.8029 lr: 5e-05\n",
      "Epoch  96 / 200 train_loss: 0.3459 train_acc: 0.9872 val_loss: 0.6704 val_acc: 0.8063 lr: 5e-05\n",
      "Epoch  97 / 200 train_loss: 0.3457 train_acc: 0.9878 val_loss: 0.6924 val_acc: 0.7903 lr: 5e-05\n",
      "Epoch  98 / 200 train_loss: 0.3454 train_acc: 0.9877 val_loss: 0.6740 val_acc: 0.8070 lr: 5e-05\n",
      "Epoch  99 / 200 train_loss: 0.3444 train_acc: 0.9879 val_loss: 0.6701 val_acc: 0.8067 lr: 5e-05\n",
      "Epoch 100 / 200 train_loss: 0.3439 train_acc: 0.9881 val_loss: 0.6695 val_acc: 0.8061 lr: 5e-05\n",
      "Epoch 101 / 200 train_loss: 0.3445 train_acc: 0.9883 val_loss: 0.6804 val_acc: 0.8025 lr: 5e-05\n",
      "Epoch 102 / 200 train_loss: 0.3440 train_acc: 0.9879 val_loss: 0.6755 val_acc: 0.8048 lr: 5e-05\n",
      "Epoch 103 / 200 train_loss: 0.3446 train_acc: 0.9876 val_loss: 0.6779 val_acc: 0.7967 lr: 5e-05\n",
      "Epoch 104 / 200 train_loss: 0.3437 train_acc: 0.9882 val_loss: 0.6694 val_acc: 0.8053 lr: 5e-05\n",
      "Epoch 105 / 200 train_loss: 0.3428 train_acc: 0.9883 val_loss: 0.6903 val_acc: 0.7954 lr: 5e-05\n",
      "Epoch 106 / 200 train_loss: 0.3428 train_acc: 0.9883 val_loss: 0.6745 val_acc: 0.8023 lr: 5e-05\n",
      "Epoch 107 / 200 train_loss: 0.3439 train_acc: 0.9883 val_loss: 0.6685 val_acc: 0.8084 lr: 5e-05\n",
      "Epoch 108 / 200 train_loss: 0.3441 train_acc: 0.9879 val_loss: 0.6691 val_acc: 0.8078 lr: 5e-05\n",
      "Epoch 109 / 200 train_loss: 0.3412 train_acc: 0.9890 val_loss: 0.6701 val_acc: 0.8037 lr: 5e-05\n",
      "Epoch 110 / 200 train_loss: 0.3437 train_acc: 0.9880 val_loss: 0.6768 val_acc: 0.7974 lr: 5e-05\n",
      "Epoch 111 / 200 train_loss: 0.3419 train_acc: 0.9889 val_loss: 0.6693 val_acc: 0.8091 lr: 5e-05\n",
      "Epoch 112 / 200 train_loss: 0.3415 train_acc: 0.9895 val_loss: 0.6727 val_acc: 0.8038 lr: 5e-05\n",
      "Epoch 113 / 200 train_loss: 0.3406 train_acc: 0.9901 val_loss: 0.6702 val_acc: 0.8065 lr: 5e-05\n",
      "Epoch 114 / 200 train_loss: 0.3415 train_acc: 0.9888 val_loss: 0.6734 val_acc: 0.8033 lr: 5e-05\n",
      "Epoch 115 / 200 train_loss: 0.3402 train_acc: 0.9900 val_loss: 0.6745 val_acc: 0.8065 lr: 5e-05\n",
      "Epoch 116 / 200 train_loss: 0.3405 train_acc: 0.9894 val_loss: 0.6719 val_acc: 0.8040 lr: 5e-05\n",
      "Epoch 117 / 200 train_loss: 0.3407 train_acc: 0.9891 val_loss: 0.6857 val_acc: 0.7959 lr: 5e-05\n",
      "Epoch 118 / 200 train_loss: 0.3406 train_acc: 0.9896 val_loss: 0.6794 val_acc: 0.7952 lr: 5e-05\n",
      "Epoch 119 / 200 train_loss: 0.3405 train_acc: 0.9898 val_loss: 0.6761 val_acc: 0.8016 lr: 5e-05\n",
      "Epoch 120 / 200 train_loss: 0.3391 train_acc: 0.9903 val_loss: 0.6765 val_acc: 0.8001 lr: 5e-05\n",
      "Epoch 121 / 200 train_loss: 0.3397 train_acc: 0.9899 val_loss: 0.6659 val_acc: 0.8091 lr: 5e-05\n",
      "Epoch 122 / 200 train_loss: 0.3382 train_acc: 0.9910 val_loss: 0.6706 val_acc: 0.8025 lr: 5e-05\n",
      "Epoch 123 / 200 train_loss: 0.3393 train_acc: 0.9902 val_loss: 0.6846 val_acc: 0.8025 lr: 5e-05\n",
      "Epoch 124 / 200 train_loss: 0.3407 train_acc: 0.9890 val_loss: 0.6781 val_acc: 0.8038 lr: 5e-05\n",
      "Epoch 125 / 200 train_loss: 0.3396 train_acc: 0.9906 val_loss: 0.6801 val_acc: 0.8003 lr: 5e-05\n",
      "Epoch 126 / 200 train_loss: 0.3394 train_acc: 0.9901 val_loss: 0.6792 val_acc: 0.7952 lr: 5e-05\n",
      "Epoch 127 / 200 train_loss: 0.3378 train_acc: 0.9912 val_loss: 0.6771 val_acc: 0.7989 lr: 5e-05\n",
      "Epoch 128 / 200 train_loss: 0.3396 train_acc: 0.9898 val_loss: 0.6744 val_acc: 0.8048 lr: 5e-05\n",
      "Epoch 129 / 200 train_loss: 0.3377 train_acc: 0.9909 val_loss: 0.6740 val_acc: 0.8042 lr: 5e-05\n",
      "Epoch 130 / 200 train_loss: 0.3363 train_acc: 0.9917 val_loss: 0.6694 val_acc: 0.8082 lr: 5e-05\n",
      "Epoch 131 / 200 train_loss: 0.3375 train_acc: 0.9915 val_loss: 0.6787 val_acc: 0.8023 lr: 5e-05\n",
      "Epoch 132 / 200 train_loss: 0.3363 train_acc: 0.9910 val_loss: 0.6708 val_acc: 0.8057 lr: 5e-05\n",
      "Epoch 133 / 200 train_loss: 0.3374 train_acc: 0.9913 val_loss: 0.6846 val_acc: 0.8005 lr: 5e-05\n",
      "Epoch 134 / 200 train_loss: 0.3372 train_acc: 0.9907 val_loss: 0.6741 val_acc: 0.8003 lr: 5e-05\n",
      "Epoch 135 / 200 train_loss: 0.3370 train_acc: 0.9912 val_loss: 0.6727 val_acc: 0.8016 lr: 5e-05\n",
      "Epoch 136 / 200 train_loss: 0.3350 train_acc: 0.9918 val_loss: 0.6694 val_acc: 0.8072 lr: 5e-05\n",
      "Epoch 137 / 200 train_loss: 0.3362 train_acc: 0.9919 val_loss: 0.6691 val_acc: 0.8031 lr: 5e-05\n",
      "Epoch 138 / 200 train_loss: 0.3369 train_acc: 0.9905 val_loss: 0.6699 val_acc: 0.8061 lr: 5e-05\n",
      "Epoch 139 / 200 train_loss: 0.3336 train_acc: 0.9930 val_loss: 0.6751 val_acc: 0.8018 lr: 5e-05\n",
      "Epoch 140 / 200 train_loss: 0.3350 train_acc: 0.9924 val_loss: 0.6733 val_acc: 0.8044 lr: 5e-05\n",
      "Epoch 141 / 200 train_loss: 0.3347 train_acc: 0.9921 val_loss: 0.6748 val_acc: 0.8063 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7943\n",
      "14000: 0.7943333333333333\n",
      " test_acc: 0.7943\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7286 train_acc: 0.7811 val_loss: 0.5905 val_acc: 0.8358 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5951 train_acc: 0.8378 val_loss: 0.5565 val_acc: 0.8590 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5646 train_acc: 0.8537 val_loss: 0.8792 val_acc: 0.6301 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5402 train_acc: 0.8681 val_loss: 0.5717 val_acc: 0.8534 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5225 train_acc: 0.8790 val_loss: 0.5290 val_acc: 0.8763 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5098 train_acc: 0.8853 val_loss: 0.5477 val_acc: 0.8690 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4986 train_acc: 0.8910 val_loss: 0.5226 val_acc: 0.8763 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.4934 train_acc: 0.8937 val_loss: 0.5455 val_acc: 0.8573 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4809 train_acc: 0.9004 val_loss: 0.5055 val_acc: 0.8833 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4736 train_acc: 0.9058 val_loss: 0.5079 val_acc: 0.8857 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4662 train_acc: 0.9090 val_loss: 0.5973 val_acc: 0.8353 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.4643 train_acc: 0.9091 val_loss: 0.4865 val_acc: 0.8929 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.4556 train_acc: 0.9139 val_loss: 0.4952 val_acc: 0.8961 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.4471 train_acc: 0.9195 val_loss: 0.4938 val_acc: 0.8942 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.4393 train_acc: 0.9234 val_loss: 0.4726 val_acc: 0.9089 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.4306 train_acc: 0.9265 val_loss: 0.5016 val_acc: 0.8918 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.4267 train_acc: 0.9306 val_loss: 0.5145 val_acc: 0.8814 lr: 0.005\n",
      "Epoch  17 / 200 train_loss: 0.4187 train_acc: 0.9339 val_loss: 0.8663 val_acc: 0.6879 lr: 0.005\n",
      "Epoch  18 / 200 train_loss: 0.4118 train_acc: 0.9372 val_loss: 0.5018 val_acc: 0.8919 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.3833 train_acc: 0.9541 val_loss: 0.4868 val_acc: 0.9004 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.3710 train_acc: 0.9609 val_loss: 0.5941 val_acc: 0.8535 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.3645 train_acc: 0.9656 val_loss: 0.4789 val_acc: 0.9121 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.3562 train_acc: 0.9707 val_loss: 0.5911 val_acc: 0.8594 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.3523 train_acc: 0.9729 val_loss: 0.5519 val_acc: 0.8733 lr: 0.0025\n",
      "Epoch  24 / 200 train_loss: 0.3474 train_acc: 0.9768 val_loss: 0.6476 val_acc: 0.8341 lr: 0.0025\n",
      "Epoch  25 / 200 train_loss: 0.3403 train_acc: 0.9799 val_loss: 0.4921 val_acc: 0.9049 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.3283 train_acc: 0.9873 val_loss: 0.4775 val_acc: 0.9153 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.3242 train_acc: 0.9892 val_loss: 0.8109 val_acc: 0.7594 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.3222 train_acc: 0.9906 val_loss: 0.5040 val_acc: 0.9027 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.3190 train_acc: 0.9920 val_loss: 0.4993 val_acc: 0.9000 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.3164 train_acc: 0.9931 val_loss: 0.4779 val_acc: 0.9164 lr: 0.00125\n",
      "Epoch  31 / 200 train_loss: 0.3160 train_acc: 0.9935 val_loss: 0.5177 val_acc: 0.8963 lr: 0.00125\n",
      "Epoch  32 / 200 train_loss: 0.3139 train_acc: 0.9941 val_loss: 0.5784 val_acc: 0.8622 lr: 0.00125\n",
      "Epoch  33 / 200 train_loss: 0.3115 train_acc: 0.9957 val_loss: 0.4816 val_acc: 0.9128 lr: 0.00125\n",
      "Epoch  34 / 200 train_loss: 0.3116 train_acc: 0.9948 val_loss: 0.6428 val_acc: 0.8308 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.3072 train_acc: 0.9975 val_loss: 0.5040 val_acc: 0.9059 lr: 0.000625\n",
      "Epoch  36 / 200 train_loss: 0.3057 train_acc: 0.9977 val_loss: 0.4752 val_acc: 0.9160 lr: 0.000625\n",
      "Epoch  37 / 200 train_loss: 0.3058 train_acc: 0.9973 val_loss: 0.4734 val_acc: 0.9164 lr: 0.000625\n",
      "Epoch  38 / 200 train_loss: 0.3046 train_acc: 0.9983 val_loss: 0.4761 val_acc: 0.9134 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.3041 train_acc: 0.9979 val_loss: 0.4872 val_acc: 0.9104 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.3033 train_acc: 0.9984 val_loss: 0.4722 val_acc: 0.9177 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.3030 train_acc: 0.9985 val_loss: 0.4743 val_acc: 0.9155 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.3027 train_acc: 0.9985 val_loss: 0.4896 val_acc: 0.9083 lr: 0.0003125\n",
      "Epoch  43 / 200 train_loss: 0.3022 train_acc: 0.9986 val_loss: 0.4728 val_acc: 0.9164 lr: 0.0003125\n",
      "Epoch  44 / 200 train_loss: 0.3020 train_acc: 0.9986 val_loss: 0.4715 val_acc: 0.9175 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.3015 train_acc: 0.9988 val_loss: 0.4753 val_acc: 0.9177 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.3012 train_acc: 0.9988 val_loss: 0.4715 val_acc: 0.9183 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.3008 train_acc: 0.9990 val_loss: 0.4719 val_acc: 0.9166 lr: 0.00015625\n",
      "Epoch  48 / 200 train_loss: 0.3006 train_acc: 0.9991 val_loss: 0.4725 val_acc: 0.9160 lr: 0.00015625\n",
      "Epoch  49 / 200 train_loss: 0.3009 train_acc: 0.9989 val_loss: 0.4723 val_acc: 0.9166 lr: 0.00015625\n",
      "Epoch  50 / 200 train_loss: 0.3006 train_acc: 0.9987 val_loss: 0.4758 val_acc: 0.9130 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.3003 train_acc: 0.9992 val_loss: 0.4738 val_acc: 0.9153 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.3000 train_acc: 0.9990 val_loss: 0.4721 val_acc: 0.9177 lr: 7.8125e-05\n",
      "Epoch  54 / 200 train_loss: 0.2999 train_acc: 0.9994 val_loss: 0.4725 val_acc: 0.9179 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3000 train_acc: 0.9991 val_loss: 0.4732 val_acc: 0.9147 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2997 train_acc: 0.9991 val_loss: 0.4710 val_acc: 0.9175 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2995 train_acc: 0.9993 val_loss: 0.4731 val_acc: 0.9162 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2996 train_acc: 0.9990 val_loss: 0.4709 val_acc: 0.9179 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2993 train_acc: 0.9994 val_loss: 0.4738 val_acc: 0.9162 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2997 train_acc: 0.9993 val_loss: 0.4727 val_acc: 0.9164 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2998 train_acc: 0.9992 val_loss: 0.4815 val_acc: 0.9098 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2996 train_acc: 0.9992 val_loss: 0.4740 val_acc: 0.9168 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2993 train_acc: 0.9993 val_loss: 0.4730 val_acc: 0.9157 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2995 train_acc: 0.9993 val_loss: 0.4727 val_acc: 0.9168 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2994 train_acc: 0.9994 val_loss: 0.4719 val_acc: 0.9181 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2992 train_acc: 0.9993 val_loss: 0.4741 val_acc: 0.9164 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2992 train_acc: 0.9992 val_loss: 0.4911 val_acc: 0.9100 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.2993 train_acc: 0.9993 val_loss: 0.4827 val_acc: 0.9093 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2991 train_acc: 0.9994 val_loss: 0.4742 val_acc: 0.9168 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2989 train_acc: 0.9994 val_loss: 0.4733 val_acc: 0.9174 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2989 train_acc: 0.9994 val_loss: 0.4722 val_acc: 0.9174 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2993 train_acc: 0.9993 val_loss: 0.4722 val_acc: 0.9170 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.2989 train_acc: 0.9995 val_loss: 0.4717 val_acc: 0.9181 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.2990 train_acc: 0.9994 val_loss: 0.4742 val_acc: 0.9172 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.2990 train_acc: 0.9995 val_loss: 0.4717 val_acc: 0.9177 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.2990 train_acc: 0.9994 val_loss: 0.4722 val_acc: 0.9172 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9106\n",
      "14000: 0.9105555555555556\n",
      " test_acc: 0.9106\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.1703 train_acc: 0.5745 val_loss: 1.1965 val_acc: 0.5444 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0350 train_acc: 0.6349 val_loss: 1.0608 val_acc: 0.6161 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9984 train_acc: 0.6561 val_loss: 0.9962 val_acc: 0.6630 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.9680 train_acc: 0.6771 val_loss: 1.0163 val_acc: 0.6218 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.9463 train_acc: 0.6882 val_loss: 0.9422 val_acc: 0.6985 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.9301 train_acc: 0.6976 val_loss: 1.4185 val_acc: 0.4210 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.9104 train_acc: 0.7154 val_loss: 1.2440 val_acc: 0.4991 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.8924 train_acc: 0.7228 val_loss: 1.0285 val_acc: 0.6479 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.8772 train_acc: 0.7340 val_loss: 1.3524 val_acc: 0.5338 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.8425 train_acc: 0.7548 val_loss: 0.9574 val_acc: 0.6824 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.8266 train_acc: 0.7643 val_loss: 0.9233 val_acc: 0.7086 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.8122 train_acc: 0.7726 val_loss: 0.9757 val_acc: 0.6831 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.8008 train_acc: 0.7802 val_loss: 1.0429 val_acc: 0.6533 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.7902 train_acc: 0.7866 val_loss: 1.0796 val_acc: 0.6570 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.7782 train_acc: 0.7926 val_loss: 0.9428 val_acc: 0.7029 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.7443 train_acc: 0.8138 val_loss: 0.9122 val_acc: 0.7233 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.7254 train_acc: 0.8239 val_loss: 0.8854 val_acc: 0.7387 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.7107 train_acc: 0.8344 val_loss: 1.0577 val_acc: 0.6635 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.6997 train_acc: 0.8421 val_loss: 0.9575 val_acc: 0.7181 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.6857 train_acc: 0.8489 val_loss: 1.0574 val_acc: 0.6949 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.6705 train_acc: 0.8580 val_loss: 1.0180 val_acc: 0.7004 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.6428 train_acc: 0.8743 val_loss: 0.9188 val_acc: 0.7420 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.6276 train_acc: 0.8841 val_loss: 0.9019 val_acc: 0.7512 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.6203 train_acc: 0.8888 val_loss: 0.8793 val_acc: 0.7556 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.6053 train_acc: 0.8985 val_loss: 0.9096 val_acc: 0.7486 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.5996 train_acc: 0.9023 val_loss: 0.8996 val_acc: 0.7498 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.5881 train_acc: 0.9094 val_loss: 0.9287 val_acc: 0.7461 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.5784 train_acc: 0.9132 val_loss: 0.8943 val_acc: 0.7597 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.5724 train_acc: 0.9176 val_loss: 1.0560 val_acc: 0.6985 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.5668 train_acc: 0.9201 val_loss: 0.9584 val_acc: 0.7433 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5539 train_acc: 0.9274 val_loss: 0.9148 val_acc: 0.7527 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5484 train_acc: 0.9313 val_loss: 1.0282 val_acc: 0.7132 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.5329 train_acc: 0.9417 val_loss: 0.9239 val_acc: 0.7517 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.5287 train_acc: 0.9440 val_loss: 0.9384 val_acc: 0.7457 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.5238 train_acc: 0.9461 val_loss: 0.9338 val_acc: 0.7531 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.5192 train_acc: 0.9497 val_loss: 0.9617 val_acc: 0.7431 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.5104 train_acc: 0.9546 val_loss: 0.9303 val_acc: 0.7548 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.5075 train_acc: 0.9558 val_loss: 0.9363 val_acc: 0.7518 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.5060 train_acc: 0.9568 val_loss: 0.9330 val_acc: 0.7551 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.5035 train_acc: 0.9589 val_loss: 0.9392 val_acc: 0.7543 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.5003 train_acc: 0.9614 val_loss: 0.9401 val_acc: 0.7516 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.4997 train_acc: 0.9605 val_loss: 0.9305 val_acc: 0.7538 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.4972 train_acc: 0.9629 val_loss: 0.9315 val_acc: 0.7571 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.4957 train_acc: 0.9629 val_loss: 0.9326 val_acc: 0.7535 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.4960 train_acc: 0.9634 val_loss: 0.9377 val_acc: 0.7525 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.4963 train_acc: 0.9627 val_loss: 0.9416 val_acc: 0.7509 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.4958 train_acc: 0.9641 val_loss: 0.9417 val_acc: 0.7507 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.4941 train_acc: 0.9642 val_loss: 0.9739 val_acc: 0.7359 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.4924 train_acc: 0.9642 val_loss: 0.9341 val_acc: 0.7542 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.4919 train_acc: 0.9648 val_loss: 0.9357 val_acc: 0.7523 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.4914 train_acc: 0.9650 val_loss: 0.9357 val_acc: 0.7544 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.4894 train_acc: 0.9665 val_loss: 0.9390 val_acc: 0.7530 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.4902 train_acc: 0.9669 val_loss: 0.9415 val_acc: 0.7545 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.4868 train_acc: 0.9678 val_loss: 0.9390 val_acc: 0.7539 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.4876 train_acc: 0.9678 val_loss: 0.9396 val_acc: 0.7549 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.4878 train_acc: 0.9666 val_loss: 0.9356 val_acc: 0.7532 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.4873 train_acc: 0.9672 val_loss: 0.9345 val_acc: 0.7555 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.4878 train_acc: 0.9666 val_loss: 0.9381 val_acc: 0.7523 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7568\n",
      "14000: 0.7568\n",
      " test_acc: 0.7568\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.0077 train_acc: 0.6398 val_loss: 1.0664 val_acc: 0.5827 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.8619 train_acc: 0.7065 val_loss: 0.8669 val_acc: 0.7044 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8212 train_acc: 0.7332 val_loss: 1.0091 val_acc: 0.6347 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.7934 train_acc: 0.7516 val_loss: 0.9624 val_acc: 0.6443 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.7713 train_acc: 0.7635 val_loss: 0.8520 val_acc: 0.7089 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7560 train_acc: 0.7722 val_loss: 0.8216 val_acc: 0.7211 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7406 train_acc: 0.7813 val_loss: 0.7535 val_acc: 0.7728 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7272 train_acc: 0.7895 val_loss: 1.0288 val_acc: 0.6175 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7181 train_acc: 0.7967 val_loss: 0.8608 val_acc: 0.7038 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7082 train_acc: 0.7995 val_loss: 1.4802 val_acc: 0.4457 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.6951 train_acc: 0.8082 val_loss: 1.1394 val_acc: 0.5798 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.6601 train_acc: 0.8289 val_loss: 0.7964 val_acc: 0.7400 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6497 train_acc: 0.8343 val_loss: 0.8350 val_acc: 0.7218 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6404 train_acc: 0.8410 val_loss: 0.8752 val_acc: 0.6970 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6293 train_acc: 0.8484 val_loss: 1.0473 val_acc: 0.6576 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.6014 train_acc: 0.8638 val_loss: 0.8303 val_acc: 0.7600 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.5912 train_acc: 0.8716 val_loss: 0.8090 val_acc: 0.7741 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.5830 train_acc: 0.8769 val_loss: 0.9453 val_acc: 0.7208 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.5730 train_acc: 0.8833 val_loss: 0.7490 val_acc: 0.7927 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5633 train_acc: 0.8893 val_loss: 0.6885 val_acc: 0.8121 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5527 train_acc: 0.8966 val_loss: 0.6692 val_acc: 0.8297 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.5439 train_acc: 0.9018 val_loss: 0.6732 val_acc: 0.8241 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.5345 train_acc: 0.9077 val_loss: 0.7584 val_acc: 0.7734 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.5244 train_acc: 0.9133 val_loss: 1.1001 val_acc: 0.7024 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.5169 train_acc: 0.9185 val_loss: 0.8026 val_acc: 0.7875 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.4929 train_acc: 0.9339 val_loss: 0.7257 val_acc: 0.8128 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.4841 train_acc: 0.9400 val_loss: 0.7085 val_acc: 0.8173 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.4805 train_acc: 0.9409 val_loss: 0.8399 val_acc: 0.7750 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.4736 train_acc: 0.9468 val_loss: 0.7421 val_acc: 0.8003 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.4615 train_acc: 0.9545 val_loss: 0.6894 val_acc: 0.8260 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.4566 train_acc: 0.9573 val_loss: 0.7057 val_acc: 0.8258 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.4562 train_acc: 0.9571 val_loss: 0.7068 val_acc: 0.8172 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.4525 train_acc: 0.9592 val_loss: 0.6932 val_acc: 0.8274 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.4460 train_acc: 0.9632 val_loss: 0.7043 val_acc: 0.8289 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.4450 train_acc: 0.9644 val_loss: 0.7113 val_acc: 0.8241 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.4440 train_acc: 0.9648 val_loss: 0.6989 val_acc: 0.8279 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.4418 train_acc: 0.9656 val_loss: 0.6975 val_acc: 0.8281 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.4379 train_acc: 0.9684 val_loss: 0.7014 val_acc: 0.8298 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.4389 train_acc: 0.9671 val_loss: 0.6986 val_acc: 0.8277 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.4360 train_acc: 0.9693 val_loss: 0.6978 val_acc: 0.8280 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.4377 train_acc: 0.9682 val_loss: 0.7012 val_acc: 0.8273 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.4351 train_acc: 0.9700 val_loss: 0.7021 val_acc: 0.8270 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.4335 train_acc: 0.9714 val_loss: 0.7004 val_acc: 0.8285 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.4331 train_acc: 0.9707 val_loss: 0.7006 val_acc: 0.8257 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.4333 train_acc: 0.9712 val_loss: 0.7013 val_acc: 0.8277 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.4317 train_acc: 0.9721 val_loss: 0.7021 val_acc: 0.8269 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.4316 train_acc: 0.9716 val_loss: 0.7032 val_acc: 0.8251 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.4305 train_acc: 0.9727 val_loss: 0.7032 val_acc: 0.8264 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.4302 train_acc: 0.9728 val_loss: 0.7013 val_acc: 0.8266 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.4324 train_acc: 0.9719 val_loss: 0.7038 val_acc: 0.8254 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.4316 train_acc: 0.9725 val_loss: 0.7040 val_acc: 0.8289 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.4286 train_acc: 0.9745 val_loss: 0.7065 val_acc: 0.8258 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.4302 train_acc: 0.9724 val_loss: 0.7019 val_acc: 0.8267 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.4292 train_acc: 0.9735 val_loss: 0.7043 val_acc: 0.8261 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.4271 train_acc: 0.9742 val_loss: 0.7024 val_acc: 0.8274 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.4272 train_acc: 0.9745 val_loss: 0.7081 val_acc: 0.8266 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.4268 train_acc: 0.9750 val_loss: 0.7042 val_acc: 0.8277 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.4282 train_acc: 0.9745 val_loss: 0.7061 val_acc: 0.8286 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.4266 train_acc: 0.9754 val_loss: 0.7007 val_acc: 0.8278 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.4263 train_acc: 0.9750 val_loss: 0.7029 val_acc: 0.8269 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.4261 train_acc: 0.9748 val_loss: 0.7045 val_acc: 0.8260 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.4269 train_acc: 0.9747 val_loss: 0.7042 val_acc: 0.8285 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.4250 train_acc: 0.9753 val_loss: 0.7159 val_acc: 0.8247 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.4246 train_acc: 0.9768 val_loss: 0.7026 val_acc: 0.8268 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.4248 train_acc: 0.9756 val_loss: 0.7061 val_acc: 0.8266 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.4232 train_acc: 0.9768 val_loss: 0.7056 val_acc: 0.8253 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.4250 train_acc: 0.9752 val_loss: 0.7048 val_acc: 0.8251 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.4243 train_acc: 0.9759 val_loss: 0.7067 val_acc: 0.8249 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8198\n",
      "14000: 0.8198333333333333\n",
      " test_acc: 0.8198\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7842 train_acc: 0.5853 val_loss: 0.7042 val_acc: 0.5900 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6431 train_acc: 0.6440 val_loss: 0.7017 val_acc: 0.5004 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6264 train_acc: 0.6690 val_loss: 0.6333 val_acc: 0.6741 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6193 train_acc: 0.6804 val_loss: 0.9719 val_acc: 0.4997 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6108 train_acc: 0.6859 val_loss: 0.6126 val_acc: 0.6755 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.6069 train_acc: 0.6923 val_loss: 0.9153 val_acc: 0.5106 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5994 train_acc: 0.7016 val_loss: 0.7823 val_acc: 0.5798 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5995 train_acc: 0.6997 val_loss: 0.6584 val_acc: 0.6472 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5929 train_acc: 0.7063 val_loss: 0.6802 val_acc: 0.6020 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.5820 train_acc: 0.7165 val_loss: 0.6000 val_acc: 0.7003 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.5782 train_acc: 0.7189 val_loss: 0.8743 val_acc: 0.5380 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.5755 train_acc: 0.7253 val_loss: 0.6026 val_acc: 0.6996 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.5703 train_acc: 0.7288 val_loss: 0.6651 val_acc: 0.6298 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.5683 train_acc: 0.7308 val_loss: 0.5863 val_acc: 0.7203 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.5642 train_acc: 0.7355 val_loss: 0.8284 val_acc: 0.5189 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.5582 train_acc: 0.7394 val_loss: 0.6106 val_acc: 0.6823 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.5508 train_acc: 0.7474 val_loss: 0.6592 val_acc: 0.6218 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.5460 train_acc: 0.7485 val_loss: 0.7216 val_acc: 0.5930 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.5234 train_acc: 0.7691 val_loss: 0.7023 val_acc: 0.6259 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5111 train_acc: 0.7817 val_loss: 0.5959 val_acc: 0.7153 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5020 train_acc: 0.7868 val_loss: 0.6226 val_acc: 0.6985 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.4881 train_acc: 0.7995 val_loss: 0.7383 val_acc: 0.6254 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.4629 train_acc: 0.8203 val_loss: 0.6658 val_acc: 0.6899 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.4492 train_acc: 0.8317 val_loss: 0.6238 val_acc: 0.7079 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.4393 train_acc: 0.8406 val_loss: 0.6330 val_acc: 0.7033 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.4223 train_acc: 0.8536 val_loss: 0.6896 val_acc: 0.6955 lr: 0.0003125\n",
      "Epoch  26 / 200 train_loss: 0.4033 train_acc: 0.8684 val_loss: 0.6600 val_acc: 0.7000 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.3936 train_acc: 0.8768 val_loss: 0.6719 val_acc: 0.7067 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.3827 train_acc: 0.8836 val_loss: 0.6875 val_acc: 0.7075 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.3772 train_acc: 0.8876 val_loss: 0.6907 val_acc: 0.7000 lr: 0.00015625\n",
      "Epoch  30 / 200 train_loss: 0.3642 train_acc: 0.8969 val_loss: 0.6975 val_acc: 0.6960 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.3570 train_acc: 0.9036 val_loss: 0.7050 val_acc: 0.6996 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.3544 train_acc: 0.9039 val_loss: 0.7052 val_acc: 0.7032 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3504 train_acc: 0.9087 val_loss: 0.7261 val_acc: 0.6943 lr: 7.8125e-05\n",
      "Epoch  34 / 200 train_loss: 0.3425 train_acc: 0.9137 val_loss: 0.7235 val_acc: 0.7026 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.3402 train_acc: 0.9146 val_loss: 0.7246 val_acc: 0.6990 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.3365 train_acc: 0.9183 val_loss: 0.7212 val_acc: 0.7014 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.3351 train_acc: 0.9183 val_loss: 0.7355 val_acc: 0.7010 lr: 5e-05\n",
      "Epoch  38 / 200 train_loss: 0.3330 train_acc: 0.9211 val_loss: 0.7313 val_acc: 0.6939 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.3301 train_acc: 0.9228 val_loss: 0.7340 val_acc: 0.7019 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3275 train_acc: 0.9247 val_loss: 0.7360 val_acc: 0.6992 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3272 train_acc: 0.9249 val_loss: 0.7325 val_acc: 0.7010 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3249 train_acc: 0.9249 val_loss: 0.7355 val_acc: 0.6989 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3294 train_acc: 0.9256 val_loss: 0.7416 val_acc: 0.6971 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7192\n",
      "14000: 0.7191666666666666\n",
      " test_acc: 0.7192\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7896 train_acc: 0.6239 val_loss: 0.6400 val_acc: 0.6509 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6289 train_acc: 0.6686 val_loss: 0.6240 val_acc: 0.6601 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6180 train_acc: 0.6759 val_loss: 0.6005 val_acc: 0.6968 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6034 train_acc: 0.6953 val_loss: 0.6940 val_acc: 0.6131 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5896 train_acc: 0.7135 val_loss: 0.7419 val_acc: 0.6056 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5750 train_acc: 0.7239 val_loss: 0.5721 val_acc: 0.7314 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5643 train_acc: 0.7359 val_loss: 0.6819 val_acc: 0.6533 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5504 train_acc: 0.7508 val_loss: 0.5990 val_acc: 0.6975 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5387 train_acc: 0.7608 val_loss: 0.7348 val_acc: 0.6340 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.5286 train_acc: 0.7681 val_loss: 0.5836 val_acc: 0.7270 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.5023 train_acc: 0.7914 val_loss: 0.5333 val_acc: 0.7702 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4892 train_acc: 0.8024 val_loss: 0.6451 val_acc: 0.7022 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4776 train_acc: 0.8122 val_loss: 0.5293 val_acc: 0.7667 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4663 train_acc: 0.8201 val_loss: 0.4939 val_acc: 0.8012 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4583 train_acc: 0.8260 val_loss: 0.5027 val_acc: 0.7863 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.4453 train_acc: 0.8377 val_loss: 0.5214 val_acc: 0.7821 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.4347 train_acc: 0.8472 val_loss: 0.9133 val_acc: 0.5767 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.4269 train_acc: 0.8498 val_loss: 0.5266 val_acc: 0.7867 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3896 train_acc: 0.8775 val_loss: 0.5111 val_acc: 0.7988 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3720 train_acc: 0.8922 val_loss: 0.5451 val_acc: 0.7850 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3611 train_acc: 0.8992 val_loss: 0.6599 val_acc: 0.7361 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3445 train_acc: 0.9125 val_loss: 0.6323 val_acc: 0.7453 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3217 train_acc: 0.9293 val_loss: 0.5275 val_acc: 0.8046 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.3082 train_acc: 0.9392 val_loss: 0.5379 val_acc: 0.7992 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3010 train_acc: 0.9438 val_loss: 0.5762 val_acc: 0.7865 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2930 train_acc: 0.9501 val_loss: 0.5492 val_acc: 0.7973 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2870 train_acc: 0.9538 val_loss: 0.5437 val_acc: 0.8018 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.2735 train_acc: 0.9646 val_loss: 0.5519 val_acc: 0.7929 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.2693 train_acc: 0.9664 val_loss: 0.5630 val_acc: 0.7966 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2643 train_acc: 0.9709 val_loss: 0.5603 val_acc: 0.7960 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2601 train_acc: 0.9733 val_loss: 0.5541 val_acc: 0.8023 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.2563 train_acc: 0.9765 val_loss: 0.5551 val_acc: 0.7974 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.2531 train_acc: 0.9781 val_loss: 0.5548 val_acc: 0.7971 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.2518 train_acc: 0.9803 val_loss: 0.5566 val_acc: 0.7953 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2514 train_acc: 0.9790 val_loss: 0.5589 val_acc: 0.7976 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.2473 train_acc: 0.9829 val_loss: 0.5590 val_acc: 0.8010 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.2457 train_acc: 0.9831 val_loss: 0.5591 val_acc: 0.7978 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.2472 train_acc: 0.9827 val_loss: 0.5637 val_acc: 0.7956 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.2447 train_acc: 0.9847 val_loss: 0.5672 val_acc: 0.7980 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.2432 train_acc: 0.9854 val_loss: 0.5606 val_acc: 0.8003 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.2438 train_acc: 0.9842 val_loss: 0.5588 val_acc: 0.7964 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.2434 train_acc: 0.9852 val_loss: 0.5703 val_acc: 0.7969 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2424 train_acc: 0.9849 val_loss: 0.5635 val_acc: 0.8002 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2422 train_acc: 0.9856 val_loss: 0.5663 val_acc: 0.7999 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2415 train_acc: 0.9859 val_loss: 0.5677 val_acc: 0.7978 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2401 train_acc: 0.9866 val_loss: 0.5628 val_acc: 0.7965 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2394 train_acc: 0.9869 val_loss: 0.5652 val_acc: 0.7963 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2394 train_acc: 0.9882 val_loss: 0.5654 val_acc: 0.7971 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2389 train_acc: 0.9877 val_loss: 0.5640 val_acc: 0.7953 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2393 train_acc: 0.9871 val_loss: 0.5641 val_acc: 0.7957 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2397 train_acc: 0.9863 val_loss: 0.5710 val_acc: 0.7972 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2378 train_acc: 0.9879 val_loss: 0.5648 val_acc: 0.7974 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2369 train_acc: 0.9886 val_loss: 0.5662 val_acc: 0.7955 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8018\n",
      "14000: 0.8018333333333333\n",
      " test_acc: 0.8018\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7493 train_acc: 0.6664 val_loss: 0.5909 val_acc: 0.7023 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5744 train_acc: 0.7333 val_loss: 0.5595 val_acc: 0.7486 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5442 train_acc: 0.7584 val_loss: 0.6384 val_acc: 0.6988 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5208 train_acc: 0.7806 val_loss: 0.5731 val_acc: 0.7490 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5040 train_acc: 0.7968 val_loss: 0.4828 val_acc: 0.8126 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.4893 train_acc: 0.8040 val_loss: 0.5584 val_acc: 0.7510 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4784 train_acc: 0.8135 val_loss: 0.4695 val_acc: 0.8184 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.4722 train_acc: 0.8153 val_loss: 0.4795 val_acc: 0.8124 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4604 train_acc: 0.8270 val_loss: 0.4915 val_acc: 0.8045 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4531 train_acc: 0.8313 val_loss: 0.5261 val_acc: 0.7812 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4479 train_acc: 0.8352 val_loss: 0.5640 val_acc: 0.7725 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4198 train_acc: 0.8572 val_loss: 0.4457 val_acc: 0.8346 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4113 train_acc: 0.8596 val_loss: 0.4513 val_acc: 0.8369 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4029 train_acc: 0.8663 val_loss: 0.4606 val_acc: 0.8258 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.3950 train_acc: 0.8724 val_loss: 0.4423 val_acc: 0.8409 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.3887 train_acc: 0.8772 val_loss: 0.4558 val_acc: 0.8341 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.3771 train_acc: 0.8858 val_loss: 0.4275 val_acc: 0.8532 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.3693 train_acc: 0.8891 val_loss: 0.4497 val_acc: 0.8386 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.3659 train_acc: 0.8945 val_loss: 0.5108 val_acc: 0.7946 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.3542 train_acc: 0.9036 val_loss: 0.4623 val_acc: 0.8317 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.3422 train_acc: 0.9116 val_loss: 0.4819 val_acc: 0.8284 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3144 train_acc: 0.9302 val_loss: 0.4531 val_acc: 0.8504 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3002 train_acc: 0.9427 val_loss: 0.4627 val_acc: 0.8427 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2931 train_acc: 0.9454 val_loss: 0.4660 val_acc: 0.8434 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.2810 train_acc: 0.9552 val_loss: 0.4587 val_acc: 0.8499 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2598 train_acc: 0.9682 val_loss: 0.4708 val_acc: 0.8483 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2555 train_acc: 0.9715 val_loss: 0.4756 val_acc: 0.8484 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2510 train_acc: 0.9726 val_loss: 0.4734 val_acc: 0.8533 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2459 train_acc: 0.9772 val_loss: 0.4804 val_acc: 0.8469 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2416 train_acc: 0.9797 val_loss: 0.4879 val_acc: 0.8477 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.2393 train_acc: 0.9811 val_loss: 0.4799 val_acc: 0.8506 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.2371 train_acc: 0.9827 val_loss: 0.4841 val_acc: 0.8494 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2289 train_acc: 0.9883 val_loss: 0.4843 val_acc: 0.8499 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2283 train_acc: 0.9877 val_loss: 0.4797 val_acc: 0.8473 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2251 train_acc: 0.9899 val_loss: 0.4849 val_acc: 0.8512 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2246 train_acc: 0.9900 val_loss: 0.4872 val_acc: 0.8492 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2213 train_acc: 0.9922 val_loss: 0.4821 val_acc: 0.8478 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2206 train_acc: 0.9923 val_loss: 0.4882 val_acc: 0.8475 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2194 train_acc: 0.9927 val_loss: 0.4916 val_acc: 0.8488 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2181 train_acc: 0.9940 val_loss: 0.4899 val_acc: 0.8472 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.2170 train_acc: 0.9947 val_loss: 0.4878 val_acc: 0.8481 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2178 train_acc: 0.9939 val_loss: 0.4865 val_acc: 0.8474 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2165 train_acc: 0.9948 val_loss: 0.4903 val_acc: 0.8487 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.2163 train_acc: 0.9947 val_loss: 0.4934 val_acc: 0.8467 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2157 train_acc: 0.9946 val_loss: 0.4902 val_acc: 0.8450 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2164 train_acc: 0.9939 val_loss: 0.4908 val_acc: 0.8486 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2150 train_acc: 0.9950 val_loss: 0.4891 val_acc: 0.8485 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2152 train_acc: 0.9949 val_loss: 0.4892 val_acc: 0.8476 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2151 train_acc: 0.9949 val_loss: 0.4870 val_acc: 0.8485 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2138 train_acc: 0.9960 val_loss: 0.4881 val_acc: 0.8471 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2144 train_acc: 0.9954 val_loss: 0.4888 val_acc: 0.8471 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2144 train_acc: 0.9953 val_loss: 0.4906 val_acc: 0.8484 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2147 train_acc: 0.9952 val_loss: 0.4895 val_acc: 0.8473 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2141 train_acc: 0.9958 val_loss: 0.4881 val_acc: 0.8477 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2141 train_acc: 0.9953 val_loss: 0.4906 val_acc: 0.8479 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2136 train_acc: 0.9958 val_loss: 0.4900 val_acc: 0.8483 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2144 train_acc: 0.9953 val_loss: 0.4905 val_acc: 0.8504 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2132 train_acc: 0.9962 val_loss: 0.4895 val_acc: 0.8464 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8457\n",
      "14000: 0.8456666666666667\n",
      " test_acc: 0.8457\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.8053 train_acc: 0.5402 val_loss: 0.6778 val_acc: 0.5779 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6765 train_acc: 0.5811 val_loss: 0.7142 val_acc: 0.5260 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6622 train_acc: 0.6122 val_loss: 0.6593 val_acc: 0.6129 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6476 train_acc: 0.6351 val_loss: 0.7350 val_acc: 0.5962 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6359 train_acc: 0.6521 val_loss: 0.6704 val_acc: 0.6285 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.6257 train_acc: 0.6651 val_loss: 0.6554 val_acc: 0.6256 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.6093 train_acc: 0.6908 val_loss: 0.6262 val_acc: 0.6570 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5978 train_acc: 0.7024 val_loss: 0.6464 val_acc: 0.6452 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5830 train_acc: 0.7174 val_loss: 0.6586 val_acc: 0.6382 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.5792 train_acc: 0.7215 val_loss: 0.6670 val_acc: 0.6353 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.5692 train_acc: 0.7280 val_loss: 0.6575 val_acc: 0.6641 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.5569 train_acc: 0.7411 val_loss: 0.6230 val_acc: 0.6682 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.5492 train_acc: 0.7494 val_loss: 0.5552 val_acc: 0.7444 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.5397 train_acc: 0.7566 val_loss: 0.8352 val_acc: 0.5774 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.5337 train_acc: 0.7628 val_loss: 0.6089 val_acc: 0.6774 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.5289 train_acc: 0.7667 val_loss: 0.6284 val_acc: 0.6681 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.5124 train_acc: 0.7814 val_loss: 0.6613 val_acc: 0.6482 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.4854 train_acc: 0.7988 val_loss: 0.5363 val_acc: 0.7594 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.4701 train_acc: 0.8149 val_loss: 0.5543 val_acc: 0.7497 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.4599 train_acc: 0.8216 val_loss: 0.5555 val_acc: 0.7509 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.4458 train_acc: 0.8336 val_loss: 0.6458 val_acc: 0.6862 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.4337 train_acc: 0.8459 val_loss: 0.5913 val_acc: 0.7311 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3998 train_acc: 0.8730 val_loss: 0.5781 val_acc: 0.7486 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3861 train_acc: 0.8814 val_loss: 0.5919 val_acc: 0.7482 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.3751 train_acc: 0.8886 val_loss: 0.5917 val_acc: 0.7551 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.3601 train_acc: 0.9026 val_loss: 0.5506 val_acc: 0.7778 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.3520 train_acc: 0.9097 val_loss: 0.5549 val_acc: 0.7795 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.3367 train_acc: 0.9201 val_loss: 0.5958 val_acc: 0.7611 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.3302 train_acc: 0.9265 val_loss: 0.5589 val_acc: 0.7761 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.3193 train_acc: 0.9345 val_loss: 0.5689 val_acc: 0.7745 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.3110 train_acc: 0.9397 val_loss: 0.5919 val_acc: 0.7695 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.2945 train_acc: 0.9535 val_loss: 0.6038 val_acc: 0.7624 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.2841 train_acc: 0.9622 val_loss: 0.5773 val_acc: 0.7775 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.2807 train_acc: 0.9637 val_loss: 0.5709 val_acc: 0.7760 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.2736 train_acc: 0.9695 val_loss: 0.5755 val_acc: 0.7771 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2672 train_acc: 0.9734 val_loss: 0.5825 val_acc: 0.7785 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2651 train_acc: 0.9748 val_loss: 0.5771 val_acc: 0.7795 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2630 train_acc: 0.9774 val_loss: 0.5809 val_acc: 0.7768 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.2612 train_acc: 0.9779 val_loss: 0.5801 val_acc: 0.7823 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.2567 train_acc: 0.9814 val_loss: 0.5857 val_acc: 0.7801 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.2570 train_acc: 0.9815 val_loss: 0.5835 val_acc: 0.7802 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.2546 train_acc: 0.9819 val_loss: 0.5819 val_acc: 0.7811 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.2534 train_acc: 0.9834 val_loss: 0.5835 val_acc: 0.7790 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2511 train_acc: 0.9843 val_loss: 0.5807 val_acc: 0.7812 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2479 train_acc: 0.9873 val_loss: 0.5784 val_acc: 0.7814 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.2470 train_acc: 0.9876 val_loss: 0.5804 val_acc: 0.7812 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.2447 train_acc: 0.9883 val_loss: 0.5822 val_acc: 0.7827 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.2442 train_acc: 0.9886 val_loss: 0.5841 val_acc: 0.7791 lr: 0.00015625\n",
      "Epoch  48 / 200 train_loss: 0.2451 train_acc: 0.9877 val_loss: 0.5796 val_acc: 0.7763 lr: 0.00015625\n",
      "Epoch  49 / 200 train_loss: 0.2438 train_acc: 0.9882 val_loss: 0.5825 val_acc: 0.7810 lr: 0.00015625\n",
      "Epoch  50 / 200 train_loss: 0.2447 train_acc: 0.9874 val_loss: 0.5818 val_acc: 0.7759 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.2414 train_acc: 0.9893 val_loss: 0.5817 val_acc: 0.7827 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.2410 train_acc: 0.9904 val_loss: 0.5811 val_acc: 0.7765 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.2415 train_acc: 0.9893 val_loss: 0.5817 val_acc: 0.7795 lr: 7.8125e-05\n",
      "Epoch  54 / 200 train_loss: 0.2415 train_acc: 0.9897 val_loss: 0.5800 val_acc: 0.7832 lr: 7.8125e-05\n",
      "Epoch  55 / 200 train_loss: 0.2402 train_acc: 0.9898 val_loss: 0.5838 val_acc: 0.7780 lr: 7.8125e-05\n",
      "Epoch  56 / 200 train_loss: 0.2397 train_acc: 0.9907 val_loss: 0.5801 val_acc: 0.7807 lr: 7.8125e-05\n",
      "Epoch  57 / 200 train_loss: 0.2397 train_acc: 0.9907 val_loss: 0.5821 val_acc: 0.7791 lr: 7.8125e-05\n",
      "Epoch  58 / 200 train_loss: 0.2381 train_acc: 0.9916 val_loss: 0.5807 val_acc: 0.7792 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2382 train_acc: 0.9912 val_loss: 0.5786 val_acc: 0.7822 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2375 train_acc: 0.9915 val_loss: 0.5787 val_acc: 0.7800 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2378 train_acc: 0.9914 val_loss: 0.5806 val_acc: 0.7807 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2389 train_acc: 0.9914 val_loss: 0.5772 val_acc: 0.7793 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2384 train_acc: 0.9916 val_loss: 0.5786 val_acc: 0.7814 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2379 train_acc: 0.9914 val_loss: 0.5813 val_acc: 0.7803 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2375 train_acc: 0.9915 val_loss: 0.5796 val_acc: 0.7824 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2378 train_acc: 0.9918 val_loss: 0.5792 val_acc: 0.7815 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2367 train_acc: 0.9921 val_loss: 0.5793 val_acc: 0.7812 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.2360 train_acc: 0.9931 val_loss: 0.5802 val_acc: 0.7798 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2360 train_acc: 0.9922 val_loss: 0.5804 val_acc: 0.7817 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2363 train_acc: 0.9921 val_loss: 0.5809 val_acc: 0.7786 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2352 train_acc: 0.9929 val_loss: 0.5809 val_acc: 0.7786 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2363 train_acc: 0.9924 val_loss: 0.5777 val_acc: 0.7815 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.2358 train_acc: 0.9928 val_loss: 0.5814 val_acc: 0.7817 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.2364 train_acc: 0.9923 val_loss: 0.5797 val_acc: 0.7811 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.2352 train_acc: 0.9932 val_loss: 0.5795 val_acc: 0.7796 lr: 5e-05\n",
      "Epoch 104 / 200 train_loss: 0.2303 train_acc: 0.9951 val_loss: 0.5800 val_acc: 0.7795 lr: 5e-05\n",
      "Epoch 105 / 200 train_loss: 0.2306 train_acc: 0.9946 val_loss: 0.5771 val_acc: 0.7795 lr: 5e-05\n",
      "Epoch 106 / 200 train_loss: 0.2304 train_acc: 0.9949 val_loss: 0.5773 val_acc: 0.7805 lr: 5e-05\n",
      "Epoch 107 / 200 train_loss: 0.2311 train_acc: 0.9949 val_loss: 0.5789 val_acc: 0.7803 lr: 5e-05\n",
      "Epoch 108 / 200 train_loss: 0.2285 train_acc: 0.9959 val_loss: 0.5768 val_acc: 0.7830 lr: 5e-05\n",
      "Epoch 109 / 200 train_loss: 0.2297 train_acc: 0.9954 val_loss: 0.5814 val_acc: 0.7787 lr: 5e-05\n",
      "Epoch 110 / 200 train_loss: 0.2293 train_acc: 0.9952 val_loss: 0.5805 val_acc: 0.7787 lr: 5e-05\n",
      "Epoch 111 / 200 train_loss: 0.2285 train_acc: 0.9959 val_loss: 0.5776 val_acc: 0.7808 lr: 5e-05\n",
      "Epoch 112 / 200 train_loss: 0.2280 train_acc: 0.9963 val_loss: 0.5783 val_acc: 0.7804 lr: 5e-05\n",
      "Epoch 113 / 200 train_loss: 0.2282 train_acc: 0.9961 val_loss: 0.5773 val_acc: 0.7808 lr: 5e-05\n",
      "Epoch 114 / 200 train_loss: 0.2281 train_acc: 0.9954 val_loss: 0.5764 val_acc: 0.7795 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7868\n",
      "14000: 0.7868333333333334\n",
      " test_acc: 0.7868\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7738 train_acc: 0.6184 val_loss: 0.6187 val_acc: 0.6965 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6093 train_acc: 0.6939 val_loss: 0.6076 val_acc: 0.6976 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5791 train_acc: 0.7249 val_loss: 0.6010 val_acc: 0.6978 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5604 train_acc: 0.7434 val_loss: 0.5910 val_acc: 0.7034 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5424 train_acc: 0.7574 val_loss: 0.5488 val_acc: 0.7575 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5279 train_acc: 0.7732 val_loss: 0.6499 val_acc: 0.6844 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5180 train_acc: 0.7790 val_loss: 0.5150 val_acc: 0.7757 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5076 train_acc: 0.7875 val_loss: 0.5725 val_acc: 0.7194 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4967 train_acc: 0.7967 val_loss: 0.5960 val_acc: 0.7228 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4880 train_acc: 0.8056 val_loss: 0.5980 val_acc: 0.7139 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4801 train_acc: 0.8114 val_loss: 0.5353 val_acc: 0.7630 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4543 train_acc: 0.8285 val_loss: 0.4660 val_acc: 0.8204 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4386 train_acc: 0.8411 val_loss: 0.4885 val_acc: 0.8053 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4327 train_acc: 0.8445 val_loss: 0.6051 val_acc: 0.7226 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4202 train_acc: 0.8564 val_loss: 0.4714 val_acc: 0.8204 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.4142 train_acc: 0.8583 val_loss: 0.4792 val_acc: 0.8121 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.3848 train_acc: 0.8822 val_loss: 0.4429 val_acc: 0.8388 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.3706 train_acc: 0.8911 val_loss: 0.4609 val_acc: 0.8290 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3655 train_acc: 0.8940 val_loss: 0.4810 val_acc: 0.8199 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3520 train_acc: 0.9035 val_loss: 0.4929 val_acc: 0.8150 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3414 train_acc: 0.9128 val_loss: 0.4743 val_acc: 0.8285 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3173 train_acc: 0.9293 val_loss: 0.4972 val_acc: 0.8220 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3066 train_acc: 0.9365 val_loss: 0.4567 val_acc: 0.8460 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.3006 train_acc: 0.9409 val_loss: 0.5143 val_acc: 0.8200 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2923 train_acc: 0.9471 val_loss: 0.4855 val_acc: 0.8387 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2854 train_acc: 0.9515 val_loss: 0.4702 val_acc: 0.8433 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2788 train_acc: 0.9553 val_loss: 0.4841 val_acc: 0.8421 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.2671 train_acc: 0.9632 val_loss: 0.4846 val_acc: 0.8410 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.2640 train_acc: 0.9663 val_loss: 0.4799 val_acc: 0.8407 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2578 train_acc: 0.9704 val_loss: 0.4943 val_acc: 0.8342 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2551 train_acc: 0.9733 val_loss: 0.4863 val_acc: 0.8422 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.2520 train_acc: 0.9737 val_loss: 0.4896 val_acc: 0.8417 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.2480 train_acc: 0.9777 val_loss: 0.4897 val_acc: 0.8375 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.2493 train_acc: 0.9752 val_loss: 0.4950 val_acc: 0.8395 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2450 train_acc: 0.9791 val_loss: 0.4981 val_acc: 0.8423 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.2443 train_acc: 0.9795 val_loss: 0.4884 val_acc: 0.8402 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.2409 train_acc: 0.9818 val_loss: 0.4958 val_acc: 0.8408 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.2420 train_acc: 0.9799 val_loss: 0.4950 val_acc: 0.8391 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.2436 train_acc: 0.9790 val_loss: 0.4958 val_acc: 0.8396 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.2397 train_acc: 0.9824 val_loss: 0.4937 val_acc: 0.8387 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.2387 train_acc: 0.9833 val_loss: 0.4940 val_acc: 0.8370 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.2404 train_acc: 0.9811 val_loss: 0.5022 val_acc: 0.8385 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2385 train_acc: 0.9832 val_loss: 0.4970 val_acc: 0.8389 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2407 train_acc: 0.9808 val_loss: 0.4938 val_acc: 0.8392 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2386 train_acc: 0.9826 val_loss: 0.4999 val_acc: 0.8374 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2375 train_acc: 0.9829 val_loss: 0.4991 val_acc: 0.8381 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2364 train_acc: 0.9846 val_loss: 0.4948 val_acc: 0.8395 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2362 train_acc: 0.9843 val_loss: 0.4993 val_acc: 0.8396 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2362 train_acc: 0.9844 val_loss: 0.4974 val_acc: 0.8403 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2347 train_acc: 0.9840 val_loss: 0.4948 val_acc: 0.8433 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2346 train_acc: 0.9852 val_loss: 0.5004 val_acc: 0.8442 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2361 train_acc: 0.9845 val_loss: 0.4995 val_acc: 0.8426 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2342 train_acc: 0.9849 val_loss: 0.4975 val_acc: 0.8436 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8352\n",
      "14000: 0.8351666666666666\n",
      " test_acc: 0.8352\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5093 train_acc: 0.8505 val_loss: 0.7642 val_acc: 0.5859 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3589 train_acc: 0.9000 val_loss: 0.3208 val_acc: 0.9210 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3396 train_acc: 0.9118 val_loss: 0.4440 val_acc: 0.8463 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3254 train_acc: 0.9209 val_loss: 0.4300 val_acc: 0.8473 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.3210 train_acc: 0.9220 val_loss: 0.3376 val_acc: 0.9128 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.3132 train_acc: 0.9273 val_loss: 0.3191 val_acc: 0.9157 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.3006 train_acc: 0.9331 val_loss: 0.4131 val_acc: 0.8608 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.2956 train_acc: 0.9362 val_loss: 0.4467 val_acc: 0.8335 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.2922 train_acc: 0.9399 val_loss: 0.3505 val_acc: 0.9090 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.2889 train_acc: 0.9411 val_loss: 0.2979 val_acc: 0.9312 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.2860 train_acc: 0.9447 val_loss: 0.2973 val_acc: 0.9391 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2843 train_acc: 0.9453 val_loss: 0.3164 val_acc: 0.9243 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2798 train_acc: 0.9493 val_loss: 0.4432 val_acc: 0.8471 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2790 train_acc: 0.9498 val_loss: 0.4358 val_acc: 0.8546 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2743 train_acc: 0.9526 val_loss: 0.2992 val_acc: 0.9354 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.2618 train_acc: 0.9604 val_loss: 0.2862 val_acc: 0.9420 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.2552 train_acc: 0.9652 val_loss: 0.3242 val_acc: 0.9243 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2524 train_acc: 0.9673 val_loss: 0.3268 val_acc: 0.9209 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2495 train_acc: 0.9714 val_loss: 0.2880 val_acc: 0.9412 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2434 train_acc: 0.9756 val_loss: 0.3038 val_acc: 0.9415 lr: 0.000625\n",
      "Epoch  20 / 200 train_loss: 0.2351 train_acc: 0.9808 val_loss: 0.3064 val_acc: 0.9354 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.2303 train_acc: 0.9842 val_loss: 0.2985 val_acc: 0.9429 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.2295 train_acc: 0.9842 val_loss: 0.2922 val_acc: 0.9446 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.2247 train_acc: 0.9872 val_loss: 0.3134 val_acc: 0.9382 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2238 train_acc: 0.9878 val_loss: 0.3087 val_acc: 0.9398 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2208 train_acc: 0.9899 val_loss: 0.3376 val_acc: 0.9258 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2181 train_acc: 0.9918 val_loss: 0.3026 val_acc: 0.9462 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2178 train_acc: 0.9915 val_loss: 0.3091 val_acc: 0.9377 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2168 train_acc: 0.9923 val_loss: 0.3183 val_acc: 0.9387 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2137 train_acc: 0.9949 val_loss: 0.3172 val_acc: 0.9404 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.2118 train_acc: 0.9954 val_loss: 0.3152 val_acc: 0.9396 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2092 train_acc: 0.9970 val_loss: 0.3087 val_acc: 0.9408 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2083 train_acc: 0.9975 val_loss: 0.3076 val_acc: 0.9419 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2077 train_acc: 0.9979 val_loss: 0.3055 val_acc: 0.9426 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2066 train_acc: 0.9983 val_loss: 0.3105 val_acc: 0.9427 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2056 train_acc: 0.9987 val_loss: 0.3077 val_acc: 0.9421 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2057 train_acc: 0.9985 val_loss: 0.3058 val_acc: 0.9431 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2048 train_acc: 0.9988 val_loss: 0.3047 val_acc: 0.9440 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2050 train_acc: 0.9985 val_loss: 0.3059 val_acc: 0.9438 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.2044 train_acc: 0.9990 val_loss: 0.3064 val_acc: 0.9436 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.2043 train_acc: 0.9993 val_loss: 0.3058 val_acc: 0.9427 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2038 train_acc: 0.9992 val_loss: 0.3058 val_acc: 0.9431 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2037 train_acc: 0.9992 val_loss: 0.3036 val_acc: 0.9449 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2039 train_acc: 0.9993 val_loss: 0.3044 val_acc: 0.9443 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2033 train_acc: 0.9995 val_loss: 0.3045 val_acc: 0.9455 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2035 train_acc: 0.9992 val_loss: 0.3054 val_acc: 0.9442 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2032 train_acc: 0.9996 val_loss: 0.3041 val_acc: 0.9445 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2032 train_acc: 0.9993 val_loss: 0.3032 val_acc: 0.9456 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2032 train_acc: 0.9994 val_loss: 0.3043 val_acc: 0.9450 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2030 train_acc: 0.9996 val_loss: 0.3046 val_acc: 0.9431 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2032 train_acc: 0.9992 val_loss: 0.3063 val_acc: 0.9418 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2033 train_acc: 0.9992 val_loss: 0.3038 val_acc: 0.9455 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2030 train_acc: 0.9994 val_loss: 0.3049 val_acc: 0.9429 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2028 train_acc: 0.9994 val_loss: 0.3053 val_acc: 0.9448 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2030 train_acc: 0.9993 val_loss: 0.3048 val_acc: 0.9445 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2029 train_acc: 0.9994 val_loss: 0.3043 val_acc: 0.9445 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2028 train_acc: 0.9995 val_loss: 0.3043 val_acc: 0.9445 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9378\n",
      "14000: 0.9378333333333333\n",
      " test_acc: 0.9378\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.4282 train_acc: 0.9105 val_loss: 0.2754 val_acc: 0.9585 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.2918 train_acc: 0.9462 val_loss: 0.3666 val_acc: 0.8925 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.2812 train_acc: 0.9541 val_loss: 0.4285 val_acc: 0.8705 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.2699 train_acc: 0.9601 val_loss: 0.4866 val_acc: 0.8309 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.2631 train_acc: 0.9645 val_loss: 0.3396 val_acc: 0.9168 lr: 0.0025\n",
      "Epoch   5 / 200 train_loss: 0.2528 train_acc: 0.9703 val_loss: 0.2545 val_acc: 0.9740 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.2486 train_acc: 0.9723 val_loss: 0.2413 val_acc: 0.9783 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.2497 train_acc: 0.9720 val_loss: 0.2689 val_acc: 0.9592 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.2434 train_acc: 0.9752 val_loss: 0.3127 val_acc: 0.9344 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.2429 train_acc: 0.9758 val_loss: 0.2331 val_acc: 0.9817 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.2421 train_acc: 0.9764 val_loss: 0.2346 val_acc: 0.9812 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2386 train_acc: 0.9784 val_loss: 0.3604 val_acc: 0.9049 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2391 train_acc: 0.9776 val_loss: 0.2382 val_acc: 0.9757 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2371 train_acc: 0.9789 val_loss: 0.2618 val_acc: 0.9663 lr: 0.00125\n",
      "Epoch  14 / 200 train_loss: 0.2275 train_acc: 0.9843 val_loss: 0.2413 val_acc: 0.9762 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.2227 train_acc: 0.9876 val_loss: 0.2329 val_acc: 0.9827 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.2211 train_acc: 0.9881 val_loss: 0.2526 val_acc: 0.9709 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2195 train_acc: 0.9888 val_loss: 0.2482 val_acc: 0.9723 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2180 train_acc: 0.9902 val_loss: 0.2441 val_acc: 0.9745 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2171 train_acc: 0.9904 val_loss: 0.2439 val_acc: 0.9757 lr: 0.000625\n",
      "Epoch  20 / 200 train_loss: 0.2126 train_acc: 0.9931 val_loss: 0.2264 val_acc: 0.9830 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.2089 train_acc: 0.9955 val_loss: 0.2304 val_acc: 0.9831 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.2079 train_acc: 0.9963 val_loss: 0.2262 val_acc: 0.9845 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.2073 train_acc: 0.9965 val_loss: 0.2273 val_acc: 0.9837 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2084 train_acc: 0.9959 val_loss: 0.2326 val_acc: 0.9811 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2058 train_acc: 0.9973 val_loss: 0.2335 val_acc: 0.9806 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2056 train_acc: 0.9971 val_loss: 0.2268 val_acc: 0.9833 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.2042 train_acc: 0.9981 val_loss: 0.2279 val_acc: 0.9835 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.2043 train_acc: 0.9977 val_loss: 0.2291 val_acc: 0.9838 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2037 train_acc: 0.9981 val_loss: 0.2262 val_acc: 0.9844 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2040 train_acc: 0.9981 val_loss: 0.2263 val_acc: 0.9847 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2028 train_acc: 0.9987 val_loss: 0.2277 val_acc: 0.9845 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2024 train_acc: 0.9990 val_loss: 0.2312 val_acc: 0.9825 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2028 train_acc: 0.9985 val_loss: 0.2288 val_acc: 0.9831 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2028 train_acc: 0.9985 val_loss: 0.2269 val_acc: 0.9844 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2020 train_acc: 0.9991 val_loss: 0.2268 val_acc: 0.9838 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2019 train_acc: 0.9991 val_loss: 0.2272 val_acc: 0.9840 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2019 train_acc: 0.9989 val_loss: 0.2261 val_acc: 0.9855 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2015 train_acc: 0.9992 val_loss: 0.2262 val_acc: 0.9847 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2012 train_acc: 0.9993 val_loss: 0.2269 val_acc: 0.9857 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2011 train_acc: 0.9995 val_loss: 0.2266 val_acc: 0.9845 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2011 train_acc: 0.9994 val_loss: 0.2294 val_acc: 0.9827 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.2011 train_acc: 0.9993 val_loss: 0.2268 val_acc: 0.9858 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2016 train_acc: 0.9990 val_loss: 0.2267 val_acc: 0.9845 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2009 train_acc: 0.9994 val_loss: 0.2261 val_acc: 0.9844 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.2260 val_acc: 0.9862 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.2006 train_acc: 0.9996 val_loss: 0.2281 val_acc: 0.9845 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.2007 train_acc: 0.9996 val_loss: 0.2277 val_acc: 0.9849 lr: 0.00015625\n",
      "Epoch  48 / 200 train_loss: 0.2007 train_acc: 0.9995 val_loss: 0.2277 val_acc: 0.9844 lr: 0.00015625\n",
      "Epoch  49 / 200 train_loss: 0.2008 train_acc: 0.9994 val_loss: 0.2316 val_acc: 0.9825 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.2007 train_acc: 0.9995 val_loss: 0.2261 val_acc: 0.9852 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.2004 train_acc: 0.9997 val_loss: 0.2267 val_acc: 0.9858 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.2002 train_acc: 0.9996 val_loss: 0.2276 val_acc: 0.9836 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2274 val_acc: 0.9849 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2004 train_acc: 0.9996 val_loss: 0.2274 val_acc: 0.9854 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2268 val_acc: 0.9848 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2002 train_acc: 0.9997 val_loss: 0.2273 val_acc: 0.9844 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2002 train_acc: 0.9997 val_loss: 0.2272 val_acc: 0.9851 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2002 train_acc: 0.9996 val_loss: 0.2256 val_acc: 0.9858 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2004 train_acc: 0.9996 val_loss: 0.2262 val_acc: 0.9852 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2256 val_acc: 0.9861 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2002 train_acc: 0.9997 val_loss: 0.2256 val_acc: 0.9857 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2002 train_acc: 0.9996 val_loss: 0.2264 val_acc: 0.9858 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2003 train_acc: 0.9997 val_loss: 0.2267 val_acc: 0.9855 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2004 train_acc: 0.9996 val_loss: 0.2271 val_acc: 0.9855 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2001 train_acc: 0.9996 val_loss: 0.2266 val_acc: 0.9847 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2001 train_acc: 0.9996 val_loss: 0.2266 val_acc: 0.9854 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.1999 train_acc: 0.9999 val_loss: 0.2273 val_acc: 0.9842 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.1999 train_acc: 0.9998 val_loss: 0.2282 val_acc: 0.9841 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.1999 train_acc: 0.9998 val_loss: 0.2286 val_acc: 0.9848 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2001 train_acc: 0.9997 val_loss: 0.2285 val_acc: 0.9851 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2285 val_acc: 0.9847 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2001 train_acc: 0.9996 val_loss: 0.2278 val_acc: 0.9851 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.1999 train_acc: 0.9998 val_loss: 0.2276 val_acc: 0.9841 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.1998 train_acc: 0.9999 val_loss: 0.2272 val_acc: 0.9858 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.1999 train_acc: 0.9998 val_loss: 0.2280 val_acc: 0.9851 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9802\n",
      "14000: 0.9801666666666666\n",
      " test_acc: 0.9802\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5390 train_acc: 0.8421 val_loss: 0.4335 val_acc: 0.8512 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3493 train_acc: 0.9087 val_loss: 0.5708 val_acc: 0.7507 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3281 train_acc: 0.9238 val_loss: 0.3832 val_acc: 0.8800 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3146 train_acc: 0.9315 val_loss: 0.3561 val_acc: 0.8998 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.3022 train_acc: 0.9391 val_loss: 0.2978 val_acc: 0.9398 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.2960 train_acc: 0.9427 val_loss: 0.3472 val_acc: 0.9012 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.2910 train_acc: 0.9456 val_loss: 0.3353 val_acc: 0.9121 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.2867 train_acc: 0.9491 val_loss: 0.4910 val_acc: 0.8116 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.2792 train_acc: 0.9542 val_loss: 0.5847 val_acc: 0.7670 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.2627 train_acc: 0.9632 val_loss: 0.3672 val_acc: 0.8998 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.2563 train_acc: 0.9673 val_loss: 0.5577 val_acc: 0.7595 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2554 train_acc: 0.9686 val_loss: 0.2663 val_acc: 0.9621 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2510 train_acc: 0.9712 val_loss: 0.5678 val_acc: 0.7683 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2481 train_acc: 0.9720 val_loss: 0.3854 val_acc: 0.8779 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2447 train_acc: 0.9756 val_loss: 0.3930 val_acc: 0.8732 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2413 train_acc: 0.9770 val_loss: 0.3993 val_acc: 0.8904 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.2306 train_acc: 0.9832 val_loss: 0.2683 val_acc: 0.9556 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2256 train_acc: 0.9864 val_loss: 0.2614 val_acc: 0.9627 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2256 train_acc: 0.9866 val_loss: 0.2928 val_acc: 0.9478 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2208 train_acc: 0.9897 val_loss: 0.3180 val_acc: 0.9330 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.2182 train_acc: 0.9910 val_loss: 0.2723 val_acc: 0.9567 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2179 train_acc: 0.9914 val_loss: 0.2743 val_acc: 0.9563 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.2134 train_acc: 0.9937 val_loss: 0.2649 val_acc: 0.9651 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.2113 train_acc: 0.9954 val_loss: 0.2661 val_acc: 0.9641 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2108 train_acc: 0.9956 val_loss: 0.2682 val_acc: 0.9598 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2093 train_acc: 0.9962 val_loss: 0.2695 val_acc: 0.9598 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2088 train_acc: 0.9965 val_loss: 0.2711 val_acc: 0.9585 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.2065 train_acc: 0.9980 val_loss: 0.2660 val_acc: 0.9637 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.2059 train_acc: 0.9981 val_loss: 0.2639 val_acc: 0.9644 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2067 train_acc: 0.9977 val_loss: 0.2675 val_acc: 0.9638 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2053 train_acc: 0.9985 val_loss: 0.2646 val_acc: 0.9639 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.2047 train_acc: 0.9986 val_loss: 0.2645 val_acc: 0.9635 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.2043 train_acc: 0.9988 val_loss: 0.2644 val_acc: 0.9652 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.2040 train_acc: 0.9992 val_loss: 0.2665 val_acc: 0.9634 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2038 train_acc: 0.9989 val_loss: 0.2682 val_acc: 0.9611 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2039 train_acc: 0.9988 val_loss: 0.2659 val_acc: 0.9625 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2033 train_acc: 0.9992 val_loss: 0.2680 val_acc: 0.9632 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.2034 train_acc: 0.9993 val_loss: 0.2661 val_acc: 0.9654 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.2034 train_acc: 0.9991 val_loss: 0.2656 val_acc: 0.9628 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.2027 train_acc: 0.9994 val_loss: 0.2663 val_acc: 0.9625 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.2028 train_acc: 0.9993 val_loss: 0.2654 val_acc: 0.9631 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2026 train_acc: 0.9996 val_loss: 0.2666 val_acc: 0.9640 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2028 train_acc: 0.9993 val_loss: 0.2652 val_acc: 0.9645 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2027 train_acc: 0.9993 val_loss: 0.2654 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2030 train_acc: 0.9994 val_loss: 0.2667 val_acc: 0.9647 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2024 train_acc: 0.9996 val_loss: 0.2651 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2024 train_acc: 0.9995 val_loss: 0.2658 val_acc: 0.9646 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2025 train_acc: 0.9993 val_loss: 0.2652 val_acc: 0.9641 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2022 train_acc: 0.9997 val_loss: 0.2659 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2023 train_acc: 0.9995 val_loss: 0.2652 val_acc: 0.9642 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2025 train_acc: 0.9994 val_loss: 0.2658 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2021 train_acc: 0.9996 val_loss: 0.2655 val_acc: 0.9641 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2019 train_acc: 0.9997 val_loss: 0.2649 val_acc: 0.9648 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2025 train_acc: 0.9993 val_loss: 0.2669 val_acc: 0.9652 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2022 train_acc: 0.9996 val_loss: 0.2669 val_acc: 0.9621 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2021 train_acc: 0.9997 val_loss: 0.2666 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2021 train_acc: 0.9996 val_loss: 0.2652 val_acc: 0.9651 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2019 train_acc: 0.9998 val_loss: 0.2663 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2026 train_acc: 0.9994 val_loss: 0.2670 val_acc: 0.9618 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2021 train_acc: 0.9995 val_loss: 0.2665 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2021 train_acc: 0.9995 val_loss: 0.2649 val_acc: 0.9649 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2021 train_acc: 0.9996 val_loss: 0.2656 val_acc: 0.9638 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2020 train_acc: 0.9995 val_loss: 0.2651 val_acc: 0.9642 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2022 train_acc: 0.9995 val_loss: 0.2656 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2017 train_acc: 0.9996 val_loss: 0.2648 val_acc: 0.9642 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2024 train_acc: 0.9991 val_loss: 0.2658 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2017 train_acc: 0.9996 val_loss: 0.2650 val_acc: 0.9649 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2017 train_acc: 0.9996 val_loss: 0.2655 val_acc: 0.9632 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9615\n",
      "14000: 0.9615\n",
      " test_acc: 0.9615\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5087 train_acc: 0.8622 val_loss: 0.6238 val_acc: 0.7473 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3272 train_acc: 0.9224 val_loss: 0.3320 val_acc: 0.9168 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3057 train_acc: 0.9390 val_loss: 0.5452 val_acc: 0.7716 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.2936 train_acc: 0.9453 val_loss: 0.3103 val_acc: 0.9279 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.2840 train_acc: 0.9504 val_loss: 0.9550 val_acc: 0.5769 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.2779 train_acc: 0.9548 val_loss: 0.6825 val_acc: 0.7301 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.2726 train_acc: 0.9564 val_loss: 0.3760 val_acc: 0.8792 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.2647 train_acc: 0.9611 val_loss: 0.7494 val_acc: 0.7279 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.2499 train_acc: 0.9713 val_loss: 0.4007 val_acc: 0.8742 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.2436 train_acc: 0.9749 val_loss: 0.2622 val_acc: 0.9591 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.2428 train_acc: 0.9745 val_loss: 0.4140 val_acc: 0.8660 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2389 train_acc: 0.9761 val_loss: 0.2916 val_acc: 0.9471 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2364 train_acc: 0.9783 val_loss: 0.2647 val_acc: 0.9590 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2343 train_acc: 0.9805 val_loss: 0.5905 val_acc: 0.7936 lr: 0.00125\n",
      "Epoch  14 / 200 train_loss: 0.2244 train_acc: 0.9862 val_loss: 0.2614 val_acc: 0.9618 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.2202 train_acc: 0.9884 val_loss: 0.2976 val_acc: 0.9419 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.2186 train_acc: 0.9891 val_loss: 0.5079 val_acc: 0.8309 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2166 train_acc: 0.9909 val_loss: 0.2919 val_acc: 0.9468 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2171 train_acc: 0.9905 val_loss: 0.2666 val_acc: 0.9628 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2151 train_acc: 0.9916 val_loss: 0.4782 val_acc: 0.8619 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.2120 train_acc: 0.9938 val_loss: 0.4339 val_acc: 0.8796 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2109 train_acc: 0.9947 val_loss: 0.2818 val_acc: 0.9539 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.2111 train_acc: 0.9941 val_loss: 0.2703 val_acc: 0.9642 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2105 train_acc: 0.9944 val_loss: 0.2783 val_acc: 0.9534 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.2100 train_acc: 0.9948 val_loss: 0.4416 val_acc: 0.8824 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.2083 train_acc: 0.9956 val_loss: 0.2696 val_acc: 0.9614 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.2083 train_acc: 0.9956 val_loss: 0.5451 val_acc: 0.8398 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2052 train_acc: 0.9974 val_loss: 0.2822 val_acc: 0.9540 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2035 train_acc: 0.9985 val_loss: 0.2641 val_acc: 0.9658 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2030 train_acc: 0.9985 val_loss: 0.2636 val_acc: 0.9648 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.2026 train_acc: 0.9988 val_loss: 0.2778 val_acc: 0.9596 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.2031 train_acc: 0.9985 val_loss: 0.2700 val_acc: 0.9621 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.2017 train_acc: 0.9993 val_loss: 0.2698 val_acc: 0.9631 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2012 train_acc: 0.9995 val_loss: 0.2611 val_acc: 0.9678 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.3034 val_acc: 0.9476 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2013 train_acc: 0.9994 val_loss: 0.2746 val_acc: 0.9601 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2010 train_acc: 0.9993 val_loss: 0.2697 val_acc: 0.9614 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2010 train_acc: 0.9993 val_loss: 0.2599 val_acc: 0.9649 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2005 train_acc: 0.9996 val_loss: 0.2595 val_acc: 0.9665 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2002 train_acc: 0.9998 val_loss: 0.2596 val_acc: 0.9673 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2003 train_acc: 0.9996 val_loss: 0.2609 val_acc: 0.9668 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2005 train_acc: 0.9995 val_loss: 0.2608 val_acc: 0.9685 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.2002 train_acc: 0.9998 val_loss: 0.2598 val_acc: 0.9676 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2601 val_acc: 0.9654 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2001 train_acc: 0.9997 val_loss: 0.2597 val_acc: 0.9666 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2613 val_acc: 0.9661 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2647 val_acc: 0.9639 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2581 val_acc: 0.9673 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2578 val_acc: 0.9676 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2570 val_acc: 0.9688 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2602 val_acc: 0.9678 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2580 val_acc: 0.9685 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2592 val_acc: 0.9690 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2593 val_acc: 0.9681 lr: 7.8125e-05\n",
      "Epoch  54 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2579 val_acc: 0.9683 lr: 7.8125e-05\n",
      "Epoch  55 / 200 train_loss: 0.1998 train_acc: 0.9999 val_loss: 0.2611 val_acc: 0.9662 lr: 7.8125e-05\n",
      "Epoch  56 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2587 val_acc: 0.9685 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.1998 train_acc: 0.9997 val_loss: 0.2581 val_acc: 0.9681 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2587 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2573 val_acc: 0.9689 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2571 val_acc: 0.9683 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2568 val_acc: 0.9683 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.1996 train_acc: 1.0000 val_loss: 0.2576 val_acc: 0.9682 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.1997 train_acc: 0.9997 val_loss: 0.2594 val_acc: 0.9673 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2575 val_acc: 0.9679 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2583 val_acc: 0.9695 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2600 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.1995 train_acc: 0.9998 val_loss: 0.2588 val_acc: 0.9685 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.1995 train_acc: 0.9998 val_loss: 0.2584 val_acc: 0.9678 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.1996 train_acc: 0.9998 val_loss: 0.2578 val_acc: 0.9682 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2597 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.1996 train_acc: 0.9998 val_loss: 0.2622 val_acc: 0.9657 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2589 val_acc: 0.9670 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2598 val_acc: 0.9672 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2616 val_acc: 0.9655 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2606 val_acc: 0.9659 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.1996 train_acc: 0.9998 val_loss: 0.2604 val_acc: 0.9662 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2596 val_acc: 0.9671 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2590 val_acc: 0.9676 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2588 val_acc: 0.9672 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.1995 train_acc: 0.9998 val_loss: 0.2627 val_acc: 0.9665 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2601 val_acc: 0.9672 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2610 val_acc: 0.9662 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2596 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.1994 train_acc: 0.9998 val_loss: 0.2602 val_acc: 0.9670 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2596 val_acc: 0.9683 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.1993 train_acc: 0.9999 val_loss: 0.2599 val_acc: 0.9682 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2611 val_acc: 0.9672 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2610 val_acc: 0.9657 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.1993 train_acc: 0.9999 val_loss: 0.2648 val_acc: 0.9663 lr: 5e-05\n",
      "Epoch  90 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2599 val_acc: 0.9692 lr: 5e-05\n",
      "Epoch  91 / 200 train_loss: 0.1992 train_acc: 1.0000 val_loss: 0.2597 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  92 / 200 train_loss: 0.1993 train_acc: 0.9998 val_loss: 0.2590 val_acc: 0.9682 lr: 5e-05\n",
      "Epoch  93 / 200 train_loss: 0.1993 train_acc: 0.9999 val_loss: 0.2611 val_acc: 0.9677 lr: 5e-05\n",
      "Epoch  94 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2611 val_acc: 0.9666 lr: 5e-05\n",
      "Epoch  95 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2622 val_acc: 0.9679 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9727\n",
      "14000: 0.9726666666666667\n",
      " test_acc: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c18</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c02</td>\n",
       "      <td>0.957833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c03</td>\n",
       "      <td>0.757333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c04</td>\n",
       "      <td>0.794333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c05</td>\n",
       "      <td>0.910556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c06</td>\n",
       "      <td>0.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c07</td>\n",
       "      <td>0.819833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c08</td>\n",
       "      <td>0.719167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c09</td>\n",
       "      <td>0.801833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c10</td>\n",
       "      <td>0.845667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c11</td>\n",
       "      <td>0.786833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c12</td>\n",
       "      <td>0.835167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c15</td>\n",
       "      <td>0.937833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c16</td>\n",
       "      <td>0.980167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>c17</td>\n",
       "      <td>0.961500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c18</td>\n",
       "      <td>0.972667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c18       acc\n",
       "0   c02  0.957833\n",
       "1   c03  0.757333\n",
       "2   c04  0.794333\n",
       "3   c05  0.910556\n",
       "4   c06  0.756800\n",
       "5   c07  0.819833\n",
       "6   c08  0.719167\n",
       "7   c09  0.801833\n",
       "8   c10  0.845667\n",
       "9   c11  0.786833\n",
       "10  c12  0.835167\n",
       "11  c15  0.937833\n",
       "12  c16  0.980167\n",
       "13  c17  0.961500\n",
       "14  c18  0.972667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = []\n",
    "for task_name, task_peps in all_task_dict.items():\n",
    "    torch.set_num_threads(10)\n",
    "    \n",
    "    all_peps = task_peps\n",
    "    \n",
    "    y_code_dict = nps.ml.set_y_codes_for_classes(np.array(all_peps)[:,None])\n",
    "    y_to_label_dict = {v:k for k,v in y_code_dict.items()}\n",
    "\n",
    "    train_objs = [f\"../../../03.results/classification_on_clean_data/GSXGS/{pep}/{pep}_valid80_clean_obj.pkl\" for pep in all_peps]\n",
    "    test_objs = [f\"../../../03.results/classification_on_clean_data/GSXGS/{pep}/{pep}_valid20_clean_obj.pkl\" for pep in all_peps]\n",
    "    labels = all_peps\n",
    "\n",
    "    acc = train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name=f'clean_data_{task_name}', train_sample_size=14000)\n",
    "\n",
    "    acc_df.append([task_name, acc])\n",
    "    \n",
    "acc_df = pd.DataFrame(acc_df)\n",
    "acc_df.columns = [task_name, 'acc']\n",
    "acc_df.to_csv(\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/clean/acc.csv\")\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586cd0f-795f-4e7e-807e-71de5779b00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
