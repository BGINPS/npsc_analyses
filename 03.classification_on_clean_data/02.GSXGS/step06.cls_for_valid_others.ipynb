{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/user/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd83f66-0acd-4695-9851-37fbc36acbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_task_dict = {\n",
    "    'c02': ['1S', '1pS'],\n",
    "    'c03': ['1S', '1SMe', '3SMe'],\n",
    "    'c04': ['1S', '1SAc', '3SAc'],\n",
    "    'c05': ['1S', '1Soct', '3Soct'],\n",
    "    'c06': ['1S', '1pS', '1SMe', '1SAc', '1Soct'],\n",
    "    'c07': ['1S', '3SMe', '3SAc', '3Soct'],\n",
    "    'c08': ['1I', '1L'],\n",
    "    'c09': ['3I', '3L'],\n",
    "    'c10': ['3dI', '3dL'],\n",
    "    'c11': ['3I', '3dI'],\n",
    "    'c12': ['3L', '3dL'],\n",
    "    'c15': ['1D', '1D02'],\n",
    "    'c16': ['1R', '1R02'],\n",
    "    'c17': ['1Y', '1Y02'],\n",
    "    'c18': ['1W', '1W02'],\n",
    "}\n",
    "\n",
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df\n",
    "\n",
    "def train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name='clean_data', train_sample_size=14000):\n",
    "    # 读取pkl文件，生成readid，X，y组成的df\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=42)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=42, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=3000, random_state=42)\n",
    "\n",
    "    # 通过data_df构建dataloader\n",
    "    batch_size = 64\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    nps.ml.seed_everything(42)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(all_peps), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    # pred\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    test_all_reads_s = pred_df['true'].value_counts()\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=all_peps)\n",
    "    cm_df.to_csv(f\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/valid/{train_name}_cm.csv\")\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/valid/{train_name}_pred_proba.csv\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.4940 train_acc: 0.8600 val_loss: 0.6903 val_acc: 0.6699 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3363 train_acc: 0.9099 val_loss: 0.4484 val_acc: 0.8382 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3258 train_acc: 0.9173 val_loss: 0.3495 val_acc: 0.8976 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3211 train_acc: 0.9216 val_loss: 0.3126 val_acc: 0.9241 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.3158 train_acc: 0.9216 val_loss: 0.3411 val_acc: 0.9118 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.3128 train_acc: 0.9241 val_loss: 0.3184 val_acc: 0.9188 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.3096 train_acc: 0.9251 val_loss: 0.3207 val_acc: 0.9165 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.3050 train_acc: 0.9273 val_loss: 0.4905 val_acc: 0.7842 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.2923 train_acc: 0.9346 val_loss: 0.3145 val_acc: 0.9193 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.2878 train_acc: 0.9391 val_loss: 0.2964 val_acc: 0.9313 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.2862 train_acc: 0.9410 val_loss: 0.3483 val_acc: 0.8999 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2857 train_acc: 0.9393 val_loss: 0.3228 val_acc: 0.9164 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2811 train_acc: 0.9442 val_loss: 0.2933 val_acc: 0.9314 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2802 train_acc: 0.9442 val_loss: 0.3769 val_acc: 0.8851 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2785 train_acc: 0.9444 val_loss: 0.3280 val_acc: 0.9122 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2757 train_acc: 0.9484 val_loss: 0.3337 val_acc: 0.9128 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.2722 train_acc: 0.9509 val_loss: 0.3049 val_acc: 0.9280 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2618 train_acc: 0.9563 val_loss: 0.2913 val_acc: 0.9341 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2574 train_acc: 0.9606 val_loss: 0.2972 val_acc: 0.9292 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2530 train_acc: 0.9654 val_loss: 0.3076 val_acc: 0.9344 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.2493 train_acc: 0.9668 val_loss: 0.3084 val_acc: 0.9308 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2477 train_acc: 0.9685 val_loss: 0.3050 val_acc: 0.9334 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.2430 train_acc: 0.9731 val_loss: 0.3035 val_acc: 0.9323 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2389 train_acc: 0.9755 val_loss: 0.3049 val_acc: 0.9324 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2300 train_acc: 0.9817 val_loss: 0.3071 val_acc: 0.9361 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2248 train_acc: 0.9854 val_loss: 0.3123 val_acc: 0.9361 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2223 train_acc: 0.9869 val_loss: 0.3134 val_acc: 0.9344 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2190 train_acc: 0.9896 val_loss: 0.3164 val_acc: 0.9337 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2164 train_acc: 0.9917 val_loss: 0.3224 val_acc: 0.9294 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2119 train_acc: 0.9944 val_loss: 0.3211 val_acc: 0.9370 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2099 train_acc: 0.9958 val_loss: 0.3231 val_acc: 0.9345 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2096 train_acc: 0.9958 val_loss: 0.3229 val_acc: 0.9378 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2077 train_acc: 0.9971 val_loss: 0.3249 val_acc: 0.9342 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2070 train_acc: 0.9973 val_loss: 0.3233 val_acc: 0.9344 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2061 train_acc: 0.9975 val_loss: 0.3239 val_acc: 0.9334 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2057 train_acc: 0.9981 val_loss: 0.3260 val_acc: 0.9350 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2047 train_acc: 0.9983 val_loss: 0.3251 val_acc: 0.9365 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2041 train_acc: 0.9984 val_loss: 0.3218 val_acc: 0.9344 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2039 train_acc: 0.9986 val_loss: 0.3217 val_acc: 0.9377 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2037 train_acc: 0.9987 val_loss: 0.3255 val_acc: 0.9341 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.2033 train_acc: 0.9989 val_loss: 0.3239 val_acc: 0.9360 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2030 train_acc: 0.9991 val_loss: 0.3220 val_acc: 0.9354 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2034 train_acc: 0.9989 val_loss: 0.3241 val_acc: 0.9354 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.2030 train_acc: 0.9990 val_loss: 0.3246 val_acc: 0.9344 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2026 train_acc: 0.9993 val_loss: 0.3246 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2023 train_acc: 0.9993 val_loss: 0.3248 val_acc: 0.9364 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2026 train_acc: 0.9991 val_loss: 0.3242 val_acc: 0.9336 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2024 train_acc: 0.9993 val_loss: 0.3253 val_acc: 0.9360 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2024 train_acc: 0.9993 val_loss: 0.3240 val_acc: 0.9342 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2024 train_acc: 0.9989 val_loss: 0.3244 val_acc: 0.9344 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2021 train_acc: 0.9993 val_loss: 0.3255 val_acc: 0.9363 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2020 train_acc: 0.9995 val_loss: 0.3259 val_acc: 0.9336 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2020 train_acc: 0.9992 val_loss: 0.3253 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2022 train_acc: 0.9992 val_loss: 0.3227 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2019 train_acc: 0.9995 val_loss: 0.3256 val_acc: 0.9342 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2020 train_acc: 0.9992 val_loss: 0.3247 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2021 train_acc: 0.9992 val_loss: 0.3251 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2015 train_acc: 0.9995 val_loss: 0.3253 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2017 train_acc: 0.9993 val_loss: 0.3245 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2015 train_acc: 0.9996 val_loss: 0.3260 val_acc: 0.9344 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2017 train_acc: 0.9993 val_loss: 0.3228 val_acc: 0.9334 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2016 train_acc: 0.9994 val_loss: 0.3238 val_acc: 0.9342 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9350\n",
      "14000: 0.935\n",
      " test_acc: 0.9350\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.0978 train_acc: 0.4976 val_loss: 1.0890 val_acc: 0.4597 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.9338 train_acc: 0.5735 val_loss: 0.9142 val_acc: 0.5979 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8998 train_acc: 0.6037 val_loss: 1.0681 val_acc: 0.4765 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8823 train_acc: 0.6177 val_loss: 1.0826 val_acc: 0.5085 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8660 train_acc: 0.6321 val_loss: 0.8776 val_acc: 0.6175 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8501 train_acc: 0.6430 val_loss: 1.1666 val_acc: 0.4516 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.8380 train_acc: 0.6518 val_loss: 0.8599 val_acc: 0.6322 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.8248 train_acc: 0.6631 val_loss: 0.8392 val_acc: 0.6421 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.8131 train_acc: 0.6700 val_loss: 1.0599 val_acc: 0.4880 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7993 train_acc: 0.6819 val_loss: 1.0097 val_acc: 0.5175 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7865 train_acc: 0.6940 val_loss: 0.8297 val_acc: 0.6660 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7758 train_acc: 0.7012 val_loss: 1.1760 val_acc: 0.4872 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.7667 train_acc: 0.7074 val_loss: 0.8901 val_acc: 0.6197 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.7545 train_acc: 0.7157 val_loss: 1.2439 val_acc: 0.4874 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.7438 train_acc: 0.7214 val_loss: 1.4412 val_acc: 0.4014 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.7061 train_acc: 0.7477 val_loss: 0.9086 val_acc: 0.6158 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6927 train_acc: 0.7585 val_loss: 0.8365 val_acc: 0.6696 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6790 train_acc: 0.7678 val_loss: 0.9634 val_acc: 0.5941 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.6656 train_acc: 0.7778 val_loss: 0.7621 val_acc: 0.7161 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.6559 train_acc: 0.7832 val_loss: 1.0249 val_acc: 0.5646 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6463 train_acc: 0.7904 val_loss: 0.8733 val_acc: 0.6525 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6286 train_acc: 0.8001 val_loss: 1.0123 val_acc: 0.6003 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6168 train_acc: 0.8100 val_loss: 0.8592 val_acc: 0.6758 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.5780 train_acc: 0.8347 val_loss: 0.8613 val_acc: 0.6753 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.5579 train_acc: 0.8489 val_loss: 0.7836 val_acc: 0.7076 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.5439 train_acc: 0.8577 val_loss: 0.7686 val_acc: 0.7186 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.5329 train_acc: 0.8637 val_loss: 0.8180 val_acc: 0.7044 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.5218 train_acc: 0.8733 val_loss: 0.7724 val_acc: 0.7302 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.5083 train_acc: 0.8802 val_loss: 0.7949 val_acc: 0.7210 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.4987 train_acc: 0.8874 val_loss: 0.8826 val_acc: 0.6849 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.4840 train_acc: 0.8973 val_loss: 0.8070 val_acc: 0.7236 lr: 0.00125\n",
      "Epoch  31 / 200 train_loss: 0.4757 train_acc: 0.9029 val_loss: 0.8048 val_acc: 0.7255 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.4533 train_acc: 0.9157 val_loss: 0.8326 val_acc: 0.7154 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.4426 train_acc: 0.9239 val_loss: 0.8258 val_acc: 0.7212 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.4354 train_acc: 0.9279 val_loss: 0.8232 val_acc: 0.7178 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.4292 train_acc: 0.9315 val_loss: 0.8669 val_acc: 0.7054 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.4174 train_acc: 0.9389 val_loss: 0.8641 val_acc: 0.7056 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.4128 train_acc: 0.9435 val_loss: 0.8468 val_acc: 0.7114 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.4071 train_acc: 0.9474 val_loss: 0.8339 val_acc: 0.7248 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.4035 train_acc: 0.9477 val_loss: 0.8435 val_acc: 0.7210 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.3966 train_acc: 0.9529 val_loss: 0.8432 val_acc: 0.7155 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.3947 train_acc: 0.9541 val_loss: 0.8444 val_acc: 0.7191 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.3944 train_acc: 0.9539 val_loss: 0.8356 val_acc: 0.7204 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.3933 train_acc: 0.9560 val_loss: 0.8353 val_acc: 0.7229 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.3917 train_acc: 0.9561 val_loss: 0.8493 val_acc: 0.7137 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.3885 train_acc: 0.9586 val_loss: 0.8566 val_acc: 0.7146 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.3890 train_acc: 0.9580 val_loss: 0.8523 val_acc: 0.7167 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.3863 train_acc: 0.9598 val_loss: 0.8491 val_acc: 0.7131 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3842 train_acc: 0.9607 val_loss: 0.8617 val_acc: 0.7080 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3859 train_acc: 0.9597 val_loss: 0.8477 val_acc: 0.7142 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3838 train_acc: 0.9618 val_loss: 0.8404 val_acc: 0.7204 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3850 train_acc: 0.9603 val_loss: 0.8433 val_acc: 0.7210 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3883 train_acc: 0.9586 val_loss: 0.8494 val_acc: 0.7155 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3827 train_acc: 0.9620 val_loss: 0.8562 val_acc: 0.7110 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3815 train_acc: 0.9629 val_loss: 0.8444 val_acc: 0.7123 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3839 train_acc: 0.9607 val_loss: 0.8472 val_acc: 0.7199 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3789 train_acc: 0.9640 val_loss: 0.8485 val_acc: 0.7199 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3797 train_acc: 0.9636 val_loss: 0.8490 val_acc: 0.7203 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7297\n",
      "14000: 0.7296666666666667\n",
      " test_acc: 0.7297\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.0192 train_acc: 0.5555 val_loss: 1.0583 val_acc: 0.4900 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.8580 train_acc: 0.6351 val_loss: 1.1293 val_acc: 0.4000 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8251 train_acc: 0.6628 val_loss: 0.9082 val_acc: 0.5804 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.7936 train_acc: 0.6877 val_loss: 0.9424 val_acc: 0.5717 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.7768 train_acc: 0.6999 val_loss: 1.0631 val_acc: 0.5597 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7602 train_acc: 0.7100 val_loss: 0.9698 val_acc: 0.5902 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7431 train_acc: 0.7222 val_loss: 0.8425 val_acc: 0.6544 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7319 train_acc: 0.7300 val_loss: 1.2856 val_acc: 0.4633 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7188 train_acc: 0.7408 val_loss: 1.1124 val_acc: 0.4819 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7038 train_acc: 0.7532 val_loss: 0.9180 val_acc: 0.5815 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.6937 train_acc: 0.7576 val_loss: 0.8529 val_acc: 0.6536 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.6571 train_acc: 0.7821 val_loss: 0.7251 val_acc: 0.7336 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6409 train_acc: 0.7944 val_loss: 1.0015 val_acc: 0.6214 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6286 train_acc: 0.8025 val_loss: 0.7151 val_acc: 0.7408 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6175 train_acc: 0.8095 val_loss: 1.0858 val_acc: 0.5928 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6057 train_acc: 0.8186 val_loss: 0.7145 val_acc: 0.7489 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.5895 train_acc: 0.8290 val_loss: 1.2481 val_acc: 0.5361 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.5792 train_acc: 0.8380 val_loss: 0.7463 val_acc: 0.7302 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.5662 train_acc: 0.8434 val_loss: 0.7393 val_acc: 0.7442 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.5433 train_acc: 0.8587 val_loss: 1.1813 val_acc: 0.5527 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5066 train_acc: 0.8808 val_loss: 0.7069 val_acc: 0.7688 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.4889 train_acc: 0.8949 val_loss: 0.6987 val_acc: 0.7705 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.4734 train_acc: 0.9068 val_loss: 1.1127 val_acc: 0.6092 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.4593 train_acc: 0.9167 val_loss: 0.7132 val_acc: 0.7720 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.4530 train_acc: 0.9183 val_loss: 0.7185 val_acc: 0.7566 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.4369 train_acc: 0.9314 val_loss: 0.7176 val_acc: 0.7743 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.4316 train_acc: 0.9349 val_loss: 0.7209 val_acc: 0.7748 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.4211 train_acc: 0.9415 val_loss: 0.7297 val_acc: 0.7592 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.4139 train_acc: 0.9466 val_loss: 0.8610 val_acc: 0.7174 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.4037 train_acc: 0.9529 val_loss: 0.7919 val_acc: 0.7372 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.3967 train_acc: 0.9582 val_loss: 0.7266 val_acc: 0.7724 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.3808 train_acc: 0.9676 val_loss: 0.7565 val_acc: 0.7566 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.3766 train_acc: 0.9698 val_loss: 0.8140 val_acc: 0.7372 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.3692 train_acc: 0.9752 val_loss: 0.7764 val_acc: 0.7523 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.3647 train_acc: 0.9770 val_loss: 0.7382 val_acc: 0.7705 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.3567 train_acc: 0.9824 val_loss: 0.7344 val_acc: 0.7705 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.3564 train_acc: 0.9825 val_loss: 0.7270 val_acc: 0.7722 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.3535 train_acc: 0.9846 val_loss: 0.7647 val_acc: 0.7573 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.3509 train_acc: 0.9856 val_loss: 0.7391 val_acc: 0.7681 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.3477 train_acc: 0.9869 val_loss: 0.7200 val_acc: 0.7765 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.3476 train_acc: 0.9865 val_loss: 0.7462 val_acc: 0.7641 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.3480 train_acc: 0.9874 val_loss: 0.7290 val_acc: 0.7677 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.3457 train_acc: 0.9880 val_loss: 0.7294 val_acc: 0.7741 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.3427 train_acc: 0.9894 val_loss: 0.7512 val_acc: 0.7626 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.3439 train_acc: 0.9894 val_loss: 0.7202 val_acc: 0.7767 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.3429 train_acc: 0.9900 val_loss: 0.7216 val_acc: 0.7799 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.3427 train_acc: 0.9899 val_loss: 0.7193 val_acc: 0.7756 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.3436 train_acc: 0.9886 val_loss: 0.7221 val_acc: 0.7726 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.3414 train_acc: 0.9901 val_loss: 0.7207 val_acc: 0.7784 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.3405 train_acc: 0.9905 val_loss: 0.7235 val_acc: 0.7779 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3401 train_acc: 0.9904 val_loss: 0.7225 val_acc: 0.7767 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3401 train_acc: 0.9904 val_loss: 0.7217 val_acc: 0.7745 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3400 train_acc: 0.9905 val_loss: 0.7257 val_acc: 0.7669 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3391 train_acc: 0.9911 val_loss: 0.7220 val_acc: 0.7730 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3388 train_acc: 0.9912 val_loss: 0.7226 val_acc: 0.7730 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3378 train_acc: 0.9920 val_loss: 0.7223 val_acc: 0.7722 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3395 train_acc: 0.9904 val_loss: 0.7206 val_acc: 0.7762 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3392 train_acc: 0.9909 val_loss: 0.7213 val_acc: 0.7782 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3380 train_acc: 0.9915 val_loss: 0.7188 val_acc: 0.7739 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3385 train_acc: 0.9916 val_loss: 0.7224 val_acc: 0.7775 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3379 train_acc: 0.9917 val_loss: 0.7347 val_acc: 0.7636 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3377 train_acc: 0.9915 val_loss: 0.7226 val_acc: 0.7762 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3363 train_acc: 0.9926 val_loss: 0.7199 val_acc: 0.7769 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3368 train_acc: 0.9918 val_loss: 0.7195 val_acc: 0.7780 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3370 train_acc: 0.9916 val_loss: 0.7290 val_acc: 0.7694 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3362 train_acc: 0.9917 val_loss: 0.7411 val_acc: 0.7639 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3367 train_acc: 0.9918 val_loss: 0.7250 val_acc: 0.7733 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3359 train_acc: 0.9926 val_loss: 0.7285 val_acc: 0.7694 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3366 train_acc: 0.9918 val_loss: 0.7237 val_acc: 0.7733 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.3363 train_acc: 0.9920 val_loss: 0.7204 val_acc: 0.7784 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.3360 train_acc: 0.9922 val_loss: 0.7312 val_acc: 0.7716 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.3352 train_acc: 0.9929 val_loss: 0.7243 val_acc: 0.7782 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.3357 train_acc: 0.9926 val_loss: 0.7288 val_acc: 0.7654 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.3356 train_acc: 0.9927 val_loss: 0.7293 val_acc: 0.7698 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.3344 train_acc: 0.9934 val_loss: 0.7191 val_acc: 0.7790 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.3351 train_acc: 0.9928 val_loss: 0.7240 val_acc: 0.7728 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7750\n",
      "14000: 0.775\n",
      " test_acc: 0.7750\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.8016 train_acc: 0.7353 val_loss: 0.6531 val_acc: 0.8001 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6419 train_acc: 0.8058 val_loss: 0.6097 val_acc: 0.8208 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6074 train_acc: 0.8272 val_loss: 0.6025 val_acc: 0.8198 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5805 train_acc: 0.8445 val_loss: 0.5695 val_acc: 0.8456 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5632 train_acc: 0.8550 val_loss: 0.6163 val_acc: 0.8213 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5484 train_acc: 0.8641 val_loss: 0.5566 val_acc: 0.8586 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5364 train_acc: 0.8713 val_loss: 0.5987 val_acc: 0.8291 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5311 train_acc: 0.8733 val_loss: 0.6170 val_acc: 0.8148 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5212 train_acc: 0.8804 val_loss: 0.5249 val_acc: 0.8726 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.5152 train_acc: 0.8822 val_loss: 0.5562 val_acc: 0.8464 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.5020 train_acc: 0.8900 val_loss: 0.5165 val_acc: 0.8750 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.4971 train_acc: 0.8922 val_loss: 0.5220 val_acc: 0.8776 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.4918 train_acc: 0.8957 val_loss: 0.9089 val_acc: 0.6723 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.4820 train_acc: 0.8985 val_loss: 0.5690 val_acc: 0.8421 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.4755 train_acc: 0.9027 val_loss: 0.5058 val_acc: 0.8899 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.4667 train_acc: 0.9093 val_loss: 0.5653 val_acc: 0.8526 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.4607 train_acc: 0.9122 val_loss: 0.5177 val_acc: 0.8771 lr: 0.005\n",
      "Epoch  17 / 200 train_loss: 0.4526 train_acc: 0.9167 val_loss: 0.5751 val_acc: 0.8547 lr: 0.005\n",
      "Epoch  18 / 200 train_loss: 0.4427 train_acc: 0.9214 val_loss: 0.7793 val_acc: 0.7523 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.4113 train_acc: 0.9408 val_loss: 0.4961 val_acc: 0.8942 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.3997 train_acc: 0.9472 val_loss: 0.4994 val_acc: 0.8961 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.3918 train_acc: 0.9516 val_loss: 0.5105 val_acc: 0.8891 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.3831 train_acc: 0.9578 val_loss: 0.4882 val_acc: 0.8957 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.3763 train_acc: 0.9619 val_loss: 0.5648 val_acc: 0.8609 lr: 0.0025\n",
      "Epoch  24 / 200 train_loss: 0.3707 train_acc: 0.9648 val_loss: 0.5015 val_acc: 0.8942 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.3541 train_acc: 0.9744 val_loss: 0.5299 val_acc: 0.8829 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.3452 train_acc: 0.9801 val_loss: 0.5288 val_acc: 0.8822 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.3408 train_acc: 0.9822 val_loss: 0.5180 val_acc: 0.8897 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.3379 train_acc: 0.9840 val_loss: 0.5616 val_acc: 0.8746 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.3313 train_acc: 0.9881 val_loss: 0.5071 val_acc: 0.8998 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.3263 train_acc: 0.9906 val_loss: 0.4935 val_acc: 0.9030 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.3264 train_acc: 0.9902 val_loss: 0.4980 val_acc: 0.8982 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.3248 train_acc: 0.9911 val_loss: 0.5207 val_acc: 0.8914 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.3223 train_acc: 0.9923 val_loss: 0.4950 val_acc: 0.9029 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.3212 train_acc: 0.9930 val_loss: 0.5089 val_acc: 0.8970 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.3188 train_acc: 0.9939 val_loss: 0.4939 val_acc: 0.9010 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.3178 train_acc: 0.9943 val_loss: 0.4915 val_acc: 0.9032 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.3177 train_acc: 0.9949 val_loss: 0.5016 val_acc: 0.9000 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.3158 train_acc: 0.9952 val_loss: 0.5059 val_acc: 0.8978 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.3154 train_acc: 0.9956 val_loss: 0.4988 val_acc: 0.9014 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.3150 train_acc: 0.9955 val_loss: 0.5008 val_acc: 0.8983 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.3133 train_acc: 0.9963 val_loss: 0.5043 val_acc: 0.8946 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.3134 train_acc: 0.9964 val_loss: 0.4914 val_acc: 0.9023 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.3130 train_acc: 0.9966 val_loss: 0.4948 val_acc: 0.8965 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.3127 train_acc: 0.9965 val_loss: 0.4919 val_acc: 0.9044 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.3124 train_acc: 0.9962 val_loss: 0.4964 val_acc: 0.9034 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.3115 train_acc: 0.9965 val_loss: 0.4928 val_acc: 0.9021 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.3114 train_acc: 0.9969 val_loss: 0.4929 val_acc: 0.9015 lr: 0.00015625\n",
      "Epoch  48 / 200 train_loss: 0.3110 train_acc: 0.9970 val_loss: 0.4926 val_acc: 0.9023 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.3112 train_acc: 0.9967 val_loss: 0.4921 val_acc: 0.9025 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.3100 train_acc: 0.9976 val_loss: 0.4921 val_acc: 0.9030 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.3102 train_acc: 0.9976 val_loss: 0.4916 val_acc: 0.9030 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.3098 train_acc: 0.9971 val_loss: 0.5090 val_acc: 0.8968 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3105 train_acc: 0.9972 val_loss: 0.4946 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3099 train_acc: 0.9975 val_loss: 0.4920 val_acc: 0.9034 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3101 train_acc: 0.9970 val_loss: 0.4920 val_acc: 0.9019 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3091 train_acc: 0.9976 val_loss: 0.4925 val_acc: 0.9046 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3097 train_acc: 0.9974 val_loss: 0.4912 val_acc: 0.9042 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3092 train_acc: 0.9973 val_loss: 0.4924 val_acc: 0.9030 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3092 train_acc: 0.9980 val_loss: 0.4923 val_acc: 0.9042 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3098 train_acc: 0.9973 val_loss: 0.4927 val_acc: 0.9040 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3088 train_acc: 0.9982 val_loss: 0.4927 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3087 train_acc: 0.9978 val_loss: 0.4955 val_acc: 0.8968 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3089 train_acc: 0.9974 val_loss: 0.5052 val_acc: 0.8966 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3090 train_acc: 0.9975 val_loss: 0.4941 val_acc: 0.9023 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3091 train_acc: 0.9979 val_loss: 0.4924 val_acc: 0.9032 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3083 train_acc: 0.9978 val_loss: 0.4909 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3086 train_acc: 0.9975 val_loss: 0.4921 val_acc: 0.9034 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3087 train_acc: 0.9979 val_loss: 0.4919 val_acc: 0.9049 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.3077 train_acc: 0.9981 val_loss: 0.4914 val_acc: 0.9038 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.3080 train_acc: 0.9980 val_loss: 0.5137 val_acc: 0.8976 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.3087 train_acc: 0.9979 val_loss: 0.4918 val_acc: 0.9042 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.3080 train_acc: 0.9982 val_loss: 0.4935 val_acc: 0.9014 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.3079 train_acc: 0.9981 val_loss: 0.4924 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.3078 train_acc: 0.9984 val_loss: 0.4928 val_acc: 0.9014 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.3079 train_acc: 0.9979 val_loss: 0.4909 val_acc: 0.9046 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.3080 train_acc: 0.9977 val_loss: 0.4919 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.3075 train_acc: 0.9983 val_loss: 0.4917 val_acc: 0.9019 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.3073 train_acc: 0.9981 val_loss: 0.4905 val_acc: 0.9059 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.3074 train_acc: 0.9982 val_loss: 0.4909 val_acc: 0.9027 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.3075 train_acc: 0.9981 val_loss: 0.4917 val_acc: 0.9030 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.3072 train_acc: 0.9981 val_loss: 0.4931 val_acc: 0.9030 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.3071 train_acc: 0.9980 val_loss: 0.4919 val_acc: 0.9032 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.3072 train_acc: 0.9982 val_loss: 0.4932 val_acc: 0.9017 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.3069 train_acc: 0.9982 val_loss: 0.4987 val_acc: 0.8974 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.3069 train_acc: 0.9984 val_loss: 0.4913 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.3072 train_acc: 0.9982 val_loss: 0.4916 val_acc: 0.9023 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.3063 train_acc: 0.9985 val_loss: 0.4956 val_acc: 0.8991 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.3063 train_acc: 0.9984 val_loss: 0.4913 val_acc: 0.9027 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.3068 train_acc: 0.9981 val_loss: 0.4920 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch  90 / 200 train_loss: 0.3062 train_acc: 0.9987 val_loss: 0.4931 val_acc: 0.9038 lr: 5e-05\n",
      "Epoch  91 / 200 train_loss: 0.3061 train_acc: 0.9987 val_loss: 0.4964 val_acc: 0.8972 lr: 5e-05\n",
      "Epoch  92 / 200 train_loss: 0.3069 train_acc: 0.9980 val_loss: 0.4933 val_acc: 0.9014 lr: 5e-05\n",
      "Epoch  93 / 200 train_loss: 0.3064 train_acc: 0.9984 val_loss: 0.4914 val_acc: 0.9044 lr: 5e-05\n",
      "Epoch  94 / 200 train_loss: 0.3066 train_acc: 0.9982 val_loss: 0.4928 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch  95 / 200 train_loss: 0.3066 train_acc: 0.9980 val_loss: 0.4952 val_acc: 0.9023 lr: 5e-05\n",
      "Epoch  96 / 200 train_loss: 0.3061 train_acc: 0.9985 val_loss: 0.4906 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  97 / 200 train_loss: 0.3063 train_acc: 0.9980 val_loss: 0.4936 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch  98 / 200 train_loss: 0.3063 train_acc: 0.9983 val_loss: 0.4918 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  99 / 200 train_loss: 0.3059 train_acc: 0.9982 val_loss: 0.4950 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch 100 / 200 train_loss: 0.3060 train_acc: 0.9984 val_loss: 0.4938 val_acc: 0.9029 lr: 5e-05\n",
      "Epoch 101 / 200 train_loss: 0.3063 train_acc: 0.9981 val_loss: 0.4933 val_acc: 0.9023 lr: 5e-05\n",
      "Epoch 102 / 200 train_loss: 0.3060 train_acc: 0.9983 val_loss: 0.5135 val_acc: 0.8965 lr: 5e-05\n",
      "Epoch 103 / 200 train_loss: 0.3065 train_acc: 0.9981 val_loss: 0.4934 val_acc: 0.9014 lr: 5e-05\n",
      "Epoch 104 / 200 train_loss: 0.3057 train_acc: 0.9983 val_loss: 0.5105 val_acc: 0.8966 lr: 5e-05\n",
      "Epoch 105 / 200 train_loss: 0.3060 train_acc: 0.9983 val_loss: 0.5097 val_acc: 0.8980 lr: 5e-05\n",
      "Epoch 106 / 200 train_loss: 0.3060 train_acc: 0.9981 val_loss: 0.4958 val_acc: 0.9014 lr: 5e-05\n",
      "Epoch 107 / 200 train_loss: 0.3054 train_acc: 0.9985 val_loss: 0.4913 val_acc: 0.9032 lr: 5e-05\n",
      "Epoch 108 / 200 train_loss: 0.3054 train_acc: 0.9984 val_loss: 0.4931 val_acc: 0.9038 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8948\n",
      "14000: 0.8947777777777778\n",
      " test_acc: 0.8948\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.2313 train_acc: 0.5398Epoch   1 / 200 train_loss: 1.0964 train_acc: 0.6012 val_loss: 1.1216 val_acc: 0.5704 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 1.0590 train_acc: 0.6236 val_loss: 1.0643 val_acc: 0.6189 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 1.0378 train_acc: 0.6352 val_loss: 1.0575 val_acc: 0.6246 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 1.0195 train_acc: 0.6481 val_loss: 1.0426 val_acc: 0.6224 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.9972 train_acc: 0.6622 val_loss: 1.0335 val_acc: 0.6381 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.9768 train_acc: 0.6757 val_loss: 1.0426 val_acc: 0.6399 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.9573 train_acc: 0.6870 val_loss: 0.9495 val_acc: 0.6899 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.9432 train_acc: 0.6964 val_loss: 1.2677 val_acc: 0.5304 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.9293 train_acc: 0.7056 val_loss: 1.0286 val_acc: 0.6308 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.9155 train_acc: 0.7140 val_loss: 1.0178 val_acc: 0.6470 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.9041 train_acc: 0.7216 val_loss: 1.3508 val_acc: 0.5260 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.8697 train_acc: 0.7409 val_loss: 0.9643 val_acc: 0.6960 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.8568 train_acc: 0.7493 val_loss: 0.9645 val_acc: 0.6795 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.8494 train_acc: 0.7541 val_loss: 1.1414 val_acc: 0.6309 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.8387 train_acc: 0.7607 val_loss: 1.1947 val_acc: 0.6085 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.8316 train_acc: 0.7659 val_loss: 0.9407 val_acc: 0.6972 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.8210 train_acc: 0.7707 val_loss: 1.4709 val_acc: 0.5526 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.8101 train_acc: 0.7779 val_loss: 1.1872 val_acc: 0.6098 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.8021 train_acc: 0.7837 val_loss: 0.8837 val_acc: 0.7361 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.7947 train_acc: 0.7890 val_loss: 1.3119 val_acc: 0.5718 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.7883 train_acc: 0.7920 val_loss: 0.9244 val_acc: 0.7137 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.7768 train_acc: 0.7984 val_loss: 0.9124 val_acc: 0.7207 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.7710 train_acc: 0.8034 val_loss: 1.2141 val_acc: 0.6165 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.7377 train_acc: 0.8230 val_loss: 1.0871 val_acc: 0.6665 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.7216 train_acc: 0.8324 val_loss: 0.9280 val_acc: 0.7124 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.7103 train_acc: 0.8377 val_loss: 1.1069 val_acc: 0.6593 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.7049 train_acc: 0.8416 val_loss: 0.9215 val_acc: 0.7256 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.6828 train_acc: 0.8561 val_loss: 1.1389 val_acc: 0.6620 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.6722 train_acc: 0.8611 val_loss: 0.9353 val_acc: 0.7252 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.6640 train_acc: 0.8662 val_loss: 1.0058 val_acc: 0.7041 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.6559 train_acc: 0.8720 val_loss: 0.9519 val_acc: 0.7191 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.6414 train_acc: 0.8793 val_loss: 0.9438 val_acc: 0.7247 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.6354 train_acc: 0.8834 val_loss: 0.9469 val_acc: 0.7252 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.6340 train_acc: 0.8834 val_loss: 0.9817 val_acc: 0.7084 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.6273 train_acc: 0.8883 val_loss: 0.9736 val_acc: 0.7147 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.6214 train_acc: 0.8918 val_loss: 0.9553 val_acc: 0.7224 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.6197 train_acc: 0.8923 val_loss: 0.9669 val_acc: 0.7187 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.6163 train_acc: 0.8956 val_loss: 0.9664 val_acc: 0.7182 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.6150 train_acc: 0.8953 val_loss: 0.9582 val_acc: 0.7221 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.6125 train_acc: 0.8968 val_loss: 0.9716 val_acc: 0.7182 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.6086 train_acc: 0.9011 val_loss: 0.9600 val_acc: 0.7255 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.6072 train_acc: 0.9007 val_loss: 0.9564 val_acc: 0.7256 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.6087 train_acc: 0.8999 val_loss: 0.9760 val_acc: 0.7156 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.6034 train_acc: 0.9036 val_loss: 0.9585 val_acc: 0.7277 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.6056 train_acc: 0.9018 val_loss: 0.9603 val_acc: 0.7270 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.6023 train_acc: 0.9029 val_loss: 0.9586 val_acc: 0.7271 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.6019 train_acc: 0.9026 val_loss: 0.9607 val_acc: 0.7253 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.6018 train_acc: 0.9026 val_loss: 0.9594 val_acc: 0.7217 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.6019 train_acc: 0.9036 val_loss: 0.9604 val_acc: 0.7273 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7335\n",
      "14000: 0.7334666666666667\n",
      " test_acc: 0.7335\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.1122 train_acc: 0.5736 val_loss: 0.9421 val_acc: 0.6607 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.9258 train_acc: 0.6633 val_loss: 0.9283 val_acc: 0.6582 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8775 train_acc: 0.7000 val_loss: 0.8564 val_acc: 0.7083 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8472 train_acc: 0.7192 val_loss: 0.9887 val_acc: 0.6267 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8185 train_acc: 0.7363 val_loss: 0.9063 val_acc: 0.6668 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8004 train_acc: 0.7470 val_loss: 0.9412 val_acc: 0.6460 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7812 train_acc: 0.7576 val_loss: 0.8129 val_acc: 0.7364 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7627 train_acc: 0.7683 val_loss: 1.1320 val_acc: 0.5441 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7495 train_acc: 0.7762 val_loss: 0.8024 val_acc: 0.7456 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7347 train_acc: 0.7860 val_loss: 1.3382 val_acc: 0.4490 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7214 train_acc: 0.7934 val_loss: 0.7590 val_acc: 0.7692 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7056 train_acc: 0.8034 val_loss: 0.7947 val_acc: 0.7607 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.6916 train_acc: 0.8123 val_loss: 1.3906 val_acc: 0.5017 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.6787 train_acc: 0.8199 val_loss: 0.8204 val_acc: 0.7278 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.6637 train_acc: 0.8293 val_loss: 1.7287 val_acc: 0.4290 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6118 train_acc: 0.8618 val_loss: 0.9709 val_acc: 0.6867 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.5934 train_acc: 0.8737 val_loss: 1.0333 val_acc: 0.6837 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.5754 train_acc: 0.8853 val_loss: 0.9941 val_acc: 0.6606 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.5566 train_acc: 0.8976 val_loss: 0.7918 val_acc: 0.7673 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5252 train_acc: 0.9172 val_loss: 0.7969 val_acc: 0.7729 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5086 train_acc: 0.9281 val_loss: 0.8026 val_acc: 0.7797 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.5012 train_acc: 0.9320 val_loss: 0.8340 val_acc: 0.7677 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.4945 train_acc: 0.9370 val_loss: 0.8237 val_acc: 0.7685 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.4834 train_acc: 0.9434 val_loss: 0.8294 val_acc: 0.7780 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.4746 train_acc: 0.9487 val_loss: 0.8815 val_acc: 0.7298 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.4581 train_acc: 0.9598 val_loss: 0.7774 val_acc: 0.7878 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.4554 train_acc: 0.9607 val_loss: 0.7938 val_acc: 0.7764 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.4497 train_acc: 0.9642 val_loss: 0.7854 val_acc: 0.7834 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.4462 train_acc: 0.9667 val_loss: 0.8036 val_acc: 0.7784 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.4441 train_acc: 0.9681 val_loss: 0.8558 val_acc: 0.7704 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.4360 train_acc: 0.9730 val_loss: 0.7852 val_acc: 0.7857 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.4326 train_acc: 0.9740 val_loss: 0.7916 val_acc: 0.7795 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.4314 train_acc: 0.9758 val_loss: 0.7911 val_acc: 0.7842 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.4293 train_acc: 0.9766 val_loss: 0.7888 val_acc: 0.7838 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.4250 train_acc: 0.9787 val_loss: 0.7950 val_acc: 0.7802 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.4245 train_acc: 0.9793 val_loss: 0.7859 val_acc: 0.7850 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.4244 train_acc: 0.9791 val_loss: 0.7930 val_acc: 0.7833 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.4222 train_acc: 0.9792 val_loss: 0.7913 val_acc: 0.7850 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.4210 train_acc: 0.9807 val_loss: 0.7833 val_acc: 0.7854 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.4199 train_acc: 0.9816 val_loss: 0.7895 val_acc: 0.7833 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.4191 train_acc: 0.9819 val_loss: 0.7854 val_acc: 0.7833 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.4191 train_acc: 0.9825 val_loss: 0.7882 val_acc: 0.7845 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.4191 train_acc: 0.9821 val_loss: 0.7856 val_acc: 0.7862 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.4196 train_acc: 0.9819 val_loss: 0.7875 val_acc: 0.7858 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.4186 train_acc: 0.9827 val_loss: 0.7890 val_acc: 0.7847 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.4173 train_acc: 0.9827 val_loss: 0.7870 val_acc: 0.7857 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.4161 train_acc: 0.9839 val_loss: 0.7861 val_acc: 0.7830 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.4186 train_acc: 0.9825 val_loss: 0.7865 val_acc: 0.7857 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.4169 train_acc: 0.9827 val_loss: 0.7878 val_acc: 0.7857 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.4171 train_acc: 0.9825 val_loss: 0.7887 val_acc: 0.7848 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.4159 train_acc: 0.9840 val_loss: 0.7898 val_acc: 0.7857 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.4168 train_acc: 0.9833 val_loss: 0.7856 val_acc: 0.7846 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.4171 train_acc: 0.9831 val_loss: 0.7895 val_acc: 0.7823 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.4164 train_acc: 0.9840 val_loss: 0.7889 val_acc: 0.7848 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.4154 train_acc: 0.9838 val_loss: 0.7845 val_acc: 0.7852 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.4154 train_acc: 0.9839 val_loss: 0.7869 val_acc: 0.7853 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7977\n",
      "14000: 0.79775\n",
      " test_acc: 0.7977\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7977 train_acc: 0.6040 val_loss: 0.6404 val_acc: 0.6640 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6417 train_acc: 0.6515 val_loss: 0.7450 val_acc: 0.5080 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6305 train_acc: 0.6664 val_loss: 0.6397 val_acc: 0.6483 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6258 train_acc: 0.6685 val_loss: 0.6831 val_acc: 0.5480 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6211 train_acc: 0.6744 val_loss: 0.6264 val_acc: 0.6613 lr: 0.0025\n",
      "Epoch   5 / 200 train_loss: 0.6100 train_acc: 0.6890 val_loss: 0.6384 val_acc: 0.6533 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.6048 train_acc: 0.6928 val_loss: 0.6462 val_acc: 0.6331 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.6011 train_acc: 0.6982 val_loss: 0.7543 val_acc: 0.5287 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.5986 train_acc: 0.7037 val_loss: 0.6557 val_acc: 0.6200 lr: 0.00125\n",
      "Epoch   9 / 200 train_loss: 0.5865 train_acc: 0.7154 val_loss: 0.9084 val_acc: 0.5188 lr: 0.00125\n",
      "Epoch  10 / 200 train_loss: 0.5834 train_acc: 0.7161 val_loss: 0.7770 val_acc: 0.5584 lr: 0.00125\n",
      "Epoch  11 / 200 train_loss: 0.5800 train_acc: 0.7222 val_loss: 0.6504 val_acc: 0.6348 lr: 0.00125\n",
      "Epoch  12 / 200 train_loss: 0.5749 train_acc: 0.7275 val_loss: 0.5945 val_acc: 0.7047 lr: 0.00125\n",
      "Epoch  13 / 200 train_loss: 0.5714 train_acc: 0.7296 val_loss: 0.8521 val_acc: 0.5512 lr: 0.00125\n",
      "Epoch  14 / 200 train_loss: 0.5670 train_acc: 0.7351 val_loss: 0.7885 val_acc: 0.5724 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.5646 train_acc: 0.7372 val_loss: 0.6938 val_acc: 0.6311 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.5574 train_acc: 0.7393 val_loss: 0.8654 val_acc: 0.5454 lr: 0.000625\n",
      "Epoch  17 / 200 train_loss: 0.5403 train_acc: 0.7557 val_loss: 0.6359 val_acc: 0.6861 lr: 0.000625\n",
      "Epoch  18 / 200 train_loss: 0.5336 train_acc: 0.7610 val_loss: 0.6358 val_acc: 0.6921 lr: 0.000625\n",
      "Epoch  19 / 200 train_loss: 0.5231 train_acc: 0.7723 val_loss: 0.6438 val_acc: 0.6752 lr: 0.000625\n",
      "Epoch  20 / 200 train_loss: 0.5165 train_acc: 0.7771 val_loss: 0.6450 val_acc: 0.6871 lr: 0.0003125\n",
      "Epoch  21 / 200 train_loss: 0.4962 train_acc: 0.7935 val_loss: 0.6325 val_acc: 0.6889 lr: 0.0003125\n",
      "Epoch  22 / 200 train_loss: 0.4837 train_acc: 0.8058 val_loss: 0.6672 val_acc: 0.6787 lr: 0.0003125\n",
      "Epoch  23 / 200 train_loss: 0.4764 train_acc: 0.8092 val_loss: 0.6431 val_acc: 0.6887 lr: 0.0003125\n",
      "Epoch  24 / 200 train_loss: 0.4679 train_acc: 0.8163 val_loss: 0.6697 val_acc: 0.6746 lr: 0.00015625\n",
      "Epoch  25 / 200 train_loss: 0.4520 train_acc: 0.8308 val_loss: 0.6732 val_acc: 0.6839 lr: 0.00015625\n",
      "Epoch  26 / 200 train_loss: 0.4464 train_acc: 0.8333 val_loss: 0.6807 val_acc: 0.6745 lr: 0.00015625\n",
      "Epoch  27 / 200 train_loss: 0.4381 train_acc: 0.8403 val_loss: 0.6685 val_acc: 0.6922 lr: 0.00015625\n",
      "Epoch  28 / 200 train_loss: 0.4365 train_acc: 0.8400 val_loss: 0.6917 val_acc: 0.6739 lr: 7.8125e-05\n",
      "Epoch  29 / 200 train_loss: 0.4262 train_acc: 0.8499 val_loss: 0.6872 val_acc: 0.6835 lr: 7.8125e-05\n",
      "Epoch  30 / 200 train_loss: 0.4249 train_acc: 0.8536 val_loss: 0.6857 val_acc: 0.6875 lr: 7.8125e-05\n",
      "Epoch  31 / 200 train_loss: 0.4206 train_acc: 0.8524 val_loss: 0.6997 val_acc: 0.6859 lr: 7.8125e-05\n",
      "Epoch  32 / 200 train_loss: 0.4168 train_acc: 0.8581 val_loss: 0.6965 val_acc: 0.6861 lr: 5e-05\n",
      "Epoch  33 / 200 train_loss: 0.4135 train_acc: 0.8609 val_loss: 0.7099 val_acc: 0.6738 lr: 5e-05\n",
      "Epoch  34 / 200 train_loss: 0.4078 train_acc: 0.8657 val_loss: 0.7014 val_acc: 0.6862 lr: 5e-05\n",
      "Epoch  35 / 200 train_loss: 0.4102 train_acc: 0.8618 val_loss: 0.7006 val_acc: 0.6858 lr: 5e-05\n",
      "Epoch  36 / 200 train_loss: 0.4051 train_acc: 0.8663 val_loss: 0.7086 val_acc: 0.6768 lr: 5e-05\n",
      "Epoch  37 / 200 train_loss: 0.4060 train_acc: 0.8669 val_loss: 0.7150 val_acc: 0.6803 lr: 5e-05\n",
      "Epoch  38 / 200 train_loss: 0.4047 train_acc: 0.8676 val_loss: 0.7082 val_acc: 0.6809 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.4024 train_acc: 0.8700 val_loss: 0.7129 val_acc: 0.6809 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3991 train_acc: 0.8713 val_loss: 0.7165 val_acc: 0.6786 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3973 train_acc: 0.8724 val_loss: 0.7165 val_acc: 0.6800 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3952 train_acc: 0.8748 val_loss: 0.7100 val_acc: 0.6837 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7107\n",
      "14000: 0.7106666666666667\n",
      " test_acc: 0.7107\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7470 train_acc: 0.6165 val_loss: 0.6465 val_acc: 0.6434 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6382 train_acc: 0.6547 val_loss: 0.6696 val_acc: 0.6075 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6266 train_acc: 0.6720 val_loss: 0.6887 val_acc: 0.5920 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6178 train_acc: 0.6793 val_loss: 0.7253 val_acc: 0.5430 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6041 train_acc: 0.6948 val_loss: 0.6208 val_acc: 0.6760 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5955 train_acc: 0.7054 val_loss: 0.6201 val_acc: 0.6692 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5847 train_acc: 0.7187 val_loss: 0.6252 val_acc: 0.6695 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5748 train_acc: 0.7277 val_loss: 0.6228 val_acc: 0.6955 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5674 train_acc: 0.7374 val_loss: 0.7427 val_acc: 0.6020 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.5595 train_acc: 0.7428 val_loss: 0.6619 val_acc: 0.6621 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.5498 train_acc: 0.7496 val_loss: 0.6387 val_acc: 0.6752 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.5456 train_acc: 0.7563 val_loss: 0.6751 val_acc: 0.6590 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.5230 train_acc: 0.7768 val_loss: 0.5709 val_acc: 0.7355 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.5139 train_acc: 0.7831 val_loss: 0.6396 val_acc: 0.6737 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.5055 train_acc: 0.7898 val_loss: 0.6360 val_acc: 0.6915 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.5043 train_acc: 0.7914 val_loss: 0.5596 val_acc: 0.7457 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.4934 train_acc: 0.8001 val_loss: 0.5948 val_acc: 0.7219 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.4878 train_acc: 0.8055 val_loss: 0.8752 val_acc: 0.6214 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.4790 train_acc: 0.8123 val_loss: 0.6040 val_acc: 0.7257 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.4650 train_acc: 0.8219 val_loss: 0.5493 val_acc: 0.7538 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.4590 train_acc: 0.8288 val_loss: 0.5725 val_acc: 0.7355 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.4480 train_acc: 0.8351 val_loss: 0.6551 val_acc: 0.6734 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.4400 train_acc: 0.8426 val_loss: 0.6272 val_acc: 0.7250 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.4292 train_acc: 0.8533 val_loss: 0.6435 val_acc: 0.7270 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.3925 train_acc: 0.8787 val_loss: 0.5518 val_acc: 0.7669 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.3760 train_acc: 0.8907 val_loss: 0.5735 val_acc: 0.7650 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.3617 train_acc: 0.9018 val_loss: 0.5867 val_acc: 0.7537 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.3520 train_acc: 0.9073 val_loss: 0.5794 val_acc: 0.7645 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.3395 train_acc: 0.9148 val_loss: 0.5915 val_acc: 0.7685 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.3305 train_acc: 0.9220 val_loss: 0.6131 val_acc: 0.7523 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.3171 train_acc: 0.9331 val_loss: 0.6527 val_acc: 0.7224 lr: 0.00125\n",
      "Epoch  31 / 200 train_loss: 0.3062 train_acc: 0.9378 val_loss: 0.6296 val_acc: 0.7617 lr: 0.00125\n",
      "Epoch  32 / 200 train_loss: 0.2972 train_acc: 0.9449 val_loss: 0.6248 val_acc: 0.7537 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.2754 train_acc: 0.9589 val_loss: 0.6462 val_acc: 0.7560 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.2667 train_acc: 0.9656 val_loss: 0.6426 val_acc: 0.7621 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.2609 train_acc: 0.9684 val_loss: 0.6560 val_acc: 0.7586 lr: 0.000625\n",
      "Epoch  36 / 200 train_loss: 0.2549 train_acc: 0.9714 val_loss: 0.7137 val_acc: 0.7436 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2473 train_acc: 0.9759 val_loss: 0.6680 val_acc: 0.7598 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.2416 train_acc: 0.9798 val_loss: 0.6617 val_acc: 0.7616 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.2373 train_acc: 0.9831 val_loss: 0.6681 val_acc: 0.7552 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.2375 train_acc: 0.9829 val_loss: 0.6874 val_acc: 0.7584 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2324 train_acc: 0.9856 val_loss: 0.6844 val_acc: 0.7580 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.2312 train_acc: 0.9862 val_loss: 0.6743 val_acc: 0.7591 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2311 train_acc: 0.9860 val_loss: 0.6815 val_acc: 0.7614 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2295 train_acc: 0.9867 val_loss: 0.6870 val_acc: 0.7580 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.2290 train_acc: 0.9873 val_loss: 0.6766 val_acc: 0.7620 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.2266 train_acc: 0.9883 val_loss: 0.6859 val_acc: 0.7600 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.2255 train_acc: 0.9890 val_loss: 0.6855 val_acc: 0.7572 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.2263 train_acc: 0.9885 val_loss: 0.6868 val_acc: 0.7612 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2254 train_acc: 0.9897 val_loss: 0.6861 val_acc: 0.7624 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2235 train_acc: 0.9906 val_loss: 0.6860 val_acc: 0.7639 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2241 train_acc: 0.9896 val_loss: 0.6839 val_acc: 0.7587 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2236 train_acc: 0.9902 val_loss: 0.6866 val_acc: 0.7596 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2237 train_acc: 0.9905 val_loss: 0.6861 val_acc: 0.7634 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2228 train_acc: 0.9914 val_loss: 0.6894 val_acc: 0.7649 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2210 train_acc: 0.9919 val_loss: 0.6912 val_acc: 0.7602 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2231 train_acc: 0.9904 val_loss: 0.6889 val_acc: 0.7614 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2221 train_acc: 0.9907 val_loss: 0.6918 val_acc: 0.7580 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2221 train_acc: 0.9915 val_loss: 0.6875 val_acc: 0.7630 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7675\n",
      "14000: 0.7675\n",
      " test_acc: 0.7675\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7564 train_acc: 0.6117 val_loss: 0.6692 val_acc: 0.5860 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6139 train_acc: 0.6898 val_loss: 0.6103 val_acc: 0.7040 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5824 train_acc: 0.7273 val_loss: 0.7637 val_acc: 0.6269 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5588 train_acc: 0.7482 val_loss: 0.5372 val_acc: 0.7692 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5408 train_acc: 0.7633 val_loss: 0.5269 val_acc: 0.7722 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5245 train_acc: 0.7774 val_loss: 0.5189 val_acc: 0.7845 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5132 train_acc: 0.7850 val_loss: 0.5910 val_acc: 0.7139 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5010 train_acc: 0.7944 val_loss: 0.5702 val_acc: 0.7169 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4908 train_acc: 0.7994 val_loss: 0.5066 val_acc: 0.7841 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4820 train_acc: 0.8117 val_loss: 0.5644 val_acc: 0.7443 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.4525 train_acc: 0.8294 val_loss: 0.4681 val_acc: 0.8142 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4406 train_acc: 0.8386 val_loss: 0.4819 val_acc: 0.8089 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4300 train_acc: 0.8460 val_loss: 0.4726 val_acc: 0.8173 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4194 train_acc: 0.8550 val_loss: 0.4938 val_acc: 0.7963 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4108 train_acc: 0.8634 val_loss: 0.4687 val_acc: 0.8228 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.4000 train_acc: 0.8678 val_loss: 0.5546 val_acc: 0.7621 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.3907 train_acc: 0.8748 val_loss: 0.4704 val_acc: 0.8198 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.3797 train_acc: 0.8834 val_loss: 0.4826 val_acc: 0.8204 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.3682 train_acc: 0.8926 val_loss: 0.4927 val_acc: 0.8068 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3336 train_acc: 0.9169 val_loss: 0.4718 val_acc: 0.8320 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3185 train_acc: 0.9284 val_loss: 0.4828 val_acc: 0.8305 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3052 train_acc: 0.9359 val_loss: 0.4944 val_acc: 0.8235 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3012 train_acc: 0.9389 val_loss: 0.4961 val_acc: 0.8291 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2905 train_acc: 0.9480 val_loss: 0.5318 val_acc: 0.8088 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2733 train_acc: 0.9601 val_loss: 0.5153 val_acc: 0.8244 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2633 train_acc: 0.9654 val_loss: 0.5106 val_acc: 0.8263 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2600 train_acc: 0.9685 val_loss: 0.5288 val_acc: 0.8215 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2550 train_acc: 0.9718 val_loss: 0.5251 val_acc: 0.8238 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.2480 train_acc: 0.9771 val_loss: 0.5331 val_acc: 0.8191 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.2461 train_acc: 0.9790 val_loss: 0.5163 val_acc: 0.8282 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2426 train_acc: 0.9798 val_loss: 0.5216 val_acc: 0.8234 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2396 train_acc: 0.9824 val_loss: 0.5209 val_acc: 0.8275 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.2373 train_acc: 0.9832 val_loss: 0.5241 val_acc: 0.8243 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.2339 train_acc: 0.9864 val_loss: 0.5270 val_acc: 0.8266 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2351 train_acc: 0.9850 val_loss: 0.5232 val_acc: 0.8252 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2340 train_acc: 0.9854 val_loss: 0.5328 val_acc: 0.8227 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.2320 train_acc: 0.9865 val_loss: 0.5255 val_acc: 0.8253 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.2303 train_acc: 0.9885 val_loss: 0.5300 val_acc: 0.8249 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.2304 train_acc: 0.9883 val_loss: 0.5268 val_acc: 0.8234 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.2305 train_acc: 0.9882 val_loss: 0.5295 val_acc: 0.8248 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.2293 train_acc: 0.9875 val_loss: 0.5313 val_acc: 0.8207 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.2293 train_acc: 0.9888 val_loss: 0.5292 val_acc: 0.8269 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2282 train_acc: 0.9896 val_loss: 0.5275 val_acc: 0.8254 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2282 train_acc: 0.9896 val_loss: 0.5343 val_acc: 0.8211 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2284 train_acc: 0.9895 val_loss: 0.5309 val_acc: 0.8262 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2289 train_acc: 0.9885 val_loss: 0.5323 val_acc: 0.8247 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2275 train_acc: 0.9901 val_loss: 0.5330 val_acc: 0.8230 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2265 train_acc: 0.9900 val_loss: 0.5313 val_acc: 0.8215 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2269 train_acc: 0.9899 val_loss: 0.5282 val_acc: 0.8265 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2259 train_acc: 0.9908 val_loss: 0.5328 val_acc: 0.8212 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8317\n",
      "14000: 0.8316666666666667\n",
      " test_acc: 0.8317\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7688 train_acc: 0.5460 val_loss: 0.6895 val_acc: 0.5369 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6776 train_acc: 0.5788 val_loss: 0.6683 val_acc: 0.6092 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6668 train_acc: 0.6041 val_loss: 0.6714 val_acc: 0.6028 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6583 train_acc: 0.6244 val_loss: 0.6512 val_acc: 0.6332 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6463 train_acc: 0.6402 val_loss: 0.6974 val_acc: 0.5866 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.6417 train_acc: 0.6470 val_loss: 0.6537 val_acc: 0.6281 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.6303 train_acc: 0.6643 val_loss: 0.6504 val_acc: 0.6413 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.6230 train_acc: 0.6715 val_loss: 0.6313 val_acc: 0.6662 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.6137 train_acc: 0.6824 val_loss: 0.6148 val_acc: 0.6784 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.6081 train_acc: 0.6916 val_loss: 0.6055 val_acc: 0.6804 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.5954 train_acc: 0.7020 val_loss: 0.6913 val_acc: 0.5669 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.5883 train_acc: 0.7111 val_loss: 0.6290 val_acc: 0.6669 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.5765 train_acc: 0.7192 val_loss: 0.5881 val_acc: 0.7096 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.5687 train_acc: 0.7317 val_loss: 0.6004 val_acc: 0.7072 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.5639 train_acc: 0.7361 val_loss: 0.6016 val_acc: 0.6916 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.5546 train_acc: 0.7424 val_loss: 0.5757 val_acc: 0.7213 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.5443 train_acc: 0.7509 val_loss: 0.6086 val_acc: 0.6919 lr: 0.005\n",
      "Epoch  17 / 200 train_loss: 0.5374 train_acc: 0.7593 val_loss: 0.5850 val_acc: 0.7150 lr: 0.005\n",
      "Epoch  18 / 200 train_loss: 0.5294 train_acc: 0.7670 val_loss: 0.7018 val_acc: 0.6089 lr: 0.005\n",
      "Epoch  19 / 200 train_loss: 0.5172 train_acc: 0.7763 val_loss: 0.6616 val_acc: 0.6769 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.4821 train_acc: 0.8076 val_loss: 0.5966 val_acc: 0.7045 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.4684 train_acc: 0.8187 val_loss: 0.5703 val_acc: 0.7440 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.4588 train_acc: 0.8227 val_loss: 0.5653 val_acc: 0.7565 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.4410 train_acc: 0.8412 val_loss: 0.7653 val_acc: 0.6436 lr: 0.0025\n",
      "Epoch  24 / 200 train_loss: 0.4318 train_acc: 0.8480 val_loss: 0.5762 val_acc: 0.7533 lr: 0.0025\n",
      "Epoch  25 / 200 train_loss: 0.4175 train_acc: 0.8582 val_loss: 0.5693 val_acc: 0.7517 lr: 0.0025\n",
      "Epoch  26 / 200 train_loss: 0.4034 train_acc: 0.8703 val_loss: 0.5741 val_acc: 0.7559 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.3675 train_acc: 0.8967 val_loss: 0.6052 val_acc: 0.7619 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.3537 train_acc: 0.9084 val_loss: 0.6126 val_acc: 0.7498 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.3397 train_acc: 0.9184 val_loss: 0.6237 val_acc: 0.7423 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.3286 train_acc: 0.9258 val_loss: 0.6462 val_acc: 0.7395 lr: 0.00125\n",
      "Epoch  31 / 200 train_loss: 0.3155 train_acc: 0.9363 val_loss: 0.6714 val_acc: 0.7301 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.2976 train_acc: 0.9501 val_loss: 0.6511 val_acc: 0.7412 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.2899 train_acc: 0.9564 val_loss: 0.6625 val_acc: 0.7413 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.2848 train_acc: 0.9591 val_loss: 0.6590 val_acc: 0.7436 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.2792 train_acc: 0.9628 val_loss: 0.6747 val_acc: 0.7369 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2683 train_acc: 0.9697 val_loss: 0.6701 val_acc: 0.7492 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2656 train_acc: 0.9725 val_loss: 0.6654 val_acc: 0.7468 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.2623 train_acc: 0.9754 val_loss: 0.6753 val_acc: 0.7441 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.2600 train_acc: 0.9760 val_loss: 0.6751 val_acc: 0.7472 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2552 train_acc: 0.9785 val_loss: 0.6745 val_acc: 0.7431 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2544 train_acc: 0.9801 val_loss: 0.6830 val_acc: 0.7438 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.2527 train_acc: 0.9805 val_loss: 0.6760 val_acc: 0.7449 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2505 train_acc: 0.9829 val_loss: 0.6801 val_acc: 0.7410 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.2507 train_acc: 0.9817 val_loss: 0.6785 val_acc: 0.7439 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.2503 train_acc: 0.9816 val_loss: 0.6800 val_acc: 0.7404 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.2472 train_acc: 0.9849 val_loss: 0.6826 val_acc: 0.7433 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.2487 train_acc: 0.9835 val_loss: 0.6823 val_acc: 0.7402 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2474 train_acc: 0.9829 val_loss: 0.6832 val_acc: 0.7444 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2468 train_acc: 0.9839 val_loss: 0.6860 val_acc: 0.7446 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2460 train_acc: 0.9838 val_loss: 0.6791 val_acc: 0.7434 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2466 train_acc: 0.9845 val_loss: 0.6823 val_acc: 0.7416 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2469 train_acc: 0.9842 val_loss: 0.6809 val_acc: 0.7403 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2449 train_acc: 0.9849 val_loss: 0.6831 val_acc: 0.7396 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2461 train_acc: 0.9833 val_loss: 0.6821 val_acc: 0.7417 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2449 train_acc: 0.9846 val_loss: 0.6846 val_acc: 0.7442 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2440 train_acc: 0.9859 val_loss: 0.6825 val_acc: 0.7435 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2439 train_acc: 0.9856 val_loss: 0.6822 val_acc: 0.7450 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.7513\n",
      "14000: 0.7513333333333333\n",
      " test_acc: 0.7513\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7449 train_acc: 0.6201 val_loss: 0.7066 val_acc: 0.5982 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6222 train_acc: 0.6728 val_loss: 0.6246 val_acc: 0.6658 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6021 train_acc: 0.6984 val_loss: 0.6744 val_acc: 0.5988 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5865 train_acc: 0.7182 val_loss: 0.5790 val_acc: 0.7104 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5692 train_acc: 0.7319 val_loss: 0.6344 val_acc: 0.6816 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5525 train_acc: 0.7490 val_loss: 0.6051 val_acc: 0.6920 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5338 train_acc: 0.7640 val_loss: 0.6201 val_acc: 0.6707 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5206 train_acc: 0.7756 val_loss: 0.5232 val_acc: 0.7761 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5070 train_acc: 0.7875 val_loss: 0.7138 val_acc: 0.6468 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4917 train_acc: 0.8018 val_loss: 0.5791 val_acc: 0.7398 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4801 train_acc: 0.8128 val_loss: 0.5367 val_acc: 0.7652 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.4737 train_acc: 0.8136 val_loss: 0.6543 val_acc: 0.7067 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4373 train_acc: 0.8445 val_loss: 0.4660 val_acc: 0.8137 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4249 train_acc: 0.8535 val_loss: 0.4696 val_acc: 0.8118 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4177 train_acc: 0.8587 val_loss: 0.5320 val_acc: 0.7857 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.4079 train_acc: 0.8663 val_loss: 0.5597 val_acc: 0.7584 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.4007 train_acc: 0.8703 val_loss: 0.4631 val_acc: 0.8210 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.3887 train_acc: 0.8787 val_loss: 0.5090 val_acc: 0.7870 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.3799 train_acc: 0.8856 val_loss: 0.4919 val_acc: 0.8083 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.3684 train_acc: 0.8925 val_loss: 0.5530 val_acc: 0.7802 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.3588 train_acc: 0.9015 val_loss: 0.5178 val_acc: 0.7862 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3317 train_acc: 0.9222 val_loss: 0.4642 val_acc: 0.8389 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3166 train_acc: 0.9315 val_loss: 0.4587 val_acc: 0.8382 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3088 train_acc: 0.9383 val_loss: 0.6770 val_acc: 0.7456 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.3003 train_acc: 0.9438 val_loss: 0.5233 val_acc: 0.8060 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.2883 train_acc: 0.9521 val_loss: 0.5589 val_acc: 0.7801 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2733 train_acc: 0.9608 val_loss: 0.4793 val_acc: 0.8306 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2662 train_acc: 0.9690 val_loss: 0.4855 val_acc: 0.8311 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2635 train_acc: 0.9702 val_loss: 0.4889 val_acc: 0.8300 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2565 train_acc: 0.9733 val_loss: 0.5074 val_acc: 0.8207 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2509 train_acc: 0.9775 val_loss: 0.4857 val_acc: 0.8366 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2478 train_acc: 0.9798 val_loss: 0.4921 val_acc: 0.8302 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2434 train_acc: 0.9833 val_loss: 0.4936 val_acc: 0.8314 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2422 train_acc: 0.9829 val_loss: 0.4953 val_acc: 0.8335 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2380 train_acc: 0.9858 val_loss: 0.4981 val_acc: 0.8304 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2382 train_acc: 0.9856 val_loss: 0.5034 val_acc: 0.8311 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2359 train_acc: 0.9872 val_loss: 0.4983 val_acc: 0.8316 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2347 train_acc: 0.9876 val_loss: 0.4977 val_acc: 0.8333 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.2341 train_acc: 0.9878 val_loss: 0.4936 val_acc: 0.8333 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.2327 train_acc: 0.9891 val_loss: 0.4981 val_acc: 0.8309 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.2322 train_acc: 0.9889 val_loss: 0.4943 val_acc: 0.8339 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2310 train_acc: 0.9895 val_loss: 0.4976 val_acc: 0.8341 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2306 train_acc: 0.9898 val_loss: 0.4976 val_acc: 0.8324 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2308 train_acc: 0.9900 val_loss: 0.4964 val_acc: 0.8319 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2300 train_acc: 0.9907 val_loss: 0.4989 val_acc: 0.8309 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2296 train_acc: 0.9908 val_loss: 0.4971 val_acc: 0.8325 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2293 train_acc: 0.9907 val_loss: 0.5009 val_acc: 0.8278 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2285 train_acc: 0.9913 val_loss: 0.5005 val_acc: 0.8323 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2281 train_acc: 0.9909 val_loss: 0.4985 val_acc: 0.8329 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2286 train_acc: 0.9904 val_loss: 0.5025 val_acc: 0.8344 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2275 train_acc: 0.9922 val_loss: 0.4998 val_acc: 0.8340 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2271 train_acc: 0.9917 val_loss: 0.5023 val_acc: 0.8267 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8367\n",
      "14000: 0.8366666666666667\n",
      " test_acc: 0.8367\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5736 train_acc: 0.8025 val_loss: 0.4987 val_acc: 0.7986 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.4089 train_acc: 0.8645 val_loss: 0.3983 val_acc: 0.8659 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3808 train_acc: 0.8788 val_loss: 0.4589 val_acc: 0.8153 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3632 train_acc: 0.8896 val_loss: 0.5381 val_acc: 0.7629 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.3522 train_acc: 0.8954 val_loss: 0.4680 val_acc: 0.8238 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.3469 train_acc: 0.9007 val_loss: 0.4370 val_acc: 0.8424 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.3291 train_acc: 0.9097 val_loss: 0.3570 val_acc: 0.8902 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.3190 train_acc: 0.9166 val_loss: 0.7415 val_acc: 0.6709 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.3159 train_acc: 0.9204 val_loss: 0.3308 val_acc: 0.9084 lr: 0.0025\n",
      "Epoch  38 / 200 train_loss: 0.2122 train_acc: 0.9962 val_loss: 0.3601 val_acc: 0.9150 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.2114 train_acc: 0.9964 val_loss: 0.3569 val_acc: 0.9138 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.2107 train_acc: 0.9971 val_loss: 0.3600 val_acc: 0.9117 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.2111 train_acc: 0.9967 val_loss: 0.3578 val_acc: 0.9137 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.2110 train_acc: 0.9969 val_loss: 0.3596 val_acc: 0.9111 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.2112 train_acc: 0.9966 val_loss: 0.3571 val_acc: 0.9133 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.2102 train_acc: 0.9971 val_loss: 0.3565 val_acc: 0.9138 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2101 train_acc: 0.9974 val_loss: 0.3575 val_acc: 0.9132 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2097 train_acc: 0.9976 val_loss: 0.3583 val_acc: 0.9108 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2105 train_acc: 0.9968 val_loss: 0.3603 val_acc: 0.9124 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2094 train_acc: 0.9973 val_loss: 0.3594 val_acc: 0.9114 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2096 train_acc: 0.9973 val_loss: 0.3595 val_acc: 0.9127 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2101 train_acc: 0.9969 val_loss: 0.3579 val_acc: 0.9154 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2093 train_acc: 0.9976 val_loss: 0.3585 val_acc: 0.9143 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2094 train_acc: 0.9976 val_loss: 0.3600 val_acc: 0.9147 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9150\n",
      "14000: 0.915\n",
      " test_acc: 0.9150\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.4587 train_acc: 0.8862 val_loss: 0.3238 val_acc: 0.9260 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3206 train_acc: 0.9301 val_loss: 0.5913 val_acc: 0.7835 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3014 train_acc: 0.9437 val_loss: 0.3217 val_acc: 0.9290 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.2926 train_acc: 0.9483 val_loss: 0.3365 val_acc: 0.9177 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.2850 train_acc: 0.9519 val_loss: 0.2964 val_acc: 0.9421 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.2798 train_acc: 0.9545 val_loss: 0.5553 val_acc: 0.7862 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.2775 train_acc: 0.9560 val_loss: 0.4316 val_acc: 0.8465 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.2749 train_acc: 0.9575 val_loss: 0.2868 val_acc: 0.9500 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.2692 train_acc: 0.9602 val_loss: 0.3145 val_acc: 0.9402 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.2660 train_acc: 0.9625 val_loss: 0.4289 val_acc: 0.8690 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.2670 train_acc: 0.9622 val_loss: 0.3238 val_acc: 0.9300 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.2637 train_acc: 0.9638 val_loss: 0.3041 val_acc: 0.9374 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2477 train_acc: 0.9727 val_loss: 0.2736 val_acc: 0.9560 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2460 train_acc: 0.9735 val_loss: 0.2694 val_acc: 0.9610 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2436 train_acc: 0.9748 val_loss: 0.2641 val_acc: 0.9635 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2428 train_acc: 0.9760 val_loss: 0.3008 val_acc: 0.9405 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.2387 train_acc: 0.9782 val_loss: 0.2634 val_acc: 0.9637 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.2345 train_acc: 0.9807 val_loss: 0.2716 val_acc: 0.9603 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.2365 train_acc: 0.9794 val_loss: 0.2698 val_acc: 0.9625 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.2321 train_acc: 0.9821 val_loss: 0.2987 val_acc: 0.9450 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.2293 train_acc: 0.9831 val_loss: 0.3356 val_acc: 0.9209 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2212 train_acc: 0.9886 val_loss: 0.2606 val_acc: 0.9681 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.2182 train_acc: 0.9901 val_loss: 0.2754 val_acc: 0.9567 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2152 train_acc: 0.9927 val_loss: 0.2647 val_acc: 0.9639 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.2159 train_acc: 0.9916 val_loss: 0.2641 val_acc: 0.9642 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.2135 train_acc: 0.9932 val_loss: 0.2667 val_acc: 0.9649 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2102 train_acc: 0.9949 val_loss: 0.2655 val_acc: 0.9661 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2087 train_acc: 0.9959 val_loss: 0.2605 val_acc: 0.9675 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2084 train_acc: 0.9959 val_loss: 0.2628 val_acc: 0.9693 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2081 train_acc: 0.9961 val_loss: 0.2650 val_acc: 0.9659 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.2075 train_acc: 0.9965 val_loss: 0.2732 val_acc: 0.9615 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.2073 train_acc: 0.9965 val_loss: 0.2632 val_acc: 0.9693 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.2065 train_acc: 0.9969 val_loss: 0.2627 val_acc: 0.9679 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2051 train_acc: 0.9979 val_loss: 0.2614 val_acc: 0.9686 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2042 train_acc: 0.9983 val_loss: 0.2627 val_acc: 0.9678 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2040 train_acc: 0.9984 val_loss: 0.2628 val_acc: 0.9693 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2040 train_acc: 0.9984 val_loss: 0.2624 val_acc: 0.9665 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2035 train_acc: 0.9984 val_loss: 0.2627 val_acc: 0.9681 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2034 train_acc: 0.9987 val_loss: 0.2590 val_acc: 0.9685 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2033 train_acc: 0.9984 val_loss: 0.2601 val_acc: 0.9683 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2030 train_acc: 0.9990 val_loss: 0.2598 val_acc: 0.9699 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2030 train_acc: 0.9987 val_loss: 0.2613 val_acc: 0.9675 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.2029 train_acc: 0.9986 val_loss: 0.2591 val_acc: 0.9690 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2031 train_acc: 0.9987 val_loss: 0.2581 val_acc: 0.9710 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2026 train_acc: 0.9990 val_loss: 0.2604 val_acc: 0.9681 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.2027 train_acc: 0.9988 val_loss: 0.2611 val_acc: 0.9678 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.2027 train_acc: 0.9989 val_loss: 0.2631 val_acc: 0.9683 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.2023 train_acc: 0.9991 val_loss: 0.2597 val_acc: 0.9693 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.2023 train_acc: 0.9991 val_loss: 0.2607 val_acc: 0.9681 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.2022 train_acc: 0.9989 val_loss: 0.2605 val_acc: 0.9692 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.2019 train_acc: 0.9991 val_loss: 0.2601 val_acc: 0.9684 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.2020 train_acc: 0.9992 val_loss: 0.2600 val_acc: 0.9688 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2020 train_acc: 0.9991 val_loss: 0.2601 val_acc: 0.9693 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2020 train_acc: 0.9993 val_loss: 0.2615 val_acc: 0.9692 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2017 train_acc: 0.9992 val_loss: 0.2594 val_acc: 0.9709 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2013 train_acc: 0.9996 val_loss: 0.2600 val_acc: 0.9695 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2018 train_acc: 0.9991 val_loss: 0.2600 val_acc: 0.9696 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2020 train_acc: 0.9989 val_loss: 0.2608 val_acc: 0.9693 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2019 train_acc: 0.9992 val_loss: 0.2612 val_acc: 0.9690 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2018 train_acc: 0.9991 val_loss: 0.2592 val_acc: 0.9693 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2015 train_acc: 0.9993 val_loss: 0.2598 val_acc: 0.9691 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2018 train_acc: 0.9991 val_loss: 0.2601 val_acc: 0.9683 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2015 train_acc: 0.9994 val_loss: 0.2611 val_acc: 0.9688 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2016 train_acc: 0.9990 val_loss: 0.2612 val_acc: 0.9702 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2016 train_acc: 0.9992 val_loss: 0.2609 val_acc: 0.9688 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2012 train_acc: 0.9996 val_loss: 0.2619 val_acc: 0.9683 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2018 train_acc: 0.9991 val_loss: 0.2605 val_acc: 0.9692 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2014 train_acc: 0.9993 val_loss: 0.2624 val_acc: 0.9667 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.2013 train_acc: 0.9994 val_loss: 0.2607 val_acc: 0.9692 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2014 train_acc: 0.9994 val_loss: 0.2602 val_acc: 0.9689 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2012 train_acc: 0.9994 val_loss: 0.2613 val_acc: 0.9674 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2022 train_acc: 0.9989 val_loss: 0.2612 val_acc: 0.9678 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2016 train_acc: 0.9992 val_loss: 0.2609 val_acc: 0.9675 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.2017 train_acc: 0.9992 val_loss: 0.2587 val_acc: 0.9686 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9692\n",
      "14000: 0.9691666666666666\n",
      " test_acc: 0.9692\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5298 train_acc: 0.8211 val_loss: 0.4560 val_acc: 0.8357 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3836 train_acc: 0.8848 val_loss: 0.7236 val_acc: 0.6005 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3587 train_acc: 0.9036 val_loss: 0.4077 val_acc: 0.8617 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3434 train_acc: 0.9120 val_loss: 0.3443 val_acc: 0.9115 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.3323 train_acc: 0.9184 val_loss: 0.3834 val_acc: 0.8797 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.3255 train_acc: 0.9238 val_loss: 0.3297 val_acc: 0.9205 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.3163 train_acc: 0.9289 val_loss: 0.3170 val_acc: 0.9291 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.3064 train_acc: 0.9363 val_loss: 0.3754 val_acc: 0.8961 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.3038 train_acc: 0.9390 val_loss: 0.3049 val_acc: 0.9358 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.2964 train_acc: 0.9414 val_loss: 0.2965 val_acc: 0.9398 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.2933 train_acc: 0.9441 val_loss: 0.3050 val_acc: 0.9394 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.2884 train_acc: 0.9469 val_loss: 0.4616 val_acc: 0.8276 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.2833 train_acc: 0.9494 val_loss: 0.3071 val_acc: 0.9357 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.2807 train_acc: 0.9507 val_loss: 0.4232 val_acc: 0.8542 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2629 train_acc: 0.9624 val_loss: 0.2913 val_acc: 0.9431 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2556 train_acc: 0.9667 val_loss: 0.2975 val_acc: 0.9403 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.2509 train_acc: 0.9715 val_loss: 0.2827 val_acc: 0.9509 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.2488 train_acc: 0.9719 val_loss: 0.2920 val_acc: 0.9459 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.2452 train_acc: 0.9731 val_loss: 0.3510 val_acc: 0.9083 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.2423 train_acc: 0.9768 val_loss: 0.3268 val_acc: 0.9279 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.2392 train_acc: 0.9773 val_loss: 0.3501 val_acc: 0.9068 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2269 train_acc: 0.9854 val_loss: 0.2835 val_acc: 0.9538 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.2230 train_acc: 0.9882 val_loss: 0.3046 val_acc: 0.9389 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.2213 train_acc: 0.9896 val_loss: 0.2842 val_acc: 0.9513 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.2190 train_acc: 0.9911 val_loss: 0.3421 val_acc: 0.9242 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.2165 train_acc: 0.9916 val_loss: 0.2885 val_acc: 0.9529 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2139 train_acc: 0.9936 val_loss: 0.2953 val_acc: 0.9480 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2114 train_acc: 0.9950 val_loss: 0.2897 val_acc: 0.9519 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2103 train_acc: 0.9960 val_loss: 0.2858 val_acc: 0.9512 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2105 train_acc: 0.9955 val_loss: 0.2938 val_acc: 0.9507 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2076 train_acc: 0.9971 val_loss: 0.2858 val_acc: 0.9526 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2068 train_acc: 0.9975 val_loss: 0.2858 val_acc: 0.9534 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2068 train_acc: 0.9976 val_loss: 0.2883 val_acc: 0.9503 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2066 train_acc: 0.9980 val_loss: 0.2906 val_acc: 0.9523 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.2058 train_acc: 0.9984 val_loss: 0.2898 val_acc: 0.9514 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.2054 train_acc: 0.9981 val_loss: 0.2861 val_acc: 0.9527 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.2057 train_acc: 0.9977 val_loss: 0.2881 val_acc: 0.9550 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.2055 train_acc: 0.9981 val_loss: 0.2863 val_acc: 0.9523 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2052 train_acc: 0.9981 val_loss: 0.2854 val_acc: 0.9530 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2048 train_acc: 0.9987 val_loss: 0.2882 val_acc: 0.9521 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2049 train_acc: 0.9982 val_loss: 0.2910 val_acc: 0.9519 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.2048 train_acc: 0.9984 val_loss: 0.2905 val_acc: 0.9530 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2045 train_acc: 0.9984 val_loss: 0.2885 val_acc: 0.9514 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.2045 train_acc: 0.9985 val_loss: 0.2857 val_acc: 0.9536 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.2044 train_acc: 0.9984 val_loss: 0.2865 val_acc: 0.9526 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.2039 train_acc: 0.9987 val_loss: 0.2870 val_acc: 0.9527 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2040 train_acc: 0.9987 val_loss: 0.2874 val_acc: 0.9529 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2041 train_acc: 0.9988 val_loss: 0.2857 val_acc: 0.9527 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2041 train_acc: 0.9989 val_loss: 0.2898 val_acc: 0.9511 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2036 train_acc: 0.9989 val_loss: 0.2877 val_acc: 0.9519 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2038 train_acc: 0.9988 val_loss: 0.2881 val_acc: 0.9533 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2031 train_acc: 0.9992 val_loss: 0.2887 val_acc: 0.9525 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2036 train_acc: 0.9988 val_loss: 0.2871 val_acc: 0.9524 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2035 train_acc: 0.9989 val_loss: 0.2893 val_acc: 0.9514 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2035 train_acc: 0.9987 val_loss: 0.2877 val_acc: 0.9540 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2033 train_acc: 0.9990 val_loss: 0.2888 val_acc: 0.9509 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2031 train_acc: 0.9992 val_loss: 0.2880 val_acc: 0.9538 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2033 train_acc: 0.9989 val_loss: 0.2889 val_acc: 0.9520 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2031 train_acc: 0.9993 val_loss: 0.2918 val_acc: 0.9508 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2033 train_acc: 0.9990 val_loss: 0.2901 val_acc: 0.9520 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2033 train_acc: 0.9988 val_loss: 0.2877 val_acc: 0.9531 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2030 train_acc: 0.9993 val_loss: 0.2888 val_acc: 0.9520 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2030 train_acc: 0.9991 val_loss: 0.2892 val_acc: 0.9530 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2030 train_acc: 0.9993 val_loss: 0.2881 val_acc: 0.9523 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2030 train_acc: 0.9988 val_loss: 0.2887 val_acc: 0.9530 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2028 train_acc: 0.9993 val_loss: 0.2880 val_acc: 0.9523 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2028 train_acc: 0.9992 val_loss: 0.2888 val_acc: 0.9510 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9517\n",
      "14000: 0.9516666666666667\n",
      " test_acc: 0.9517\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.5110 train_acc: 0.8445 val_loss: 0.4886 val_acc: 0.8116 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.3466 train_acc: 0.9104 val_loss: 0.7788 val_acc: 0.6321 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.3194 train_acc: 0.9278 val_loss: 0.5487 val_acc: 0.7613 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.3076 train_acc: 0.9354 val_loss: 0.3460 val_acc: 0.9049 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.2949 train_acc: 0.9442 val_loss: 0.5725 val_acc: 0.7571 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.2890 train_acc: 0.9477 val_loss: 0.6115 val_acc: 0.7664 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.2807 train_acc: 0.9528 val_loss: 0.3034 val_acc: 0.9343 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.2761 train_acc: 0.9553 val_loss: 0.5940 val_acc: 0.7191 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.2717 train_acc: 0.9572 val_loss: 0.3276 val_acc: 0.9215 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.2667 train_acc: 0.9609 val_loss: 0.5910 val_acc: 0.7550 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.2652 train_acc: 0.9622 val_loss: 0.5588 val_acc: 0.7785 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.2468 train_acc: 0.9720 val_loss: 1.0273 val_acc: 0.5643 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.2416 train_acc: 0.9756 val_loss: 0.2827 val_acc: 0.9473 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.2387 train_acc: 0.9775 val_loss: 0.3370 val_acc: 0.9113 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.2371 train_acc: 0.9790 val_loss: 0.2847 val_acc: 0.9471 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.2349 train_acc: 0.9798 val_loss: 0.5044 val_acc: 0.8297 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.2297 train_acc: 0.9836 val_loss: 0.2934 val_acc: 0.9470 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.2208 train_acc: 0.9886 val_loss: 0.3100 val_acc: 0.9354 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.2192 train_acc: 0.9896 val_loss: 0.2706 val_acc: 0.9546 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.2161 train_acc: 0.9915 val_loss: 0.3460 val_acc: 0.9201 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.2149 train_acc: 0.9927 val_loss: 0.3345 val_acc: 0.9294 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.2132 train_acc: 0.9929 val_loss: 0.3083 val_acc: 0.9343 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.2124 train_acc: 0.9937 val_loss: 0.3250 val_acc: 0.9339 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.2084 train_acc: 0.9962 val_loss: 0.2745 val_acc: 0.9568 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.2070 train_acc: 0.9971 val_loss: 0.3140 val_acc: 0.9389 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.2063 train_acc: 0.9971 val_loss: 0.2711 val_acc: 0.9610 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.2056 train_acc: 0.9976 val_loss: 0.2764 val_acc: 0.9580 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.2045 train_acc: 0.9984 val_loss: 0.2704 val_acc: 0.9592 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.2045 train_acc: 0.9984 val_loss: 0.2738 val_acc: 0.9567 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.2039 train_acc: 0.9984 val_loss: 0.3082 val_acc: 0.9452 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.2033 train_acc: 0.9987 val_loss: 0.2765 val_acc: 0.9570 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.2026 train_acc: 0.9991 val_loss: 0.2877 val_acc: 0.9517 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.2026 train_acc: 0.9991 val_loss: 0.2673 val_acc: 0.9602 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2025 train_acc: 0.9991 val_loss: 0.2677 val_acc: 0.9617 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2028 train_acc: 0.9988 val_loss: 0.2717 val_acc: 0.9595 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2018 train_acc: 0.9994 val_loss: 0.2692 val_acc: 0.9618 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2018 train_acc: 0.9993 val_loss: 0.2809 val_acc: 0.9553 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2020 train_acc: 0.9994 val_loss: 0.2671 val_acc: 0.9615 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.2016 train_acc: 0.9994 val_loss: 0.2676 val_acc: 0.9629 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.2012 train_acc: 0.9996 val_loss: 0.2686 val_acc: 0.9622 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.2009 train_acc: 0.9997 val_loss: 0.2672 val_acc: 0.9618 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.2017 train_acc: 0.9993 val_loss: 0.3147 val_acc: 0.9392 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.2019 train_acc: 0.9990 val_loss: 0.2760 val_acc: 0.9588 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.2012 train_acc: 0.9994 val_loss: 0.2671 val_acc: 0.9629 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.2692 val_acc: 0.9612 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.2681 val_acc: 0.9621 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.2677 val_acc: 0.9629 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.2006 train_acc: 0.9996 val_loss: 0.2666 val_acc: 0.9628 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.2008 train_acc: 0.9996 val_loss: 0.2668 val_acc: 0.9624 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.2004 train_acc: 0.9996 val_loss: 0.2661 val_acc: 0.9632 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.2004 train_acc: 0.9998 val_loss: 0.2674 val_acc: 0.9622 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.2003 train_acc: 0.9998 val_loss: 0.2661 val_acc: 0.9628 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.2002 train_acc: 0.9999 val_loss: 0.2679 val_acc: 0.9634 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.2005 train_acc: 0.9996 val_loss: 0.2681 val_acc: 0.9624 lr: 7.8125e-05\n",
      "Epoch  54 / 200 train_loss: 0.2003 train_acc: 0.9997 val_loss: 0.2721 val_acc: 0.9608 lr: 7.8125e-05\n",
      "Epoch  55 / 200 train_loss: 0.2002 train_acc: 0.9999 val_loss: 0.2706 val_acc: 0.9622 lr: 7.8125e-05\n",
      "Epoch  56 / 200 train_loss: 0.2003 train_acc: 0.9998 val_loss: 0.2667 val_acc: 0.9622 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2002 train_acc: 0.9998 val_loss: 0.2666 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2673 val_acc: 0.9633 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2000 train_acc: 1.0000 val_loss: 0.2672 val_acc: 0.9629 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2680 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2663 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2676 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.1999 train_acc: 0.9999 val_loss: 0.2662 val_acc: 0.9630 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2001 train_acc: 0.9998 val_loss: 0.2665 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2001 train_acc: 0.9997 val_loss: 0.2678 val_acc: 0.9624 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2659 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2002 train_acc: 0.9998 val_loss: 0.2665 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.1998 train_acc: 1.0000 val_loss: 0.2668 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2663 val_acc: 0.9621 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.1998 train_acc: 0.9999 val_loss: 0.2668 val_acc: 0.9629 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2661 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.1999 train_acc: 1.0000 val_loss: 0.2665 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.1999 train_acc: 0.9999 val_loss: 0.2673 val_acc: 0.9626 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2669 val_acc: 0.9625 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.1999 train_acc: 1.0000 val_loss: 0.2677 val_acc: 0.9638 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2692 val_acc: 0.9605 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2662 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.1999 train_acc: 0.9998 val_loss: 0.2674 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.2000 train_acc: 0.9998 val_loss: 0.2688 val_acc: 0.9627 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2660 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.1999 train_acc: 0.9999 val_loss: 0.2672 val_acc: 0.9620 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.1997 train_acc: 1.0000 val_loss: 0.2678 val_acc: 0.9637 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.1998 train_acc: 0.9999 val_loss: 0.2667 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.1997 train_acc: 1.0000 val_loss: 0.2672 val_acc: 0.9641 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2677 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2684 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2664 val_acc: 0.9644 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.1996 train_acc: 1.0000 val_loss: 0.2671 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2664 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  90 / 200 train_loss: 0.1996 train_acc: 1.0000 val_loss: 0.2667 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  91 / 200 train_loss: 0.1996 train_acc: 1.0000 val_loss: 0.2661 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  92 / 200 train_loss: 0.1999 train_acc: 0.9997 val_loss: 0.2689 val_acc: 0.9619 lr: 5e-05\n",
      "Epoch  93 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2696 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  94 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2666 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch  95 / 200 train_loss: 0.1998 train_acc: 0.9998 val_loss: 0.2675 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch  96 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2674 val_acc: 0.9631 lr: 5e-05\n",
      "Epoch  97 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2664 val_acc: 0.9641 lr: 5e-05\n",
      "Epoch  98 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2684 val_acc: 0.9627 lr: 5e-05\n",
      "Epoch  99 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2679 val_acc: 0.9624 lr: 5e-05\n",
      "Epoch 100 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2674 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch 101 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2676 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch 102 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2663 val_acc: 0.9644 lr: 5e-05\n",
      "Epoch 103 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2666 val_acc: 0.9638 lr: 5e-05\n",
      "Epoch 104 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2672 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch 105 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2671 val_acc: 0.9655 lr: 5e-05\n",
      "Epoch 106 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2678 val_acc: 0.9621 lr: 5e-05\n",
      "Epoch 107 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2680 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch 108 / 200 train_loss: 0.1996 train_acc: 0.9998 val_loss: 0.2678 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch 109 / 200 train_loss: 0.1995 train_acc: 0.9998 val_loss: 0.2723 val_acc: 0.9610 lr: 5e-05\n",
      "Epoch 110 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2679 val_acc: 0.9641 lr: 5e-05\n",
      "Epoch 111 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2685 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch 112 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2693 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch 113 / 200 train_loss: 0.1997 train_acc: 0.9999 val_loss: 0.2680 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch 114 / 200 train_loss: 0.1996 train_acc: 0.9999 val_loss: 0.2686 val_acc: 0.9621 lr: 5e-05\n",
      "Epoch 115 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2673 val_acc: 0.9639 lr: 5e-05\n",
      "Epoch 116 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2670 val_acc: 0.9625 lr: 5e-05\n",
      "Epoch 117 / 200 train_loss: 0.1995 train_acc: 0.9999 val_loss: 0.2709 val_acc: 0.9618 lr: 5e-05\n",
      "Epoch 118 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2669 val_acc: 0.9642 lr: 5e-05\n",
      "Epoch 119 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2685 val_acc: 0.9637 lr: 5e-05\n",
      "Epoch 120 / 200 train_loss: 0.1995 train_acc: 1.0000 val_loss: 0.2686 val_acc: 0.9628 lr: 5e-05\n",
      "Epoch 121 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2695 val_acc: 0.9618 lr: 5e-05\n",
      "Epoch 122 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2680 val_acc: 0.9627 lr: 5e-05\n",
      "Epoch 123 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2688 val_acc: 0.9638 lr: 5e-05\n",
      "Epoch 124 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2684 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch 125 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2690 val_acc: 0.9635 lr: 5e-05\n",
      "Epoch 126 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2688 val_acc: 0.9640 lr: 5e-05\n",
      "Epoch 127 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2692 val_acc: 0.9627 lr: 5e-05\n",
      "Epoch 128 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2678 val_acc: 0.9632 lr: 5e-05\n",
      "Epoch 129 / 200 train_loss: 0.1994 train_acc: 0.9999 val_loss: 0.2677 val_acc: 0.9629 lr: 5e-05\n",
      "Epoch 130 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2692 val_acc: 0.9618 lr: 5e-05\n",
      "Epoch 131 / 200 train_loss: 0.1997 train_acc: 0.9998 val_loss: 0.2671 val_acc: 0.9634 lr: 5e-05\n",
      "Epoch 132 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2675 val_acc: 0.9638 lr: 5e-05\n",
      "Epoch 133 / 200 train_loss: 0.1993 train_acc: 1.0000 val_loss: 0.2665 val_acc: 0.9648 lr: 5e-05\n",
      "Epoch 134 / 200 train_loss: 0.1994 train_acc: 1.0000 val_loss: 0.2680 val_acc: 0.9612 lr: 5e-05\n",
      "Epoch 135 / 200 train_loss: 0.1992 train_acc: 1.0000 val_loss: 0.2699 val_acc: 0.9617 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9665\n",
      "14000: 0.9665\n",
      " test_acc: 0.9665\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c18</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c02</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c03</td>\n",
       "      <td>0.729667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c04</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c05</td>\n",
       "      <td>0.894778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c06</td>\n",
       "      <td>0.733467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c07</td>\n",
       "      <td>0.797750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c08</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c09</td>\n",
       "      <td>0.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c10</td>\n",
       "      <td>0.831667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c11</td>\n",
       "      <td>0.751333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c12</td>\n",
       "      <td>0.836667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c15</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c16</td>\n",
       "      <td>0.969167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>c17</td>\n",
       "      <td>0.951667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c18</td>\n",
       "      <td>0.966500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c18       acc\n",
       "0   c02  0.935000\n",
       "1   c03  0.729667\n",
       "2   c04  0.775000\n",
       "3   c05  0.894778\n",
       "4   c06  0.733467\n",
       "5   c07  0.797750\n",
       "6   c08  0.710667\n",
       "7   c09  0.767500\n",
       "8   c10  0.831667\n",
       "9   c11  0.751333\n",
       "10  c12  0.836667\n",
       "11  c15  0.915000\n",
       "12  c16  0.969167\n",
       "13  c17  0.951667\n",
       "14  c18  0.966500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = []\n",
    "for task_name, task_peps in all_task_dict.items():\n",
    "    torch.set_num_threads(10)\n",
    "    \n",
    "    all_peps = task_peps\n",
    "    \n",
    "    y_code_dict = nps.ml.set_y_codes_for_classes(np.array(all_peps)[:,None])\n",
    "    y_to_label_dict = {v:k for k,v in y_code_dict.items()}\n",
    "\n",
    "    train_objs = [f\"../../../00.data/GSXGS/{pep}_valid80.pkl\" for pep in all_peps]\n",
    "    test_objs = [f\"../../../00.data/GSXGS/{pep}_valid20.pkl\" for pep in all_peps]\n",
    "    labels = all_peps\n",
    "\n",
    "    acc = train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name=f'valid_data_{task_name}', train_sample_size=14000)\n",
    "\n",
    "    acc_df.append([task_name, acc])\n",
    "    \n",
    "acc_df = pd.DataFrame(acc_df)\n",
    "acc_df.columns = [task_name, 'acc']\n",
    "acc_df.to_csv(\"../../../03.results/classification_on_clean_data/GSXGS/diff_task/valid/acc.csv\")\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586cd0f-795f-4e7e-807e-71de5779b00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df91d0-b5c4-4e2f-94be-223361323d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
