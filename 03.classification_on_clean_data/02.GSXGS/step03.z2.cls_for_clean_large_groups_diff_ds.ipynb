{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/uesr/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea64e7f-45b8-46db-b049-23707b3a8ea7",
   "metadata": {},
   "source": [
    "# 全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1K': 0,\n",
       "  '1R': 0,\n",
       "  '1D': 1,\n",
       "  '1E': 1,\n",
       "  '1F': 2,\n",
       "  '1W': 2,\n",
       "  '1Y': 2,\n",
       "  '1I': 2,\n",
       "  '1L': 2,\n",
       "  '1M': 2,\n",
       "  '1V': 2,\n",
       "  '1H': 2,\n",
       "  '1Q': 2,\n",
       "  '1A': 2,\n",
       "  '1G': 2,\n",
       "  '1S': 2,\n",
       "  '1C': 2,\n",
       "  '1P': 2,\n",
       "  '1T': 2,\n",
       "  '1N': 2},\n",
       " {0: 'positive', 1: 'negative', 2: 'neutral'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "\n",
    "all_peps = [['1K', '1R'], ['1D', '1E'], ['1F', '1W', '1Y', '1I', '1L', '1M', '1V', '1H', '1Q', '1A', '1G', '1S', '1C', '1P', '1T', '1N']]\n",
    "\n",
    "y_code_dict = nps.ml.set_y_codes_for_classes(all_peps)\n",
    "y_code_dict\n",
    "y_to_label_dict = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "y_code_dict, y_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5052bfdf-370e-4fcd-8e70-f4dd3f41f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in all_peps:\n",
    "    tmp.extend(i)\n",
    "all_peps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591e8a7d-e387-4f28-b79d-9e7a3c7beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs = [f\"../../../03.results/classification_on_clean_data/GSXGS/{pep}/{pep}_valid80_clean_obj.pkl\" for pep in all_peps]\n",
    "test_objs = [f\"../../../03.results/classification_on_clean_data/GSXGS/{pep}/{pep}_valid20_clean_obj.pkl\" for pep in all_peps]\n",
    "labels = all_peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a4b762-6fee-4531-b733-838c7f758d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4376c13c-8a5e-4ccf-89c8-cf82c528cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train_objs, test_objs, labels, train_name='clean_data', train_sample_size=14000):\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=42)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=42, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=6000, random_state=42)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    nps.ml.seed_everything(42)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(y_to_label_dict), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=y_to_label_dict.values())\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    cm_df.to_csv(f\"../../../04.tables/classification/GSXGS/clean/{train_name}_cm.csv\")\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../04.tables/classification/GSXGS/clean/{train_name}_pred_proba.csv\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae31691-842e-41db-93ae-63cfb4241d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.6851 train_acc: 0.7914 val_loss: 0.7524 val_acc: 0.7495 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5368 train_acc: 0.8674 val_loss: 0.5591 val_acc: 0.8465 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.4976 train_acc: 0.8904 val_loss: 0.7793 val_acc: 0.6826 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.4796 train_acc: 0.9004 val_loss: 0.6300 val_acc: 0.8097 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.4640 train_acc: 0.9082 val_loss: 0.9814 val_acc: 0.6038 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.4526 train_acc: 0.9146 val_loss: 0.5869 val_acc: 0.8353 lr: 0.0025\n",
      "Epoch   6 / 200 train_loss: 0.4261 train_acc: 0.9286 val_loss: 0.4411 val_acc: 0.9180 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.4173 train_acc: 0.9333 val_loss: 0.4457 val_acc: 0.9131 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.4089 train_acc: 0.9379 val_loss: 0.4353 val_acc: 0.9207 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.4023 train_acc: 0.9418 val_loss: 0.4314 val_acc: 0.9226 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.3958 train_acc: 0.9451 val_loss: 0.4305 val_acc: 0.9243 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.3880 train_acc: 0.9495 val_loss: 0.4900 val_acc: 0.8932 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.3821 train_acc: 0.9528 val_loss: 0.4379 val_acc: 0.9222 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.3765 train_acc: 0.9557 val_loss: 0.4416 val_acc: 0.9232 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.3708 train_acc: 0.9596 val_loss: 0.4443 val_acc: 0.9193 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.3526 train_acc: 0.9695 val_loss: 0.4256 val_acc: 0.9299 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.3473 train_acc: 0.9728 val_loss: 0.4203 val_acc: 0.9323 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.3419 train_acc: 0.9752 val_loss: 0.4276 val_acc: 0.9271 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3394 train_acc: 0.9772 val_loss: 0.4337 val_acc: 0.9276 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3347 train_acc: 0.9798 val_loss: 0.4274 val_acc: 0.9289 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3322 train_acc: 0.9818 val_loss: 0.4422 val_acc: 0.9211 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3241 train_acc: 0.9860 val_loss: 0.4357 val_acc: 0.9302 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3209 train_acc: 0.9876 val_loss: 0.4360 val_acc: 0.9280 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.3188 train_acc: 0.9888 val_loss: 0.4402 val_acc: 0.9263 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3164 train_acc: 0.9903 val_loss: 0.4394 val_acc: 0.9253 lr: 0.0003125\n",
      "Epoch  25 / 200 train_loss: 0.3136 train_acc: 0.9917 val_loss: 0.4321 val_acc: 0.9295 lr: 0.0003125\n",
      "Epoch  26 / 200 train_loss: 0.3117 train_acc: 0.9928 val_loss: 0.4346 val_acc: 0.9290 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.3106 train_acc: 0.9933 val_loss: 0.4343 val_acc: 0.9287 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.3100 train_acc: 0.9941 val_loss: 0.4365 val_acc: 0.9276 lr: 0.00015625\n",
      "Epoch  29 / 200 train_loss: 0.3082 train_acc: 0.9948 val_loss: 0.4345 val_acc: 0.9296 lr: 0.00015625\n",
      "Epoch  30 / 200 train_loss: 0.3074 train_acc: 0.9949 val_loss: 0.4354 val_acc: 0.9283 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.3068 train_acc: 0.9953 val_loss: 0.4362 val_acc: 0.9296 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.3068 train_acc: 0.9954 val_loss: 0.4375 val_acc: 0.9285 lr: 7.8125e-05\n",
      "Epoch  33 / 200 train_loss: 0.3056 train_acc: 0.9963 val_loss: 0.4373 val_acc: 0.9294 lr: 7.8125e-05\n",
      "Epoch  34 / 200 train_loss: 0.3053 train_acc: 0.9960 val_loss: 0.4364 val_acc: 0.9281 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.3053 train_acc: 0.9960 val_loss: 0.4368 val_acc: 0.9288 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.3049 train_acc: 0.9964 val_loss: 0.4381 val_acc: 0.9298 lr: 5e-05\n",
      "Epoch  37 / 200 train_loss: 0.3046 train_acc: 0.9963 val_loss: 0.4367 val_acc: 0.9307 lr: 5e-05\n",
      "Epoch  38 / 200 train_loss: 0.3042 train_acc: 0.9967 val_loss: 0.4372 val_acc: 0.9292 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.3041 train_acc: 0.9966 val_loss: 0.4371 val_acc: 0.9292 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3040 train_acc: 0.9967 val_loss: 0.4386 val_acc: 0.9269 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3037 train_acc: 0.9970 val_loss: 0.4379 val_acc: 0.9287 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3036 train_acc: 0.9971 val_loss: 0.4372 val_acc: 0.9284 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3034 train_acc: 0.9972 val_loss: 0.4360 val_acc: 0.9284 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3035 train_acc: 0.9969 val_loss: 0.4373 val_acc: 0.9284 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3033 train_acc: 0.9973 val_loss: 0.4378 val_acc: 0.9285 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3029 train_acc: 0.9973 val_loss: 0.4378 val_acc: 0.9290 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9331\n",
      "25000: 0.9330555555555555\n",
      " test_acc: 0.9331\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.6853 train_acc: 0.7962 val_loss: 0.8021 val_acc: 0.7134 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5406 train_acc: 0.8664 val_loss: 0.8168 val_acc: 0.7210 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5055 train_acc: 0.8863 val_loss: 0.4818 val_acc: 0.9016 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.4851 train_acc: 0.8963 val_loss: 0.6956 val_acc: 0.7649 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.4706 train_acc: 0.9048 val_loss: 0.5006 val_acc: 0.8892 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.4589 train_acc: 0.9124 val_loss: 0.5519 val_acc: 0.8530 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4483 train_acc: 0.9172 val_loss: 0.8282 val_acc: 0.7212 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.4196 train_acc: 0.9320 val_loss: 0.6042 val_acc: 0.8207 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.4118 train_acc: 0.9362 val_loss: 0.4601 val_acc: 0.9096 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.4040 train_acc: 0.9405 val_loss: 0.4677 val_acc: 0.9086 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.3943 train_acc: 0.9465 val_loss: 0.7452 val_acc: 0.7733 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.3884 train_acc: 0.9492 val_loss: 0.4872 val_acc: 0.8965 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.3794 train_acc: 0.9549 val_loss: 0.4624 val_acc: 0.9139 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.3733 train_acc: 0.9576 val_loss: 0.5322 val_acc: 0.8700 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.3671 train_acc: 0.9614 val_loss: 0.5064 val_acc: 0.8861 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.3622 train_acc: 0.9640 val_loss: 0.4625 val_acc: 0.9105 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.3559 train_acc: 0.9676 val_loss: 0.4745 val_acc: 0.9062 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.3388 train_acc: 0.9774 val_loss: 0.4594 val_acc: 0.9153 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3328 train_acc: 0.9810 val_loss: 0.4668 val_acc: 0.9141 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3290 train_acc: 0.9828 val_loss: 0.4409 val_acc: 0.9232 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3258 train_acc: 0.9842 val_loss: 0.4375 val_acc: 0.9266 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3218 train_acc: 0.9872 val_loss: 0.4668 val_acc: 0.9117 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3208 train_acc: 0.9873 val_loss: 0.4338 val_acc: 0.9281 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3167 train_acc: 0.9898 val_loss: 0.4386 val_acc: 0.9259 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.3147 train_acc: 0.9904 val_loss: 0.4502 val_acc: 0.9231 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.3123 train_acc: 0.9918 val_loss: 0.4448 val_acc: 0.9262 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.3109 train_acc: 0.9932 val_loss: 0.4482 val_acc: 0.9267 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.3053 train_acc: 0.9956 val_loss: 0.4396 val_acc: 0.9286 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.3034 train_acc: 0.9968 val_loss: 0.4386 val_acc: 0.9304 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.3024 train_acc: 0.9973 val_loss: 0.4461 val_acc: 0.9269 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.3011 train_acc: 0.9976 val_loss: 0.4438 val_acc: 0.9274 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.3006 train_acc: 0.9978 val_loss: 0.4450 val_acc: 0.9283 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.3003 train_acc: 0.9978 val_loss: 0.4407 val_acc: 0.9297 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.2987 train_acc: 0.9982 val_loss: 0.4375 val_acc: 0.9331 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.2976 train_acc: 0.9986 val_loss: 0.4381 val_acc: 0.9316 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.2974 train_acc: 0.9989 val_loss: 0.4396 val_acc: 0.9313 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.2970 train_acc: 0.9990 val_loss: 0.4438 val_acc: 0.9314 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.2971 train_acc: 0.9988 val_loss: 0.4399 val_acc: 0.9326 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.2966 train_acc: 0.9990 val_loss: 0.4381 val_acc: 0.9320 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.2960 train_acc: 0.9992 val_loss: 0.4368 val_acc: 0.9326 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.2962 train_acc: 0.9990 val_loss: 0.4400 val_acc: 0.9302 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.2957 train_acc: 0.9992 val_loss: 0.4388 val_acc: 0.9314 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.2954 train_acc: 0.9993 val_loss: 0.4412 val_acc: 0.9313 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.2955 train_acc: 0.9993 val_loss: 0.4379 val_acc: 0.9317 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.2954 train_acc: 0.9993 val_loss: 0.4385 val_acc: 0.9323 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.2951 train_acc: 0.9994 val_loss: 0.4408 val_acc: 0.9309 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.2949 train_acc: 0.9994 val_loss: 0.4400 val_acc: 0.9324 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.2949 train_acc: 0.9995 val_loss: 0.4379 val_acc: 0.9332 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2949 train_acc: 0.9994 val_loss: 0.4375 val_acc: 0.9323 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2948 train_acc: 0.9994 val_loss: 0.4396 val_acc: 0.9312 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2948 train_acc: 0.9995 val_loss: 0.4414 val_acc: 0.9304 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2947 train_acc: 0.9995 val_loss: 0.4403 val_acc: 0.9314 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2946 train_acc: 0.9996 val_loss: 0.4402 val_acc: 0.9331 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2947 train_acc: 0.9996 val_loss: 0.4394 val_acc: 0.9322 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2944 train_acc: 0.9996 val_loss: 0.4374 val_acc: 0.9330 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2946 train_acc: 0.9995 val_loss: 0.4400 val_acc: 0.9311 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2946 train_acc: 0.9995 val_loss: 0.4406 val_acc: 0.9319 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2945 train_acc: 0.9995 val_loss: 0.4432 val_acc: 0.9292 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2944 train_acc: 0.9997 val_loss: 0.4389 val_acc: 0.9332 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2945 train_acc: 0.9995 val_loss: 0.4392 val_acc: 0.9312 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2945 train_acc: 0.9995 val_loss: 0.4394 val_acc: 0.9315 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2944 train_acc: 0.9995 val_loss: 0.4414 val_acc: 0.9308 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2944 train_acc: 0.9995 val_loss: 0.4389 val_acc: 0.9325 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2943 train_acc: 0.9996 val_loss: 0.4393 val_acc: 0.9323 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2942 train_acc: 0.9996 val_loss: 0.4399 val_acc: 0.9318 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2942 train_acc: 0.9996 val_loss: 0.4409 val_acc: 0.9308 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.2942 train_acc: 0.9996 val_loss: 0.4405 val_acc: 0.9313 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.2943 train_acc: 0.9995 val_loss: 0.4393 val_acc: 0.9318 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.2943 train_acc: 0.9995 val_loss: 0.4426 val_acc: 0.9309 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.2941 train_acc: 0.9996 val_loss: 0.4429 val_acc: 0.9314 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.2942 train_acc: 0.9996 val_loss: 0.4402 val_acc: 0.9323 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.2941 train_acc: 0.9996 val_loss: 0.4444 val_acc: 0.9291 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.2940 train_acc: 0.9997 val_loss: 0.4410 val_acc: 0.9315 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.2939 train_acc: 0.9997 val_loss: 0.4400 val_acc: 0.9313 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.2938 train_acc: 0.9997 val_loss: 0.4403 val_acc: 0.9307 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.2939 train_acc: 0.9997 val_loss: 0.4415 val_acc: 0.9315 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.2942 train_acc: 0.9996 val_loss: 0.4414 val_acc: 0.9307 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.2939 train_acc: 0.9998 val_loss: 0.4442 val_acc: 0.9296 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9287\n",
      "20000: 0.9287222222222222\n",
      " test_acc: 0.9287\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7214 train_acc: 0.7728 val_loss: 0.8610 val_acc: 0.6750 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5648 train_acc: 0.8496 val_loss: 0.5878 val_acc: 0.8381 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5306 train_acc: 0.8695 val_loss: 0.5094 val_acc: 0.8893 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5101 train_acc: 0.8819 val_loss: 0.6787 val_acc: 0.7726 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.4948 train_acc: 0.8901 val_loss: 0.5469 val_acc: 0.8501 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.4814 train_acc: 0.8984 val_loss: 0.7244 val_acc: 0.7416 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4715 train_acc: 0.9051 val_loss: 0.6270 val_acc: 0.8114 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.4393 train_acc: 0.9226 val_loss: 0.4684 val_acc: 0.9047 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.4317 train_acc: 0.9261 val_loss: 0.4836 val_acc: 0.8972 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.4229 train_acc: 0.9296 val_loss: 0.5226 val_acc: 0.8756 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.4158 train_acc: 0.9345 val_loss: 0.5920 val_acc: 0.8373 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4104 train_acc: 0.9381 val_loss: 0.4722 val_acc: 0.8998 lr: 0.00125\n",
      "Epoch  12 / 200 train_loss: 0.3873 train_acc: 0.9518 val_loss: 0.4270 val_acc: 0.9267 lr: 0.00125\n",
      "Epoch  13 / 200 train_loss: 0.3761 train_acc: 0.9576 val_loss: 0.5548 val_acc: 0.8604 lr: 0.00125\n",
      "Epoch  14 / 200 train_loss: 0.3709 train_acc: 0.9595 val_loss: 0.4545 val_acc: 0.9129 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.3655 train_acc: 0.9623 val_loss: 0.4406 val_acc: 0.9219 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.3578 train_acc: 0.9676 val_loss: 0.4731 val_acc: 0.9057 lr: 0.000625\n",
      "Epoch  17 / 200 train_loss: 0.3470 train_acc: 0.9737 val_loss: 0.4263 val_acc: 0.9294 lr: 0.000625\n",
      "Epoch  18 / 200 train_loss: 0.3425 train_acc: 0.9767 val_loss: 0.4534 val_acc: 0.9176 lr: 0.000625\n",
      "Epoch  19 / 200 train_loss: 0.3387 train_acc: 0.9784 val_loss: 0.4462 val_acc: 0.9210 lr: 0.000625\n",
      "Epoch  20 / 200 train_loss: 0.3356 train_acc: 0.9802 val_loss: 0.4574 val_acc: 0.9145 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3337 train_acc: 0.9815 val_loss: 0.4419 val_acc: 0.9257 lr: 0.0003125\n",
      "Epoch  22 / 200 train_loss: 0.3271 train_acc: 0.9849 val_loss: 0.4342 val_acc: 0.9284 lr: 0.0003125\n",
      "Epoch  23 / 200 train_loss: 0.3244 train_acc: 0.9869 val_loss: 0.4398 val_acc: 0.9282 lr: 0.0003125\n",
      "Epoch  24 / 200 train_loss: 0.3229 train_acc: 0.9872 val_loss: 0.4362 val_acc: 0.9251 lr: 0.0003125\n",
      "Epoch  25 / 200 train_loss: 0.3207 train_acc: 0.9886 val_loss: 0.4353 val_acc: 0.9276 lr: 0.00015625\n",
      "Epoch  26 / 200 train_loss: 0.3181 train_acc: 0.9900 val_loss: 0.4362 val_acc: 0.9273 lr: 0.00015625\n",
      "Epoch  27 / 200 train_loss: 0.3170 train_acc: 0.9905 val_loss: 0.4336 val_acc: 0.9304 lr: 0.00015625\n",
      "Epoch  28 / 200 train_loss: 0.3164 train_acc: 0.9911 val_loss: 0.4370 val_acc: 0.9285 lr: 0.00015625\n",
      "Epoch  29 / 200 train_loss: 0.3153 train_acc: 0.9913 val_loss: 0.4369 val_acc: 0.9296 lr: 0.00015625\n",
      "Epoch  30 / 200 train_loss: 0.3143 train_acc: 0.9922 val_loss: 0.4369 val_acc: 0.9282 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.3138 train_acc: 0.9926 val_loss: 0.4381 val_acc: 0.9266 lr: 7.8125e-05\n",
      "Epoch  32 / 200 train_loss: 0.3123 train_acc: 0.9929 val_loss: 0.4365 val_acc: 0.9294 lr: 7.8125e-05\n",
      "Epoch  33 / 200 train_loss: 0.3113 train_acc: 0.9940 val_loss: 0.4363 val_acc: 0.9289 lr: 7.8125e-05\n",
      "Epoch  34 / 200 train_loss: 0.3114 train_acc: 0.9941 val_loss: 0.4385 val_acc: 0.9274 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.3116 train_acc: 0.9935 val_loss: 0.4377 val_acc: 0.9283 lr: 5e-05\n",
      "Epoch  36 / 200 train_loss: 0.3109 train_acc: 0.9941 val_loss: 0.4372 val_acc: 0.9295 lr: 5e-05\n",
      "Epoch  37 / 200 train_loss: 0.3109 train_acc: 0.9941 val_loss: 0.4377 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  38 / 200 train_loss: 0.3104 train_acc: 0.9940 val_loss: 0.4375 val_acc: 0.9267 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.3100 train_acc: 0.9943 val_loss: 0.4382 val_acc: 0.9294 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3097 train_acc: 0.9944 val_loss: 0.4375 val_acc: 0.9315 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3094 train_acc: 0.9949 val_loss: 0.4371 val_acc: 0.9307 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3090 train_acc: 0.9947 val_loss: 0.4395 val_acc: 0.9280 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3093 train_acc: 0.9944 val_loss: 0.4363 val_acc: 0.9304 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3082 train_acc: 0.9954 val_loss: 0.4380 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3085 train_acc: 0.9949 val_loss: 0.4385 val_acc: 0.9297 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3080 train_acc: 0.9953 val_loss: 0.4390 val_acc: 0.9281 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3081 train_acc: 0.9955 val_loss: 0.4392 val_acc: 0.9282 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3075 train_acc: 0.9958 val_loss: 0.4396 val_acc: 0.9277 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3071 train_acc: 0.9960 val_loss: 0.4387 val_acc: 0.9301 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3079 train_acc: 0.9956 val_loss: 0.4392 val_acc: 0.9299 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3073 train_acc: 0.9956 val_loss: 0.4384 val_acc: 0.9273 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3071 train_acc: 0.9959 val_loss: 0.4425 val_acc: 0.9272 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3067 train_acc: 0.9962 val_loss: 0.4397 val_acc: 0.9295 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3067 train_acc: 0.9962 val_loss: 0.4383 val_acc: 0.9274 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3063 train_acc: 0.9963 val_loss: 0.4412 val_acc: 0.9262 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3067 train_acc: 0.9958 val_loss: 0.4391 val_acc: 0.9303 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3062 train_acc: 0.9961 val_loss: 0.4388 val_acc: 0.9303 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3063 train_acc: 0.9960 val_loss: 0.4408 val_acc: 0.9280 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3062 train_acc: 0.9963 val_loss: 0.4416 val_acc: 0.9258 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3056 train_acc: 0.9965 val_loss: 0.4406 val_acc: 0.9294 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3050 train_acc: 0.9971 val_loss: 0.4407 val_acc: 0.9274 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3054 train_acc: 0.9967 val_loss: 0.4402 val_acc: 0.9284 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3052 train_acc: 0.9968 val_loss: 0.4395 val_acc: 0.9292 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3048 train_acc: 0.9970 val_loss: 0.4398 val_acc: 0.9306 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3044 train_acc: 0.9972 val_loss: 0.4400 val_acc: 0.9292 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3051 train_acc: 0.9969 val_loss: 0.4416 val_acc: 0.9278 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3045 train_acc: 0.9970 val_loss: 0.4404 val_acc: 0.9295 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3039 train_acc: 0.9973 val_loss: 0.4416 val_acc: 0.9287 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.3042 train_acc: 0.9969 val_loss: 0.4418 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.3040 train_acc: 0.9972 val_loss: 0.4412 val_acc: 0.9292 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9243\n",
      "15000: 0.9242777777777778\n",
      " test_acc: 0.9243\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.8463 train_acc: 0.7047 val_loss: 0.6950 val_acc: 0.7753 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6061 train_acc: 0.8267 val_loss: 0.6352 val_acc: 0.8106 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5675 train_acc: 0.8506 val_loss: 0.6500 val_acc: 0.7957 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5451 train_acc: 0.8642 val_loss: 0.9019 val_acc: 0.6290 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5240 train_acc: 0.8742 val_loss: 0.6051 val_acc: 0.8268 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5063 train_acc: 0.8844 val_loss: 0.7787 val_acc: 0.7315 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4944 train_acc: 0.8920 val_loss: 0.5280 val_acc: 0.8682 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.4860 train_acc: 0.8956 val_loss: 0.7618 val_acc: 0.7304 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4736 train_acc: 0.9030 val_loss: 0.9336 val_acc: 0.6309 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4626 train_acc: 0.9094 val_loss: 0.5778 val_acc: 0.8397 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4545 train_acc: 0.9137 val_loss: 1.0391 val_acc: 0.6274 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4235 train_acc: 0.9325 val_loss: 0.4835 val_acc: 0.8993 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4142 train_acc: 0.9369 val_loss: 0.4902 val_acc: 0.8962 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4039 train_acc: 0.9419 val_loss: 0.5365 val_acc: 0.8590 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.3988 train_acc: 0.9454 val_loss: 0.4504 val_acc: 0.9168 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.3906 train_acc: 0.9503 val_loss: 0.4787 val_acc: 0.9027 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.3831 train_acc: 0.9535 val_loss: 0.5054 val_acc: 0.8915 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.3746 train_acc: 0.9603 val_loss: 0.5554 val_acc: 0.8643 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.3684 train_acc: 0.9632 val_loss: 0.5136 val_acc: 0.8850 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3493 train_acc: 0.9729 val_loss: 0.4486 val_acc: 0.9215 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3421 train_acc: 0.9781 val_loss: 0.4564 val_acc: 0.9162 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3379 train_acc: 0.9807 val_loss: 0.4600 val_acc: 0.9123 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3337 train_acc: 0.9821 val_loss: 0.4534 val_acc: 0.9195 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3310 train_acc: 0.9846 val_loss: 0.4730 val_acc: 0.9115 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3233 train_acc: 0.9885 val_loss: 0.4478 val_acc: 0.9193 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.3202 train_acc: 0.9904 val_loss: 0.4507 val_acc: 0.9186 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3164 train_acc: 0.9925 val_loss: 0.4466 val_acc: 0.9233 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.3143 train_acc: 0.9930 val_loss: 0.4508 val_acc: 0.9215 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.3139 train_acc: 0.9933 val_loss: 0.4524 val_acc: 0.9193 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.3122 train_acc: 0.9940 val_loss: 0.4497 val_acc: 0.9230 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.3114 train_acc: 0.9948 val_loss: 0.4460 val_acc: 0.9256 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.3095 train_acc: 0.9954 val_loss: 0.4547 val_acc: 0.9201 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.3097 train_acc: 0.9954 val_loss: 0.4494 val_acc: 0.9205 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.3073 train_acc: 0.9968 val_loss: 0.4566 val_acc: 0.9180 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.3068 train_acc: 0.9970 val_loss: 0.4552 val_acc: 0.9203 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.3040 train_acc: 0.9979 val_loss: 0.4528 val_acc: 0.9260 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.3036 train_acc: 0.9979 val_loss: 0.4547 val_acc: 0.9191 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.3036 train_acc: 0.9980 val_loss: 0.4496 val_acc: 0.9232 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.3027 train_acc: 0.9984 val_loss: 0.4575 val_acc: 0.9179 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.3015 train_acc: 0.9990 val_loss: 0.4554 val_acc: 0.9189 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.3008 train_acc: 0.9991 val_loss: 0.4523 val_acc: 0.9227 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.3008 train_acc: 0.9987 val_loss: 0.4548 val_acc: 0.9206 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.3011 train_acc: 0.9988 val_loss: 0.4535 val_acc: 0.9225 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.3006 train_acc: 0.9989 val_loss: 0.4516 val_acc: 0.9246 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.3000 train_acc: 0.9990 val_loss: 0.4541 val_acc: 0.9200 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.2997 train_acc: 0.9992 val_loss: 0.4513 val_acc: 0.9221 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.2996 train_acc: 0.9992 val_loss: 0.4532 val_acc: 0.9209 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.2998 train_acc: 0.9988 val_loss: 0.4520 val_acc: 0.9204 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.2994 train_acc: 0.9991 val_loss: 0.4528 val_acc: 0.9208 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.2990 train_acc: 0.9995 val_loss: 0.4507 val_acc: 0.9220 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.2989 train_acc: 0.9995 val_loss: 0.4532 val_acc: 0.9221 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.2991 train_acc: 0.9993 val_loss: 0.4533 val_acc: 0.9243 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.2989 train_acc: 0.9995 val_loss: 0.4518 val_acc: 0.9238 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.2992 train_acc: 0.9994 val_loss: 0.4541 val_acc: 0.9220 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.2991 train_acc: 0.9993 val_loss: 0.4542 val_acc: 0.9204 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.2987 train_acc: 0.9995 val_loss: 0.4554 val_acc: 0.9218 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.2986 train_acc: 0.9992 val_loss: 0.4532 val_acc: 0.9230 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.2986 train_acc: 0.9994 val_loss: 0.4540 val_acc: 0.9221 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.2985 train_acc: 0.9996 val_loss: 0.4526 val_acc: 0.9216 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.2986 train_acc: 0.9996 val_loss: 0.4536 val_acc: 0.9234 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.2989 train_acc: 0.9990 val_loss: 0.4509 val_acc: 0.9233 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.2982 train_acc: 0.9995 val_loss: 0.4514 val_acc: 0.9212 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.2983 train_acc: 0.9995 val_loss: 0.4533 val_acc: 0.9215 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.2983 train_acc: 0.9994 val_loss: 0.4540 val_acc: 0.9216 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.2982 train_acc: 0.9995 val_loss: 0.4541 val_acc: 0.9215 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.2982 train_acc: 0.9995 val_loss: 0.4529 val_acc: 0.9224 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9162\n",
      "10000: 0.9162222222222223\n",
      " test_acc: 0.9162\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.9028 train_acc: 0.6877 val_loss: 0.7533 val_acc: 0.7294 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6484 train_acc: 0.8011 val_loss: 0.7445 val_acc: 0.7337 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6035 train_acc: 0.8293 val_loss: 0.9711 val_acc: 0.5787 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5828 train_acc: 0.8391 val_loss: 0.8643 val_acc: 0.6698 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5568 train_acc: 0.8576 val_loss: 0.6278 val_acc: 0.8150 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5429 train_acc: 0.8643 val_loss: 0.9101 val_acc: 0.6655 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5305 train_acc: 0.8703 val_loss: 0.5443 val_acc: 0.8717 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.5133 train_acc: 0.8811 val_loss: 0.6213 val_acc: 0.8251 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.5104 train_acc: 0.8838 val_loss: 0.6400 val_acc: 0.7880 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.5033 train_acc: 0.8865 val_loss: 0.5595 val_acc: 0.8548 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4876 train_acc: 0.8962 val_loss: 0.5973 val_acc: 0.8289 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4684 train_acc: 0.9073 val_loss: 0.5520 val_acc: 0.8544 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4603 train_acc: 0.9114 val_loss: 0.6812 val_acc: 0.7499 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4466 train_acc: 0.9180 val_loss: 0.5298 val_acc: 0.8632 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4359 train_acc: 0.9243 val_loss: 0.7423 val_acc: 0.7426 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.4140 train_acc: 0.9382 val_loss: 0.5307 val_acc: 0.8704 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.4045 train_acc: 0.9436 val_loss: 0.4876 val_acc: 0.8947 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.3987 train_acc: 0.9487 val_loss: 0.4700 val_acc: 0.9059 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3965 train_acc: 0.9491 val_loss: 0.4674 val_acc: 0.9066 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3864 train_acc: 0.9571 val_loss: 0.4704 val_acc: 0.9090 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3813 train_acc: 0.9593 val_loss: 0.4786 val_acc: 0.9007 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3768 train_acc: 0.9611 val_loss: 0.4827 val_acc: 0.8973 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3713 train_acc: 0.9648 val_loss: 0.5451 val_acc: 0.8713 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3664 train_acc: 0.9677 val_loss: 0.5147 val_acc: 0.8876 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3577 train_acc: 0.9714 val_loss: 0.4708 val_acc: 0.9055 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.3475 train_acc: 0.9794 val_loss: 0.4752 val_acc: 0.9085 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3452 train_acc: 0.9801 val_loss: 0.4792 val_acc: 0.8998 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.3420 train_acc: 0.9816 val_loss: 0.4678 val_acc: 0.9109 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.3387 train_acc: 0.9823 val_loss: 0.4865 val_acc: 0.9010 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.3363 train_acc: 0.9851 val_loss: 0.4803 val_acc: 0.9055 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.3382 train_acc: 0.9835 val_loss: 0.4672 val_acc: 0.9151 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.3323 train_acc: 0.9873 val_loss: 0.4845 val_acc: 0.9064 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.3298 train_acc: 0.9873 val_loss: 0.4671 val_acc: 0.9165 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.3263 train_acc: 0.9906 val_loss: 0.5641 val_acc: 0.8665 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.3264 train_acc: 0.9892 val_loss: 0.4776 val_acc: 0.9116 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.3237 train_acc: 0.9907 val_loss: 0.4868 val_acc: 0.9067 lr: 0.000625\n",
      "Epoch  36 / 200 train_loss: 0.3249 train_acc: 0.9903 val_loss: 0.4903 val_acc: 0.9052 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.3183 train_acc: 0.9941 val_loss: 0.4695 val_acc: 0.9197 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.3160 train_acc: 0.9943 val_loss: 0.4636 val_acc: 0.9203 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.3161 train_acc: 0.9949 val_loss: 0.4612 val_acc: 0.9154 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.3164 train_acc: 0.9948 val_loss: 0.4638 val_acc: 0.9163 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.3141 train_acc: 0.9954 val_loss: 0.4704 val_acc: 0.9116 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.3121 train_acc: 0.9955 val_loss: 0.4633 val_acc: 0.9165 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.3104 train_acc: 0.9976 val_loss: 0.4664 val_acc: 0.9163 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.3100 train_acc: 0.9976 val_loss: 0.4647 val_acc: 0.9140 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.3097 train_acc: 0.9976 val_loss: 0.4697 val_acc: 0.9098 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.3097 train_acc: 0.9971 val_loss: 0.4666 val_acc: 0.9137 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.3077 train_acc: 0.9980 val_loss: 0.4605 val_acc: 0.9175 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.3088 train_acc: 0.9980 val_loss: 0.4599 val_acc: 0.9177 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.3078 train_acc: 0.9981 val_loss: 0.4656 val_acc: 0.9133 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.3078 train_acc: 0.9983 val_loss: 0.4608 val_acc: 0.9163 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3077 train_acc: 0.9977 val_loss: 0.4599 val_acc: 0.9163 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3082 train_acc: 0.9970 val_loss: 0.4631 val_acc: 0.9149 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3067 train_acc: 0.9987 val_loss: 0.4677 val_acc: 0.9156 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3069 train_acc: 0.9977 val_loss: 0.4649 val_acc: 0.9130 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3063 train_acc: 0.9989 val_loss: 0.4658 val_acc: 0.9114 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3064 train_acc: 0.9985 val_loss: 0.4621 val_acc: 0.9189 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3063 train_acc: 0.9984 val_loss: 0.4642 val_acc: 0.9156 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3104 train_acc: 0.9971 val_loss: 0.4652 val_acc: 0.9196 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3063 train_acc: 0.9987 val_loss: 0.4620 val_acc: 0.9168 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3061 train_acc: 0.9985 val_loss: 0.4719 val_acc: 0.9138 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.3059 train_acc: 0.9986 val_loss: 0.4599 val_acc: 0.9170 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.3059 train_acc: 0.9989 val_loss: 0.4587 val_acc: 0.9170 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.3069 train_acc: 0.9975 val_loss: 0.4626 val_acc: 0.9163 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.3086 train_acc: 0.9977 val_loss: 0.4638 val_acc: 0.9173 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.3053 train_acc: 0.9986 val_loss: 0.4640 val_acc: 0.9150 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.3068 train_acc: 0.9974 val_loss: 0.4619 val_acc: 0.9187 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.3054 train_acc: 0.9986 val_loss: 0.4584 val_acc: 0.9160 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.3046 train_acc: 0.9989 val_loss: 0.4635 val_acc: 0.9149 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9017\n",
      "5000: 0.9017222222222222\n",
      " test_acc: 0.9017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sample_size</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.933056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.928722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.924278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.916222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.901722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_sample_size       acc\n",
       "0              25000  0.933056\n",
       "1              20000  0.928722\n",
       "2              15000  0.924278\n",
       "3              10000  0.916222\n",
       "4               5000  0.901722"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = []\n",
    "train_sample_sizes = [25000, 20000, 15000, 10000, 5000]\n",
    "for train_sample_size in train_sample_sizes:\n",
    "    acc = train_pipeline(train_objs, test_objs, labels, train_name=f'clean_data_large_group_ds_{train_sample_size}', train_sample_size=train_sample_size)\n",
    "    acc_df.append([train_sample_size, acc])\n",
    "acc_df =pd.DataFrame(acc_df)\n",
    "acc_df.columns = ['train_sample_size', 'acc']\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6435a4-c78f-44d7-87a5-ca4cf3862c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
