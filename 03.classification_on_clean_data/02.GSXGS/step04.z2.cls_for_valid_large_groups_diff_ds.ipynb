{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/user/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea64e7f-45b8-46db-b049-23707b3a8ea7",
   "metadata": {},
   "source": [
    "# 全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1K': 0,\n",
       "  '1R': 0,\n",
       "  '1D': 1,\n",
       "  '1E': 1,\n",
       "  '1F': 2,\n",
       "  '1W': 2,\n",
       "  '1Y': 2,\n",
       "  '1I': 2,\n",
       "  '1L': 2,\n",
       "  '1M': 2,\n",
       "  '1V': 2,\n",
       "  '1H': 2,\n",
       "  '1Q': 2,\n",
       "  '1A': 2,\n",
       "  '1G': 2,\n",
       "  '1S': 2,\n",
       "  '1C': 2,\n",
       "  '1P': 2,\n",
       "  '1T': 2,\n",
       "  '1N': 2},\n",
       " {0: 'positive', 1: 'negative', 2: 'neutral'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "\n",
    "all_peps = [['1K', '1R'], ['1D', '1E'], ['1F', '1W', '1Y', '1I', '1L', '1M', '1V', '1H', '1Q', '1A', '1G', '1S', '1C', '1P', '1T', '1N']]\n",
    "\n",
    "y_code_dict = nps.ml.set_y_codes_for_classes(all_peps)\n",
    "y_code_dict\n",
    "y_to_label_dict = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "y_code_dict, y_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5052bfdf-370e-4fcd-8e70-f4dd3f41f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in all_peps:\n",
    "    tmp.extend(i)\n",
    "all_peps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591e8a7d-e387-4f28-b79d-9e7a3c7beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs = [f\"../../../00.data/GSXGS/{pep}_valid80.pkl\" for pep in all_peps]\n",
    "test_objs = [f\"../../../00.data/GSXGS/{pep}_valid20.pkl\" for pep in all_peps]\n",
    "labels = all_peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a4b762-6fee-4531-b733-838c7f758d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4376c13c-8a5e-4ccf-89c8-cf82c528cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train_objs, test_objs, labels, train_name='clean_data', train_sample_size=14000):\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=42)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=42, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=6000, random_state=42)\n",
    "\n",
    "    batch_size = 64\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    nps.ml.seed_everything(42)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(y_to_label_dict), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=y_to_label_dict.values())\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    cm_df.to_csv(f\"../../../04.tables/classification/GSXGS/valid/{train_name}_cm.csv\")\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../04.tables/classification/GSXGS/valid/{train_name}_pred_proba.csv\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae31691-842e-41db-93ae-63cfb4241d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7464 train_acc: 0.7495 val_loss: 0.8834 val_acc: 0.6656 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5984 train_acc: 0.8276 val_loss: 0.9231 val_acc: 0.5738 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5604 train_acc: 0.8489 val_loss: 0.5408 val_acc: 0.8563 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5420 train_acc: 0.8600 val_loss: 0.5933 val_acc: 0.8300 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5204 train_acc: 0.8721 val_loss: 0.7176 val_acc: 0.7425 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5102 train_acc: 0.8787 val_loss: 0.7462 val_acc: 0.7301 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5000 train_acc: 0.8852 val_loss: 0.6668 val_acc: 0.7781 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.4690 train_acc: 0.9009 val_loss: 0.5791 val_acc: 0.8351 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.4588 train_acc: 0.9065 val_loss: 0.5807 val_acc: 0.8376 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.4515 train_acc: 0.9102 val_loss: 0.4959 val_acc: 0.8842 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.4443 train_acc: 0.9148 val_loss: 0.4680 val_acc: 0.8993 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4363 train_acc: 0.9188 val_loss: 0.5465 val_acc: 0.8474 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4300 train_acc: 0.9227 val_loss: 0.4769 val_acc: 0.8969 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4234 train_acc: 0.9263 val_loss: 0.4944 val_acc: 0.8886 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4189 train_acc: 0.9296 val_loss: 0.6949 val_acc: 0.7802 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.3963 train_acc: 0.9426 val_loss: 0.5326 val_acc: 0.8644 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.3883 train_acc: 0.9478 val_loss: 0.4838 val_acc: 0.8917 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.3830 train_acc: 0.9501 val_loss: 0.4714 val_acc: 0.9030 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3793 train_acc: 0.9521 val_loss: 0.4759 val_acc: 0.9015 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3735 train_acc: 0.9559 val_loss: 0.4733 val_acc: 0.9055 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3693 train_acc: 0.9580 val_loss: 0.4705 val_acc: 0.9047 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3648 train_acc: 0.9617 val_loss: 0.4749 val_acc: 0.9038 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3610 train_acc: 0.9629 val_loss: 0.5400 val_acc: 0.8698 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3577 train_acc: 0.9659 val_loss: 0.5357 val_acc: 0.8765 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3456 train_acc: 0.9726 val_loss: 0.4794 val_acc: 0.9004 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.3407 train_acc: 0.9759 val_loss: 0.4814 val_acc: 0.9040 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3378 train_acc: 0.9777 val_loss: 0.4835 val_acc: 0.9017 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.3359 train_acc: 0.9788 val_loss: 0.4898 val_acc: 0.9001 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.3292 train_acc: 0.9826 val_loss: 0.4876 val_acc: 0.9018 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.3272 train_acc: 0.9841 val_loss: 0.4801 val_acc: 0.9064 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.3257 train_acc: 0.9844 val_loss: 0.4877 val_acc: 0.9034 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.3246 train_acc: 0.9857 val_loss: 0.4829 val_acc: 0.9038 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.3224 train_acc: 0.9869 val_loss: 0.4887 val_acc: 0.9032 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.3214 train_acc: 0.9872 val_loss: 0.4898 val_acc: 0.9025 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3189 train_acc: 0.9888 val_loss: 0.4888 val_acc: 0.9055 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.3175 train_acc: 0.9898 val_loss: 0.4874 val_acc: 0.9040 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.3168 train_acc: 0.9897 val_loss: 0.4900 val_acc: 0.9043 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.3168 train_acc: 0.9899 val_loss: 0.4891 val_acc: 0.9048 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3152 train_acc: 0.9908 val_loss: 0.4907 val_acc: 0.9032 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.3142 train_acc: 0.9915 val_loss: 0.4916 val_acc: 0.9029 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.3144 train_acc: 0.9913 val_loss: 0.4919 val_acc: 0.9041 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.3138 train_acc: 0.9918 val_loss: 0.4934 val_acc: 0.9046 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3132 train_acc: 0.9924 val_loss: 0.4933 val_acc: 0.9016 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3131 train_acc: 0.9920 val_loss: 0.4920 val_acc: 0.9039 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3131 train_acc: 0.9924 val_loss: 0.4927 val_acc: 0.9039 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3127 train_acc: 0.9922 val_loss: 0.4932 val_acc: 0.9025 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3126 train_acc: 0.9924 val_loss: 0.4928 val_acc: 0.9044 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3123 train_acc: 0.9923 val_loss: 0.4941 val_acc: 0.9026 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3115 train_acc: 0.9929 val_loss: 0.4933 val_acc: 0.9031 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3115 train_acc: 0.9930 val_loss: 0.4935 val_acc: 0.9026 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3113 train_acc: 0.9931 val_loss: 0.4914 val_acc: 0.9037 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3109 train_acc: 0.9935 val_loss: 0.4947 val_acc: 0.9023 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3111 train_acc: 0.9936 val_loss: 0.4928 val_acc: 0.9027 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3106 train_acc: 0.9932 val_loss: 0.4942 val_acc: 0.9052 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3105 train_acc: 0.9938 val_loss: 0.4921 val_acc: 0.9035 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3103 train_acc: 0.9938 val_loss: 0.4925 val_acc: 0.9027 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3106 train_acc: 0.9934 val_loss: 0.4947 val_acc: 0.9040 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3107 train_acc: 0.9934 val_loss: 0.4954 val_acc: 0.9045 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3100 train_acc: 0.9942 val_loss: 0.4938 val_acc: 0.9058 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3101 train_acc: 0.9938 val_loss: 0.4956 val_acc: 0.9033 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9078\n",
      "25000: 0.9078333333333334\n",
      " test_acc: 0.9078\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.7608 train_acc: 0.7399 val_loss: 0.7302 val_acc: 0.7383 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.5979 train_acc: 0.8264 val_loss: 0.6426 val_acc: 0.7991 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.5629 train_acc: 0.8481 val_loss: 0.6928 val_acc: 0.7823 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5379 train_acc: 0.8623 val_loss: 0.6851 val_acc: 0.7666 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5206 train_acc: 0.8729 val_loss: 0.6558 val_acc: 0.7805 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5062 train_acc: 0.8801 val_loss: 0.5193 val_acc: 0.8744 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.4939 train_acc: 0.8874 val_loss: 0.6162 val_acc: 0.8096 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.4831 train_acc: 0.8937 val_loss: 0.5625 val_acc: 0.8496 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.4735 train_acc: 0.8985 val_loss: 0.5059 val_acc: 0.8798 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.4637 train_acc: 0.9031 val_loss: 0.6407 val_acc: 0.7993 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.4535 train_acc: 0.9112 val_loss: 0.6060 val_acc: 0.8195 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.4443 train_acc: 0.9155 val_loss: 0.4759 val_acc: 0.8959 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.4346 train_acc: 0.9215 val_loss: 0.6707 val_acc: 0.7953 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.4283 train_acc: 0.9259 val_loss: 0.8729 val_acc: 0.6635 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.4208 train_acc: 0.9293 val_loss: 0.6165 val_acc: 0.8188 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.4126 train_acc: 0.9350 val_loss: 0.4842 val_acc: 0.8917 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.3852 train_acc: 0.9505 val_loss: 0.4671 val_acc: 0.9026 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.3738 train_acc: 0.9584 val_loss: 0.4989 val_acc: 0.8850 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.3661 train_acc: 0.9615 val_loss: 0.5051 val_acc: 0.8844 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.3625 train_acc: 0.9646 val_loss: 0.4821 val_acc: 0.8972 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.3556 train_acc: 0.9691 val_loss: 0.5232 val_acc: 0.8779 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.3432 train_acc: 0.9765 val_loss: 0.4720 val_acc: 0.9040 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.3378 train_acc: 0.9794 val_loss: 0.4903 val_acc: 0.8929 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.3334 train_acc: 0.9824 val_loss: 0.4751 val_acc: 0.9022 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.3308 train_acc: 0.9838 val_loss: 0.4874 val_acc: 0.8950 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.3273 train_acc: 0.9856 val_loss: 0.4894 val_acc: 0.8959 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3216 train_acc: 0.9893 val_loss: 0.4725 val_acc: 0.9061 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.3186 train_acc: 0.9907 val_loss: 0.4794 val_acc: 0.9058 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.3171 train_acc: 0.9912 val_loss: 0.4884 val_acc: 0.8965 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.3159 train_acc: 0.9921 val_loss: 0.4807 val_acc: 0.9034 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.3143 train_acc: 0.9928 val_loss: 0.4832 val_acc: 0.9021 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.3116 train_acc: 0.9950 val_loss: 0.4770 val_acc: 0.9030 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.3107 train_acc: 0.9952 val_loss: 0.4786 val_acc: 0.9037 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.3106 train_acc: 0.9945 val_loss: 0.4804 val_acc: 0.9032 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.3096 train_acc: 0.9953 val_loss: 0.4771 val_acc: 0.9054 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.3086 train_acc: 0.9958 val_loss: 0.4787 val_acc: 0.9058 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.3083 train_acc: 0.9960 val_loss: 0.4794 val_acc: 0.9033 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.3076 train_acc: 0.9963 val_loss: 0.4787 val_acc: 0.9050 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.3067 train_acc: 0.9971 val_loss: 0.4785 val_acc: 0.9048 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.3065 train_acc: 0.9969 val_loss: 0.4777 val_acc: 0.9040 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.3068 train_acc: 0.9964 val_loss: 0.4791 val_acc: 0.9021 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.3063 train_acc: 0.9967 val_loss: 0.4789 val_acc: 0.9043 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.3061 train_acc: 0.9971 val_loss: 0.4771 val_acc: 0.9047 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3058 train_acc: 0.9971 val_loss: 0.4780 val_acc: 0.9055 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3058 train_acc: 0.9972 val_loss: 0.4789 val_acc: 0.9038 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3053 train_acc: 0.9975 val_loss: 0.4783 val_acc: 0.9032 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3054 train_acc: 0.9971 val_loss: 0.4781 val_acc: 0.9040 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3051 train_acc: 0.9976 val_loss: 0.4761 val_acc: 0.9044 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3052 train_acc: 0.9973 val_loss: 0.4774 val_acc: 0.9032 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3048 train_acc: 0.9974 val_loss: 0.4763 val_acc: 0.9049 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3050 train_acc: 0.9974 val_loss: 0.4778 val_acc: 0.9034 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3050 train_acc: 0.9974 val_loss: 0.4781 val_acc: 0.9036 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3052 train_acc: 0.9972 val_loss: 0.4759 val_acc: 0.9052 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3050 train_acc: 0.9972 val_loss: 0.4791 val_acc: 0.9039 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3048 train_acc: 0.9978 val_loss: 0.4772 val_acc: 0.9053 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3049 train_acc: 0.9971 val_loss: 0.4801 val_acc: 0.9027 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3043 train_acc: 0.9978 val_loss: 0.4777 val_acc: 0.9038 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9067\n",
      "20000: 0.9066666666666666\n",
      " test_acc: 0.9067\n",
      "Epoch  17 / 200 train_loss: 0.3784 train_acc: 0.9550 val_loss: 0.5021 val_acc: 0.8842 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.3710 train_acc: 0.9595 val_loss: 0.5480 val_acc: 0.8654 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.3657 train_acc: 0.9635 val_loss: 0.4999 val_acc: 0.8920 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.3617 train_acc: 0.9650 val_loss: 0.5439 val_acc: 0.8656 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3499 train_acc: 0.9720 val_loss: 0.4910 val_acc: 0.8932 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3437 train_acc: 0.9760 val_loss: 0.4912 val_acc: 0.8946 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.3411 train_acc: 0.9777 val_loss: 0.5057 val_acc: 0.8917 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3372 train_acc: 0.9797 val_loss: 0.5072 val_acc: 0.8867 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.3338 train_acc: 0.9818 val_loss: 0.5087 val_acc: 0.8903 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3317 train_acc: 0.9830 val_loss: 0.5049 val_acc: 0.8925 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.3257 train_acc: 0.9865 val_loss: 0.4994 val_acc: 0.8982 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.3240 train_acc: 0.9878 val_loss: 0.4989 val_acc: 0.8962 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.3223 train_acc: 0.9887 val_loss: 0.5010 val_acc: 0.8980 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.3210 train_acc: 0.9894 val_loss: 0.5052 val_acc: 0.8940 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.3195 train_acc: 0.9904 val_loss: 0.4981 val_acc: 0.8952 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.3174 train_acc: 0.9918 val_loss: 0.5015 val_acc: 0.8963 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3157 train_acc: 0.9924 val_loss: 0.5029 val_acc: 0.8961 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3153 train_acc: 0.9922 val_loss: 0.5012 val_acc: 0.8949 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.3144 train_acc: 0.9928 val_loss: 0.5076 val_acc: 0.8954 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.3137 train_acc: 0.9938 val_loss: 0.5046 val_acc: 0.8965 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.3131 train_acc: 0.9937 val_loss: 0.5033 val_acc: 0.8934 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3127 train_acc: 0.9938 val_loss: 0.5041 val_acc: 0.8927 lr: 7.8125e-05\n",
      "Epoch  39 / 200 train_loss: 0.3129 train_acc: 0.9942 val_loss: 0.5052 val_acc: 0.8939 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3118 train_acc: 0.9944 val_loss: 0.5042 val_acc: 0.8951 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3121 train_acc: 0.9941 val_loss: 0.5044 val_acc: 0.8954 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3120 train_acc: 0.9946 val_loss: 0.5050 val_acc: 0.8953 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3119 train_acc: 0.9942 val_loss: 0.5043 val_acc: 0.8958 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3111 train_acc: 0.9946 val_loss: 0.5055 val_acc: 0.8932 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3107 train_acc: 0.9949 val_loss: 0.5068 val_acc: 0.8950 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3109 train_acc: 0.9946 val_loss: 0.5041 val_acc: 0.8952 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3106 train_acc: 0.9953 val_loss: 0.5053 val_acc: 0.8948 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3104 train_acc: 0.9951 val_loss: 0.5056 val_acc: 0.8942 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3101 train_acc: 0.9951 val_loss: 0.5059 val_acc: 0.8950 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3105 train_acc: 0.9950 val_loss: 0.5055 val_acc: 0.8953 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3097 train_acc: 0.9957 val_loss: 0.5051 val_acc: 0.8964 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3097 train_acc: 0.9955 val_loss: 0.5048 val_acc: 0.8937 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3098 train_acc: 0.9953 val_loss: 0.5073 val_acc: 0.8944 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3095 train_acc: 0.9956 val_loss: 0.5058 val_acc: 0.8945 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3095 train_acc: 0.9953 val_loss: 0.5034 val_acc: 0.8961 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3095 train_acc: 0.9952 val_loss: 0.5055 val_acc: 0.8939 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3091 train_acc: 0.9958 val_loss: 0.5071 val_acc: 0.8950 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8968\n",
      "15000: 0.8967777777777778\n",
      " test_acc: 0.8968\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.8707 train_acc: 0.6896 val_loss: 0.7695 val_acc: 0.7283 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.6539 train_acc: 0.7926 val_loss: 0.8392 val_acc: 0.6763 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6170 train_acc: 0.8150 val_loss: 0.6040 val_acc: 0.8181 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.5946 train_acc: 0.8268 val_loss: 0.6333 val_acc: 0.8138 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.5710 train_acc: 0.8422 val_loss: 0.6453 val_acc: 0.8005 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.5614 train_acc: 0.8478 val_loss: 0.7540 val_acc: 0.7099 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5484 train_acc: 0.8581 val_loss: 0.6595 val_acc: 0.8072 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.5141 train_acc: 0.8763 val_loss: 0.5187 val_acc: 0.8745 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.5049 train_acc: 0.8803 val_loss: 0.5386 val_acc: 0.8649 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.4961 train_acc: 0.8843 val_loss: 0.5511 val_acc: 0.8585 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.4899 train_acc: 0.8893 val_loss: 0.5136 val_acc: 0.8745 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.4853 train_acc: 0.8929 val_loss: 0.7679 val_acc: 0.7344 lr: 0.00125\n",
      "Epoch  12 / 200 train_loss: 0.4621 train_acc: 0.9063 val_loss: 0.5157 val_acc: 0.8764 lr: 0.00125\n",
      "Epoch  13 / 200 train_loss: 0.4502 train_acc: 0.9143 val_loss: 0.4992 val_acc: 0.8816 lr: 0.00125\n",
      "Epoch  14 / 200 train_loss: 0.4438 train_acc: 0.9179 val_loss: 0.5315 val_acc: 0.8670 lr: 0.00125\n",
      "Epoch  15 / 200 train_loss: 0.4342 train_acc: 0.9237 val_loss: 0.5113 val_acc: 0.8758 lr: 0.00125\n",
      "Epoch  16 / 200 train_loss: 0.4258 train_acc: 0.9297 val_loss: 0.5246 val_acc: 0.8729 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.4190 train_acc: 0.9324 val_loss: 0.5598 val_acc: 0.8519 lr: 0.000625\n",
      "Epoch  18 / 200 train_loss: 0.4043 train_acc: 0.9422 val_loss: 0.5065 val_acc: 0.8862 lr: 0.000625\n",
      "Epoch  19 / 200 train_loss: 0.3939 train_acc: 0.9468 val_loss: 0.5316 val_acc: 0.8669 lr: 0.000625\n",
      "Epoch  20 / 200 train_loss: 0.3904 train_acc: 0.9506 val_loss: 0.5207 val_acc: 0.8781 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3881 train_acc: 0.9509 val_loss: 0.5245 val_acc: 0.8737 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3839 train_acc: 0.9536 val_loss: 0.5235 val_acc: 0.8790 lr: 0.0003125\n",
      "Epoch  23 / 200 train_loss: 0.3725 train_acc: 0.9600 val_loss: 0.5125 val_acc: 0.8849 lr: 0.0003125\n",
      "Epoch  24 / 200 train_loss: 0.3679 train_acc: 0.9629 val_loss: 0.5410 val_acc: 0.8737 lr: 0.0003125\n",
      "Epoch  25 / 200 train_loss: 0.3661 train_acc: 0.9650 val_loss: 0.5164 val_acc: 0.8842 lr: 0.0003125\n",
      "Epoch  26 / 200 train_loss: 0.3602 train_acc: 0.9680 val_loss: 0.5183 val_acc: 0.8841 lr: 0.00015625\n",
      "Epoch  27 / 200 train_loss: 0.3571 train_acc: 0.9700 val_loss: 0.5276 val_acc: 0.8786 lr: 0.00015625\n",
      "Epoch  28 / 200 train_loss: 0.3551 train_acc: 0.9708 val_loss: 0.5217 val_acc: 0.8826 lr: 0.00015625\n",
      "Epoch  29 / 200 train_loss: 0.3533 train_acc: 0.9718 val_loss: 0.5222 val_acc: 0.8828 lr: 0.00015625\n",
      "Epoch  30 / 200 train_loss: 0.3516 train_acc: 0.9726 val_loss: 0.5205 val_acc: 0.8871 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.3490 train_acc: 0.9747 val_loss: 0.5238 val_acc: 0.8809 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.3478 train_acc: 0.9747 val_loss: 0.5232 val_acc: 0.8829 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3451 train_acc: 0.9770 val_loss: 0.5315 val_acc: 0.8765 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3445 train_acc: 0.9768 val_loss: 0.5254 val_acc: 0.8790 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.3413 train_acc: 0.9793 val_loss: 0.5286 val_acc: 0.8813 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.3395 train_acc: 0.9803 val_loss: 0.5267 val_acc: 0.8807 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.3381 train_acc: 0.9808 val_loss: 0.5267 val_acc: 0.8799 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3391 train_acc: 0.9801 val_loss: 0.5276 val_acc: 0.8793 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.3369 train_acc: 0.9815 val_loss: 0.5299 val_acc: 0.8803 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3362 train_acc: 0.9821 val_loss: 0.5310 val_acc: 0.8797 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3357 train_acc: 0.9822 val_loss: 0.5307 val_acc: 0.8828 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3352 train_acc: 0.9824 val_loss: 0.5338 val_acc: 0.8785 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3341 train_acc: 0.9831 val_loss: 0.5307 val_acc: 0.8804 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3335 train_acc: 0.9834 val_loss: 0.5310 val_acc: 0.8779 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3337 train_acc: 0.9835 val_loss: 0.5302 val_acc: 0.8787 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3322 train_acc: 0.9845 val_loss: 0.5322 val_acc: 0.8784 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3319 train_acc: 0.9839 val_loss: 0.5312 val_acc: 0.8802 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3321 train_acc: 0.9846 val_loss: 0.5286 val_acc: 0.8798 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3317 train_acc: 0.9844 val_loss: 0.5310 val_acc: 0.8785 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3308 train_acc: 0.9850 val_loss: 0.5306 val_acc: 0.8767 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3291 train_acc: 0.9860 val_loss: 0.5321 val_acc: 0.8818 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3303 train_acc: 0.9853 val_loss: 0.5354 val_acc: 0.8795 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.3311 train_acc: 0.9853 val_loss: 0.5337 val_acc: 0.8784 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.3282 train_acc: 0.9867 val_loss: 0.5370 val_acc: 0.8780 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.3277 train_acc: 0.9869 val_loss: 0.5359 val_acc: 0.8771 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.3273 train_acc: 0.9879 val_loss: 0.5359 val_acc: 0.8793 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.3265 train_acc: 0.9878 val_loss: 0.5355 val_acc: 0.8780 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.3264 train_acc: 0.9874 val_loss: 0.5353 val_acc: 0.8784 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.3261 train_acc: 0.9877 val_loss: 0.5330 val_acc: 0.8814 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.3250 train_acc: 0.9889 val_loss: 0.5334 val_acc: 0.8815 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8849\n",
      "10000: 0.8849444444444444\n",
      " test_acc: 0.8849\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 0.9641 train_acc: 0.6275 val_loss: 1.0578 val_acc: 0.4566 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.7102 train_acc: 0.7552 val_loss: 1.0100 val_acc: 0.5966 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.6641 train_acc: 0.7853 val_loss: 0.7323 val_acc: 0.7438 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.6426 train_acc: 0.7983 val_loss: 0.8889 val_acc: 0.6367 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.6211 train_acc: 0.8118 val_loss: 1.0616 val_acc: 0.5437 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.6024 train_acc: 0.8250 val_loss: 0.7352 val_acc: 0.7350 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.5905 train_acc: 0.8290 val_loss: 0.7951 val_acc: 0.7070 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.5545 train_acc: 0.8514 val_loss: 0.5966 val_acc: 0.8225 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.5349 train_acc: 0.8640 val_loss: 0.5648 val_acc: 0.8463 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.5276 train_acc: 0.8714 val_loss: 0.5674 val_acc: 0.8482 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.5176 train_acc: 0.8755 val_loss: 0.6087 val_acc: 0.8126 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.5087 train_acc: 0.8794 val_loss: 0.5546 val_acc: 0.8501 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.4982 train_acc: 0.8841 val_loss: 0.5190 val_acc: 0.8755 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.4952 train_acc: 0.8855 val_loss: 0.6953 val_acc: 0.7659 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.4904 train_acc: 0.8941 val_loss: 0.5666 val_acc: 0.8388 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.4828 train_acc: 0.8959 val_loss: 0.5381 val_acc: 0.8608 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.4644 train_acc: 0.9071 val_loss: 0.9368 val_acc: 0.6547 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.4462 train_acc: 0.9180 val_loss: 0.5237 val_acc: 0.8722 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.4230 train_acc: 0.9341 val_loss: 0.5155 val_acc: 0.8715 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.4172 train_acc: 0.9392 val_loss: 0.5245 val_acc: 0.8694 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.4076 train_acc: 0.9434 val_loss: 0.5332 val_acc: 0.8694 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.3964 train_acc: 0.9534 val_loss: 0.5371 val_acc: 0.8652 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.3878 train_acc: 0.9563 val_loss: 0.5135 val_acc: 0.8847 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.3798 train_acc: 0.9632 val_loss: 0.5083 val_acc: 0.8767 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.3769 train_acc: 0.9635 val_loss: 0.5230 val_acc: 0.8758 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.3753 train_acc: 0.9641 val_loss: 0.5160 val_acc: 0.8770 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.3695 train_acc: 0.9687 val_loss: 0.5500 val_acc: 0.8524 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.3612 train_acc: 0.9747 val_loss: 0.5160 val_acc: 0.8800 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.3572 train_acc: 0.9754 val_loss: 0.5218 val_acc: 0.8765 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.3515 train_acc: 0.9797 val_loss: 0.5222 val_acc: 0.8790 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.3508 train_acc: 0.9790 val_loss: 0.5176 val_acc: 0.8706 lr: 0.00015625\n",
      "Epoch  31 / 200 train_loss: 0.3471 train_acc: 0.9817 val_loss: 0.5163 val_acc: 0.8791 lr: 0.00015625\n",
      "Epoch  32 / 200 train_loss: 0.3457 train_acc: 0.9829 val_loss: 0.5175 val_acc: 0.8816 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.3438 train_acc: 0.9838 val_loss: 0.5144 val_acc: 0.8772 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.3432 train_acc: 0.9843 val_loss: 0.5208 val_acc: 0.8764 lr: 7.8125e-05\n",
      "Epoch  35 / 200 train_loss: 0.3439 train_acc: 0.9841 val_loss: 0.5122 val_acc: 0.8800 lr: 7.8125e-05\n",
      "Epoch  36 / 200 train_loss: 0.3403 train_acc: 0.9850 val_loss: 0.5187 val_acc: 0.8803 lr: 7.8125e-05\n",
      "Epoch  37 / 200 train_loss: 0.3390 train_acc: 0.9860 val_loss: 0.5190 val_acc: 0.8760 lr: 7.8125e-05\n",
      "Epoch  38 / 200 train_loss: 0.3383 train_acc: 0.9854 val_loss: 0.5169 val_acc: 0.8760 lr: 5e-05\n",
      "Epoch  39 / 200 train_loss: 0.3386 train_acc: 0.9858 val_loss: 0.5244 val_acc: 0.8793 lr: 5e-05\n",
      "Epoch  40 / 200 train_loss: 0.3373 train_acc: 0.9873 val_loss: 0.5181 val_acc: 0.8758 lr: 5e-05\n",
      "Epoch  41 / 200 train_loss: 0.3387 train_acc: 0.9852 val_loss: 0.5191 val_acc: 0.8772 lr: 5e-05\n",
      "Epoch  42 / 200 train_loss: 0.3369 train_acc: 0.9876 val_loss: 0.5180 val_acc: 0.8788 lr: 5e-05\n",
      "Epoch  43 / 200 train_loss: 0.3362 train_acc: 0.9873 val_loss: 0.5336 val_acc: 0.8669 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.3356 train_acc: 0.9866 val_loss: 0.5358 val_acc: 0.8721 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.3344 train_acc: 0.9894 val_loss: 0.5248 val_acc: 0.8730 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.3352 train_acc: 0.9882 val_loss: 0.5185 val_acc: 0.8791 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.3354 train_acc: 0.9872 val_loss: 0.5220 val_acc: 0.8765 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.3340 train_acc: 0.9885 val_loss: 0.5235 val_acc: 0.8727 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.3351 train_acc: 0.9882 val_loss: 0.5210 val_acc: 0.8772 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.3329 train_acc: 0.9886 val_loss: 0.5274 val_acc: 0.8749 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.3335 train_acc: 0.9885 val_loss: 0.5231 val_acc: 0.8768 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.3329 train_acc: 0.9895 val_loss: 0.5245 val_acc: 0.8768 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8757\n",
      "5000: 0.8756666666666667\n",
      " test_acc: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sample_size</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.907833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.896778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.884944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.875667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_sample_size       acc\n",
       "0              25000  0.907833\n",
       "1              20000  0.906667\n",
       "2              15000  0.896778\n",
       "3              10000  0.884944\n",
       "4               5000  0.875667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = []\n",
    "train_sample_sizes = [25000, 20000, 15000, 10000, 5000]\n",
    "for train_sample_size in train_sample_sizes:\n",
    "    acc = train_pipeline(train_objs, test_objs, labels, train_name=f'valid_data_large_group_ds_{train_sample_size}', train_sample_size=train_sample_size)\n",
    "    acc_df.append([train_sample_size, acc])\n",
    "acc_df =pd.DataFrame(acc_df)\n",
    "acc_df.columns = ['train_sample_size', 'acc']\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6435a4-c78f-44d7-87a5-ca4cf3862c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
