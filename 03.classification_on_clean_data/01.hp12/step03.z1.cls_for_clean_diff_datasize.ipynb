{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/user/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea64e7f-45b8-46db-b049-23707b3a8ea7",
   "metadata": {},
   "source": [
    "# 全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({np.str_('hp1_1'): 0,\n",
       "  np.str_('hp1_2'): 1,\n",
       "  np.str_('hp1_3'): 2,\n",
       "  np.str_('hp1_4'): 3,\n",
       "  np.str_('hp1_5'): 4,\n",
       "  np.str_('hp1_6'): 5,\n",
       "  np.str_('hp1_7'): 6,\n",
       "  np.str_('hp1_8'): 7,\n",
       "  np.str_('hp1_9'): 8,\n",
       "  np.str_('hp2_1'): 9,\n",
       "  np.str_('hp2_2'): 10,\n",
       "  np.str_('hp2_3'): 11,\n",
       "  np.str_('hp2_4'): 12,\n",
       "  np.str_('hp2_5'): 13,\n",
       "  np.str_('hp2_6'): 14},\n",
       " {0: np.str_('hp1_1'),\n",
       "  1: np.str_('hp1_2'),\n",
       "  2: np.str_('hp1_3'),\n",
       "  3: np.str_('hp1_4'),\n",
       "  4: np.str_('hp1_5'),\n",
       "  5: np.str_('hp1_6'),\n",
       "  6: np.str_('hp1_7'),\n",
       "  7: np.str_('hp1_8'),\n",
       "  8: np.str_('hp1_9'),\n",
       "  9: np.str_('hp2_1'),\n",
       "  10: np.str_('hp2_2'),\n",
       "  11: np.str_('hp2_3'),\n",
       "  12: np.str_('hp2_4'),\n",
       "  13: np.str_('hp2_5'),\n",
       "  14: np.str_('hp2_6')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "\n",
    "all_peps = [\n",
    "    'hp1_1', 'hp1_2', 'hp1_3', 'hp1_4', 'hp1_5', 'hp1_6', 'hp1_7', 'hp1_8', 'hp1_9',\n",
    "    'hp2_1', 'hp2_2', 'hp2_3', 'hp2_4', 'hp2_5', 'hp2_6',\n",
    "]\n",
    "\n",
    "y_code_dict = nps.ml.set_y_codes_for_classes(np.array(all_peps)[:,None])\n",
    "y_code_dict\n",
    "y_to_label_dict = {v:k for k,v in y_code_dict.items()}\n",
    "y_code_dict, y_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e8a7d-e387-4f28-b79d-9e7a3c7beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs = [f\"../../../03.results/classification_on_clean_data/hp12/{pep}/{pep}_valid80_clean_obj.pkl\" for pep in all_peps]\n",
    "test_objs = [f\"../../../03.results/classification_on_clean_data/hp12/{pep}/{pep}_valid20_clean_obj.pkl\" for pep in all_peps]\n",
    "labels = all_peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a4b762-6fee-4531-b733-838c7f758d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96a0de-ed6b-4c1f-8ccf-55c32b444a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ac4b4e-1899-48e3-ad61-8515569804ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name='clean_data', train_sample_size=14000, seed=42):\n",
    "    # 读取pkl文件，生成readid，X，y组成的df\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=seed)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=seed, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=6000, random_state=seed)\n",
    "\n",
    "    # 通过data_df构建dataloader\n",
    "    batch_size = 128\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    nps.ml.seed_everything(seed)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(all_peps), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    # pred\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    test_all_reads_s = pred_df['true'].value_counts()\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=all_peps)\n",
    "    cm_df.to_csv(f\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/clean/{train_name}_cm.csv\")\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/clean/{train_name}_pred_proba.csv\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f38c00c-f860-4afa-9b6e-26ad5e44803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.1765 train_acc: 0.7502 val_loss: 1.0903 val_acc: 0.7832 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.8916 train_acc: 0.8720 val_loss: 1.1603 val_acc: 0.7499 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8187 train_acc: 0.9010 val_loss: 1.5012 val_acc: 0.6511 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.7794 train_acc: 0.9160 val_loss: 0.7806 val_acc: 0.9125 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.7528 train_acc: 0.9261 val_loss: 2.1566 val_acc: 0.4659 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7315 train_acc: 0.9344 val_loss: 0.9016 val_acc: 0.8544 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7159 train_acc: 0.9405 val_loss: 1.1812 val_acc: 0.7516 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7029 train_acc: 0.9456 val_loss: 0.7719 val_acc: 0.9143 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.6910 train_acc: 0.9505 val_loss: 1.6486 val_acc: 0.5971 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.6815 train_acc: 0.9546 val_loss: 0.7536 val_acc: 0.9226 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.6727 train_acc: 0.9578 val_loss: 1.1231 val_acc: 0.7795 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.6653 train_acc: 0.9608 val_loss: 1.3015 val_acc: 0.6969 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.6588 train_acc: 0.9635 val_loss: 1.8754 val_acc: 0.5599 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.6525 train_acc: 0.9659 val_loss: 0.8514 val_acc: 0.8839 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6219 train_acc: 0.9787 val_loss: 1.1067 val_acc: 0.8026 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6143 train_acc: 0.9819 val_loss: 1.1037 val_acc: 0.8077 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6090 train_acc: 0.9843 val_loss: 1.1281 val_acc: 0.7915 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6043 train_acc: 0.9859 val_loss: 0.9109 val_acc: 0.8710 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.5919 train_acc: 0.9911 val_loss: 0.8700 val_acc: 0.8849 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.5884 train_acc: 0.9922 val_loss: 0.6944 val_acc: 0.9495 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.5859 train_acc: 0.9930 val_loss: 0.7817 val_acc: 0.9162 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.5838 train_acc: 0.9940 val_loss: 0.6903 val_acc: 0.9509 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.5821 train_acc: 0.9945 val_loss: 1.0901 val_acc: 0.8184 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.5803 train_acc: 0.9950 val_loss: 1.2002 val_acc: 0.7859 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.5790 train_acc: 0.9955 val_loss: 0.7031 val_acc: 0.9469 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.5781 train_acc: 0.9958 val_loss: 0.6931 val_acc: 0.9510 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.5738 train_acc: 0.9972 val_loss: 0.6892 val_acc: 0.9524 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.5722 train_acc: 0.9977 val_loss: 0.6965 val_acc: 0.9494 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.5714 train_acc: 0.9979 val_loss: 0.7000 val_acc: 0.9484 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.5709 train_acc: 0.9980 val_loss: 0.6937 val_acc: 0.9507 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5702 train_acc: 0.9981 val_loss: 0.6939 val_acc: 0.9510 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.5687 train_acc: 0.9986 val_loss: 0.6925 val_acc: 0.9512 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.5680 train_acc: 0.9987 val_loss: 0.6973 val_acc: 0.9500 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.5676 train_acc: 0.9988 val_loss: 0.6921 val_acc: 0.9521 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.5672 train_acc: 0.9989 val_loss: 0.7072 val_acc: 0.9470 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.5665 train_acc: 0.9990 val_loss: 0.6963 val_acc: 0.9504 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.5663 train_acc: 0.9990 val_loss: 0.6959 val_acc: 0.9507 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.5662 train_acc: 0.9991 val_loss: 0.6913 val_acc: 0.9528 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.5659 train_acc: 0.9992 val_loss: 0.6944 val_acc: 0.9513 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.5657 train_acc: 0.9992 val_loss: 0.6930 val_acc: 0.9521 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.5656 train_acc: 0.9992 val_loss: 0.7122 val_acc: 0.9448 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5654 train_acc: 0.9992 val_loss: 0.6940 val_acc: 0.9522 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.5652 train_acc: 0.9993 val_loss: 0.6922 val_acc: 0.9524 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.5651 train_acc: 0.9993 val_loss: 0.6936 val_acc: 0.9518 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.5649 train_acc: 0.9993 val_loss: 0.6944 val_acc: 0.9518 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.5648 train_acc: 0.9993 val_loss: 0.6939 val_acc: 0.9516 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.5647 train_acc: 0.9993 val_loss: 0.6932 val_acc: 0.9522 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.5646 train_acc: 0.9994 val_loss: 0.6929 val_acc: 0.9521 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.5647 train_acc: 0.9993 val_loss: 0.6931 val_acc: 0.9522 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.5646 train_acc: 0.9993 val_loss: 0.6929 val_acc: 0.9526 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.5646 train_acc: 0.9994 val_loss: 0.6932 val_acc: 0.9522 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5645 train_acc: 0.9994 val_loss: 0.6938 val_acc: 0.9518 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5645 train_acc: 0.9993 val_loss: 0.6940 val_acc: 0.9523 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5643 train_acc: 0.9995 val_loss: 0.6932 val_acc: 0.9519 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5644 train_acc: 0.9994 val_loss: 0.6952 val_acc: 0.9514 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5642 train_acc: 0.9994 val_loss: 0.6935 val_acc: 0.9518 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5643 train_acc: 0.9994 val_loss: 0.6940 val_acc: 0.9523 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5643 train_acc: 0.9994 val_loss: 0.6945 val_acc: 0.9515 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5641 train_acc: 0.9995 val_loss: 0.6935 val_acc: 0.9521 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5642 train_acc: 0.9994 val_loss: 0.6933 val_acc: 0.9519 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5640 train_acc: 0.9995 val_loss: 0.6939 val_acc: 0.9516 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5642 train_acc: 0.9994 val_loss: 0.6941 val_acc: 0.9524 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5641 train_acc: 0.9994 val_loss: 0.6934 val_acc: 0.9516 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5640 train_acc: 0.9994 val_loss: 0.6958 val_acc: 0.9519 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5639 train_acc: 0.9994 val_loss: 0.6941 val_acc: 0.9521 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5639 train_acc: 0.9995 val_loss: 0.6945 val_acc: 0.9517 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5639 train_acc: 0.9995 val_loss: 0.6942 val_acc: 0.9520 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5638 train_acc: 0.9994 val_loss: 0.6945 val_acc: 0.9518 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9523\n",
      "25000: 0.9523222222222222\n",
      " test_acc: 0.9523\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.3426 train_acc: 0.6773 val_loss: 1.0648 val_acc: 0.7955 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0217 train_acc: 0.8182 val_loss: 1.7864 val_acc: 0.5195 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9263 train_acc: 0.8555 val_loss: 1.8412 val_acc: 0.5355 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8715 train_acc: 0.8772 val_loss: 0.9537 val_acc: 0.8401 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8357 train_acc: 0.8913 val_loss: 1.8290 val_acc: 0.5475 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8096 train_acc: 0.9017 val_loss: 0.8760 val_acc: 0.8717 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7882 train_acc: 0.9108 val_loss: 1.6035 val_acc: 0.6143 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7743 train_acc: 0.9162 val_loss: 1.5264 val_acc: 0.6406 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7591 train_acc: 0.9227 val_loss: 0.8407 val_acc: 0.8830 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7492 train_acc: 0.9264 val_loss: 1.5220 val_acc: 0.6182 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7384 train_acc: 0.9310 val_loss: 1.4685 val_acc: 0.6603 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7305 train_acc: 0.9337 val_loss: 1.7729 val_acc: 0.5844 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.7221 train_acc: 0.9377 val_loss: 1.4098 val_acc: 0.6777 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6875 train_acc: 0.9517 val_loss: 1.0644 val_acc: 0.7978 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6785 train_acc: 0.9555 val_loss: 1.5474 val_acc: 0.6159 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6721 train_acc: 0.9583 val_loss: 1.3585 val_acc: 0.7117 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6655 train_acc: 0.9607 val_loss: 0.7503 val_acc: 0.9231 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6613 train_acc: 0.9623 val_loss: 0.8595 val_acc: 0.8808 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.6568 train_acc: 0.9645 val_loss: 0.7497 val_acc: 0.9234 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.6527 train_acc: 0.9663 val_loss: 0.8116 val_acc: 0.9005 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6492 train_acc: 0.9673 val_loss: 1.1365 val_acc: 0.7809 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6447 train_acc: 0.9694 val_loss: 1.0140 val_acc: 0.8181 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6418 train_acc: 0.9708 val_loss: 1.1148 val_acc: 0.7809 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.6240 train_acc: 0.9782 val_loss: 0.7496 val_acc: 0.9264 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.6194 train_acc: 0.9801 val_loss: 0.8798 val_acc: 0.8765 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.6158 train_acc: 0.9819 val_loss: 1.0784 val_acc: 0.8093 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.6136 train_acc: 0.9829 val_loss: 0.7203 val_acc: 0.9376 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.6113 train_acc: 0.9836 val_loss: 0.8794 val_acc: 0.8793 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.6092 train_acc: 0.9840 val_loss: 1.0105 val_acc: 0.8379 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.6065 train_acc: 0.9853 val_loss: 0.8737 val_acc: 0.8808 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.6054 train_acc: 0.9856 val_loss: 0.9300 val_acc: 0.8649 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5974 train_acc: 0.9890 val_loss: 0.7220 val_acc: 0.9384 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5955 train_acc: 0.9896 val_loss: 0.8145 val_acc: 0.9044 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5940 train_acc: 0.9902 val_loss: 0.8049 val_acc: 0.9088 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.5930 train_acc: 0.9905 val_loss: 0.7333 val_acc: 0.9350 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.5915 train_acc: 0.9912 val_loss: 0.7309 val_acc: 0.9358 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5881 train_acc: 0.9925 val_loss: 0.7283 val_acc: 0.9361 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5872 train_acc: 0.9930 val_loss: 0.7349 val_acc: 0.9355 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5865 train_acc: 0.9931 val_loss: 0.7257 val_acc: 0.9382 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5863 train_acc: 0.9931 val_loss: 0.7261 val_acc: 0.9383 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.5846 train_acc: 0.9938 val_loss: 0.7218 val_acc: 0.9398 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5837 train_acc: 0.9941 val_loss: 0.7247 val_acc: 0.9391 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5837 train_acc: 0.9941 val_loss: 0.7285 val_acc: 0.9378 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5830 train_acc: 0.9943 val_loss: 0.7291 val_acc: 0.9369 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.5828 train_acc: 0.9943 val_loss: 0.7452 val_acc: 0.9321 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.5823 train_acc: 0.9945 val_loss: 0.7228 val_acc: 0.9402 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.5818 train_acc: 0.9949 val_loss: 0.7222 val_acc: 0.9402 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.5816 train_acc: 0.9947 val_loss: 0.7240 val_acc: 0.9397 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5815 train_acc: 0.9949 val_loss: 0.7251 val_acc: 0.9393 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5811 train_acc: 0.9949 val_loss: 0.7243 val_acc: 0.9395 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.5811 train_acc: 0.9948 val_loss: 0.7247 val_acc: 0.9397 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5809 train_acc: 0.9952 val_loss: 0.7245 val_acc: 0.9394 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5808 train_acc: 0.9951 val_loss: 0.7248 val_acc: 0.9393 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5808 train_acc: 0.9950 val_loss: 0.7248 val_acc: 0.9396 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5808 train_acc: 0.9951 val_loss: 0.7303 val_acc: 0.9369 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5805 train_acc: 0.9951 val_loss: 0.7244 val_acc: 0.9392 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5805 train_acc: 0.9951 val_loss: 0.7253 val_acc: 0.9394 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5804 train_acc: 0.9952 val_loss: 0.7248 val_acc: 0.9395 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5802 train_acc: 0.9951 val_loss: 0.7249 val_acc: 0.9393 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5801 train_acc: 0.9954 val_loss: 0.7251 val_acc: 0.9391 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5799 train_acc: 0.9953 val_loss: 0.7253 val_acc: 0.9393 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5798 train_acc: 0.9954 val_loss: 0.7246 val_acc: 0.9394 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5800 train_acc: 0.9954 val_loss: 0.7256 val_acc: 0.9392 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5797 train_acc: 0.9955 val_loss: 0.7263 val_acc: 0.9388 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5797 train_acc: 0.9953 val_loss: 0.7253 val_acc: 0.9387 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5796 train_acc: 0.9954 val_loss: 0.7256 val_acc: 0.9393 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5797 train_acc: 0.9953 val_loss: 0.7267 val_acc: 0.9388 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5794 train_acc: 0.9956 val_loss: 0.7303 val_acc: 0.9379 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5797 train_acc: 0.9952 val_loss: 0.7260 val_acc: 0.9394 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5791 train_acc: 0.9955 val_loss: 0.7262 val_acc: 0.9395 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5794 train_acc: 0.9953 val_loss: 0.7266 val_acc: 0.9389 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5791 train_acc: 0.9955 val_loss: 0.7273 val_acc: 0.9389 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5792 train_acc: 0.9955 val_loss: 0.7274 val_acc: 0.9387 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5789 train_acc: 0.9957 val_loss: 0.7260 val_acc: 0.9392 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5788 train_acc: 0.9956 val_loss: 0.7267 val_acc: 0.9394 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5788 train_acc: 0.9957 val_loss: 0.7276 val_acc: 0.9385 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5790 train_acc: 0.9956 val_loss: 0.7268 val_acc: 0.9398 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9401\n",
      "20000: 0.9400555555555555\n",
      " test_acc: 0.9401\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.2978 train_acc: 0.6986 val_loss: 1.7229 val_acc: 0.5708 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0157 train_acc: 0.8217 val_loss: 0.9496 val_acc: 0.8479 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9375 train_acc: 0.8556 val_loss: 1.0902 val_acc: 0.7823 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8868 train_acc: 0.8765 val_loss: 1.1299 val_acc: 0.7696 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8494 train_acc: 0.8914 val_loss: 1.6811 val_acc: 0.5792 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8209 train_acc: 0.9016 val_loss: 0.8038 val_acc: 0.9063 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7964 train_acc: 0.9110 val_loss: 1.1716 val_acc: 0.7583 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7751 train_acc: 0.9195 val_loss: 2.1893 val_acc: 0.4840 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7572 train_acc: 0.9265 val_loss: 0.8109 val_acc: 0.8997 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7417 train_acc: 0.9326 val_loss: 1.1967 val_acc: 0.7351 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.6976 train_acc: 0.9512 val_loss: 1.0126 val_acc: 0.8143 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.6827 train_acc: 0.9574 val_loss: 1.6980 val_acc: 0.6130 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6728 train_acc: 0.9614 val_loss: 0.7433 val_acc: 0.9277 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6618 train_acc: 0.9659 val_loss: 0.8914 val_acc: 0.8699 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6551 train_acc: 0.9687 val_loss: 0.7521 val_acc: 0.9259 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6469 train_acc: 0.9720 val_loss: 1.0193 val_acc: 0.8268 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6411 train_acc: 0.9742 val_loss: 2.1753 val_acc: 0.5097 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.6205 train_acc: 0.9829 val_loss: 1.3806 val_acc: 0.7265 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.6150 train_acc: 0.9850 val_loss: 0.9387 val_acc: 0.8599 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.6107 train_acc: 0.9864 val_loss: 1.2725 val_acc: 0.7546 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.6072 train_acc: 0.9876 val_loss: 1.6596 val_acc: 0.6431 lr: 0.000625\n",
      "Epoch  21 / 200 train_loss: 0.5988 train_acc: 0.9909 val_loss: 0.7707 val_acc: 0.9190 lr: 0.000625\n",
      "Epoch  22 / 200 train_loss: 0.5959 train_acc: 0.9919 val_loss: 0.7338 val_acc: 0.9333 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.5940 train_acc: 0.9928 val_loss: 0.7372 val_acc: 0.9329 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.5919 train_acc: 0.9933 val_loss: 0.7315 val_acc: 0.9358 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.5904 train_acc: 0.9938 val_loss: 0.8758 val_acc: 0.8809 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.5890 train_acc: 0.9942 val_loss: 0.7947 val_acc: 0.9128 lr: 0.000625\n",
      "Epoch  27 / 200 train_loss: 0.5872 train_acc: 0.9946 val_loss: 0.7588 val_acc: 0.9267 lr: 0.000625\n",
      "Epoch  28 / 200 train_loss: 0.5866 train_acc: 0.9947 val_loss: 0.7693 val_acc: 0.9206 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.5829 train_acc: 0.9961 val_loss: 0.7573 val_acc: 0.9264 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.5823 train_acc: 0.9960 val_loss: 0.7492 val_acc: 0.9284 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.5810 train_acc: 0.9965 val_loss: 0.7317 val_acc: 0.9362 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.5805 train_acc: 0.9966 val_loss: 0.8131 val_acc: 0.9067 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.5798 train_acc: 0.9968 val_loss: 0.7485 val_acc: 0.9308 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.5789 train_acc: 0.9971 val_loss: 0.7252 val_acc: 0.9376 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.5783 train_acc: 0.9973 val_loss: 0.7570 val_acc: 0.9266 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5777 train_acc: 0.9974 val_loss: 0.7271 val_acc: 0.9381 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5770 train_acc: 0.9975 val_loss: 0.7525 val_acc: 0.9293 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5767 train_acc: 0.9975 val_loss: 0.7580 val_acc: 0.9270 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5761 train_acc: 0.9977 val_loss: 0.7354 val_acc: 0.9348 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.5757 train_acc: 0.9976 val_loss: 0.7311 val_acc: 0.9362 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5750 train_acc: 0.9979 val_loss: 0.7314 val_acc: 0.9366 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5745 train_acc: 0.9979 val_loss: 0.7284 val_acc: 0.9381 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5738 train_acc: 0.9982 val_loss: 0.7320 val_acc: 0.9358 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.5735 train_acc: 0.9981 val_loss: 0.7397 val_acc: 0.9334 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.5730 train_acc: 0.9984 val_loss: 0.7299 val_acc: 0.9368 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.5726 train_acc: 0.9984 val_loss: 0.7270 val_acc: 0.9380 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.5726 train_acc: 0.9985 val_loss: 0.7288 val_acc: 0.9374 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5724 train_acc: 0.9985 val_loss: 0.7264 val_acc: 0.9384 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5723 train_acc: 0.9985 val_loss: 0.7436 val_acc: 0.9324 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.5721 train_acc: 0.9985 val_loss: 0.7301 val_acc: 0.9371 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.5719 train_acc: 0.9985 val_loss: 0.7318 val_acc: 0.9357 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.5720 train_acc: 0.9985 val_loss: 0.7307 val_acc: 0.9371 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5715 train_acc: 0.9987 val_loss: 0.7302 val_acc: 0.9374 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5715 train_acc: 0.9986 val_loss: 0.7285 val_acc: 0.9374 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5713 train_acc: 0.9986 val_loss: 0.7287 val_acc: 0.9373 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5714 train_acc: 0.9986 val_loss: 0.7285 val_acc: 0.9371 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5714 train_acc: 0.9986 val_loss: 0.7291 val_acc: 0.9376 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5711 train_acc: 0.9987 val_loss: 0.7304 val_acc: 0.9365 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5711 train_acc: 0.9987 val_loss: 0.7440 val_acc: 0.9322 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5713 train_acc: 0.9986 val_loss: 0.7320 val_acc: 0.9364 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5708 train_acc: 0.9988 val_loss: 0.7324 val_acc: 0.9364 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5708 train_acc: 0.9988 val_loss: 0.7303 val_acc: 0.9369 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5706 train_acc: 0.9988 val_loss: 0.7320 val_acc: 0.9365 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5707 train_acc: 0.9987 val_loss: 0.7300 val_acc: 0.9368 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5707 train_acc: 0.9986 val_loss: 0.7293 val_acc: 0.9373 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5705 train_acc: 0.9987 val_loss: 0.7347 val_acc: 0.9355 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5705 train_acc: 0.9988 val_loss: 0.7317 val_acc: 0.9368 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5705 train_acc: 0.9986 val_loss: 0.7310 val_acc: 0.9373 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5704 train_acc: 0.9988 val_loss: 0.7303 val_acc: 0.9374 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5703 train_acc: 0.9988 val_loss: 0.7303 val_acc: 0.9377 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5701 train_acc: 0.9990 val_loss: 0.7322 val_acc: 0.9366 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5703 train_acc: 0.9988 val_loss: 0.7301 val_acc: 0.9372 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5702 train_acc: 0.9988 val_loss: 0.7296 val_acc: 0.9371 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5700 train_acc: 0.9990 val_loss: 0.7364 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5702 train_acc: 0.9987 val_loss: 0.7312 val_acc: 0.9369 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5699 train_acc: 0.9988 val_loss: 0.7311 val_acc: 0.9370 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.5698 train_acc: 0.9988 val_loss: 0.7297 val_acc: 0.9368 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.5698 train_acc: 0.9989 val_loss: 0.7325 val_acc: 0.9362 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9388\n",
      "15000: 0.9387777777777778\n",
      " test_acc: 0.9388\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.4087 train_acc: 0.6479 val_loss: 1.3300 val_acc: 0.6754 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0361 train_acc: 0.8102 val_loss: 1.1112 val_acc: 0.7759 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9354 train_acc: 0.8539 val_loss: 1.4102 val_acc: 0.6350 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8776 train_acc: 0.8775 val_loss: 0.8755 val_acc: 0.8761 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8387 train_acc: 0.8938 val_loss: 0.9910 val_acc: 0.8248 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8081 train_acc: 0.9064 val_loss: 0.8932 val_acc: 0.8635 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7833 train_acc: 0.9164 val_loss: 1.2760 val_acc: 0.7165 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7605 train_acc: 0.9256 val_loss: 0.8637 val_acc: 0.8792 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7443 train_acc: 0.9318 val_loss: 1.0218 val_acc: 0.8127 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7258 train_acc: 0.9401 val_loss: 1.2649 val_acc: 0.7315 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7115 train_acc: 0.9456 val_loss: 1.8812 val_acc: 0.5546 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.6981 train_acc: 0.9512 val_loss: 0.8504 val_acc: 0.8825 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.6882 train_acc: 0.9554 val_loss: 1.8617 val_acc: 0.5598 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.6767 train_acc: 0.9603 val_loss: 1.5604 val_acc: 0.6491 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.6680 train_acc: 0.9635 val_loss: 0.8217 val_acc: 0.8963 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.6610 train_acc: 0.9669 val_loss: 1.3442 val_acc: 0.7214 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.6543 train_acc: 0.9696 val_loss: 0.8441 val_acc: 0.8861 lr: 0.005\n",
      "Epoch  17 / 200 train_loss: 0.6466 train_acc: 0.9725 val_loss: 1.3703 val_acc: 0.6969 lr: 0.005\n",
      "Epoch  18 / 200 train_loss: 0.6413 train_acc: 0.9745 val_loss: 0.7835 val_acc: 0.9128 lr: 0.005\n",
      "Epoch  19 / 200 train_loss: 0.6380 train_acc: 0.9760 val_loss: 1.5629 val_acc: 0.6496 lr: 0.005\n",
      "Epoch  20 / 200 train_loss: 0.6326 train_acc: 0.9777 val_loss: 1.0816 val_acc: 0.7976 lr: 0.005\n",
      "Epoch  21 / 200 train_loss: 0.6280 train_acc: 0.9800 val_loss: 1.4086 val_acc: 0.6883 lr: 0.005\n",
      "Epoch  22 / 200 train_loss: 0.6254 train_acc: 0.9808 val_loss: 0.7877 val_acc: 0.9113 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.6002 train_acc: 0.9908 val_loss: 1.0105 val_acc: 0.8318 lr: 0.0025\n",
      "Epoch  24 / 200 train_loss: 0.5930 train_acc: 0.9932 val_loss: 0.7664 val_acc: 0.9201 lr: 0.0025\n",
      "Epoch  25 / 200 train_loss: 0.5910 train_acc: 0.9938 val_loss: 1.0142 val_acc: 0.8308 lr: 0.0025\n",
      "Epoch  26 / 200 train_loss: 0.5884 train_acc: 0.9946 val_loss: 0.8358 val_acc: 0.8942 lr: 0.0025\n",
      "Epoch  27 / 200 train_loss: 0.5874 train_acc: 0.9948 val_loss: 1.0568 val_acc: 0.8175 lr: 0.0025\n",
      "Epoch  28 / 200 train_loss: 0.5856 train_acc: 0.9951 val_loss: 1.2486 val_acc: 0.7639 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.5779 train_acc: 0.9976 val_loss: 1.0369 val_acc: 0.8358 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.5753 train_acc: 0.9981 val_loss: 0.8751 val_acc: 0.8818 lr: 0.00125\n",
      "Epoch  31 / 200 train_loss: 0.5743 train_acc: 0.9983 val_loss: 0.7888 val_acc: 0.9129 lr: 0.00125\n",
      "Epoch  32 / 200 train_loss: 0.5735 train_acc: 0.9986 val_loss: 0.7729 val_acc: 0.9197 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5704 train_acc: 0.9991 val_loss: 0.7326 val_acc: 0.9350 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.5694 train_acc: 0.9993 val_loss: 1.0691 val_acc: 0.8200 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.5692 train_acc: 0.9992 val_loss: 0.7373 val_acc: 0.9344 lr: 0.000625\n",
      "Epoch  36 / 200 train_loss: 0.5683 train_acc: 0.9994 val_loss: 0.7436 val_acc: 0.9313 lr: 0.000625\n",
      "Epoch  37 / 200 train_loss: 0.5681 train_acc: 0.9994 val_loss: 0.7428 val_acc: 0.9307 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5670 train_acc: 0.9996 val_loss: 0.7346 val_acc: 0.9360 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5664 train_acc: 0.9997 val_loss: 0.7343 val_acc: 0.9344 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.5661 train_acc: 0.9997 val_loss: 0.7474 val_acc: 0.9310 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.5660 train_acc: 0.9996 val_loss: 0.7318 val_acc: 0.9366 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.5656 train_acc: 0.9997 val_loss: 0.7659 val_acc: 0.9216 lr: 0.0003125\n",
      "Epoch  43 / 200 train_loss: 0.5656 train_acc: 0.9997 val_loss: 0.7342 val_acc: 0.9361 lr: 0.0003125\n",
      "Epoch  44 / 200 train_loss: 0.5653 train_acc: 0.9997 val_loss: 0.8180 val_acc: 0.9054 lr: 0.0003125\n",
      "Epoch  45 / 200 train_loss: 0.5650 train_acc: 0.9998 val_loss: 0.7344 val_acc: 0.9365 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.5646 train_acc: 0.9998 val_loss: 0.7390 val_acc: 0.9331 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.5645 train_acc: 0.9998 val_loss: 0.7350 val_acc: 0.9352 lr: 0.00015625\n",
      "Epoch  48 / 200 train_loss: 0.5645 train_acc: 0.9998 val_loss: 0.7381 val_acc: 0.9332 lr: 0.00015625\n",
      "Epoch  49 / 200 train_loss: 0.5643 train_acc: 0.9998 val_loss: 0.7567 val_acc: 0.9279 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.5641 train_acc: 0.9998 val_loss: 0.7358 val_acc: 0.9342 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.5640 train_acc: 0.9998 val_loss: 0.7332 val_acc: 0.9359 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.5639 train_acc: 0.9998 val_loss: 0.7328 val_acc: 0.9364 lr: 7.8125e-05\n",
      "Epoch  53 / 200 train_loss: 0.5638 train_acc: 0.9998 val_loss: 0.7402 val_acc: 0.9336 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5638 train_acc: 0.9998 val_loss: 0.7335 val_acc: 0.9358 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5638 train_acc: 0.9998 val_loss: 0.7346 val_acc: 0.9348 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5638 train_acc: 0.9998 val_loss: 0.7342 val_acc: 0.9359 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5637 train_acc: 0.9998 val_loss: 0.7335 val_acc: 0.9357 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5637 train_acc: 0.9998 val_loss: 0.7343 val_acc: 0.9362 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5635 train_acc: 0.9998 val_loss: 0.7330 val_acc: 0.9366 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5635 train_acc: 0.9998 val_loss: 0.7348 val_acc: 0.9362 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5636 train_acc: 0.9999 val_loss: 0.7371 val_acc: 0.9346 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5634 train_acc: 0.9998 val_loss: 0.7346 val_acc: 0.9356 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5635 train_acc: 0.9998 val_loss: 0.7352 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5635 train_acc: 0.9998 val_loss: 0.7339 val_acc: 0.9357 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5633 train_acc: 0.9999 val_loss: 0.7338 val_acc: 0.9356 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5633 train_acc: 0.9999 val_loss: 0.7336 val_acc: 0.9358 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5634 train_acc: 0.9999 val_loss: 0.7381 val_acc: 0.9331 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5634 train_acc: 0.9998 val_loss: 0.7341 val_acc: 0.9359 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5633 train_acc: 0.9998 val_loss: 0.7349 val_acc: 0.9355 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5633 train_acc: 0.9998 val_loss: 0.7356 val_acc: 0.9359 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5632 train_acc: 0.9999 val_loss: 0.7344 val_acc: 0.9361 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5631 train_acc: 0.9999 val_loss: 0.7364 val_acc: 0.9356 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5632 train_acc: 0.9998 val_loss: 0.7366 val_acc: 0.9356 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5631 train_acc: 0.9998 val_loss: 0.7357 val_acc: 0.9357 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5630 train_acc: 0.9999 val_loss: 0.7344 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5631 train_acc: 0.9999 val_loss: 0.7342 val_acc: 0.9354 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.5630 train_acc: 0.9999 val_loss: 0.7377 val_acc: 0.9344 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.5631 train_acc: 0.9998 val_loss: 0.7363 val_acc: 0.9357 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.5630 train_acc: 0.9999 val_loss: 0.7349 val_acc: 0.9354 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.5630 train_acc: 0.9999 val_loss: 0.7363 val_acc: 0.9355 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.5630 train_acc: 0.9998 val_loss: 0.7368 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.5629 train_acc: 0.9999 val_loss: 0.7365 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.5629 train_acc: 0.9999 val_loss: 0.7361 val_acc: 0.9348 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.5629 train_acc: 0.9999 val_loss: 0.7365 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.5629 train_acc: 0.9999 val_loss: 0.7370 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.5629 train_acc: 0.9999 val_loss: 0.7374 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.5628 train_acc: 0.9999 val_loss: 0.7363 val_acc: 0.9360 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.5627 train_acc: 0.9999 val_loss: 0.7365 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.5628 train_acc: 0.9998 val_loss: 0.7355 val_acc: 0.9351 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9356\n",
      "10000: 0.9355666666666667\n",
      " test_acc: 0.9356\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.6125 train_acc: 0.5658 val_loss: 1.9194 val_acc: 0.4180 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.2274 train_acc: 0.7260 val_loss: 1.2857 val_acc: 0.6961 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 1.1098 train_acc: 0.7794 val_loss: 1.0844 val_acc: 0.7827 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 1.0397 train_acc: 0.8105 val_loss: 1.6869 val_acc: 0.5672 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.9922 train_acc: 0.8305 val_loss: 1.9477 val_acc: 0.5148 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.9556 train_acc: 0.8477 val_loss: 1.7781 val_acc: 0.5582 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.9204 train_acc: 0.8629 val_loss: 1.8234 val_acc: 0.5655 lr: 0.0025\n",
      "Epoch   7 / 200 train_loss: 0.8532 train_acc: 0.8907 val_loss: 1.2534 val_acc: 0.7305 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.8322 train_acc: 0.8998 val_loss: 0.9260 val_acc: 0.8512 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.8116 train_acc: 0.9092 val_loss: 0.9570 val_acc: 0.8432 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.7921 train_acc: 0.9180 val_loss: 1.0150 val_acc: 0.8212 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.7786 train_acc: 0.9226 val_loss: 1.3993 val_acc: 0.6953 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.7619 train_acc: 0.9308 val_loss: 0.9132 val_acc: 0.8627 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.7446 train_acc: 0.9388 val_loss: 1.4434 val_acc: 0.6847 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.7301 train_acc: 0.9450 val_loss: 1.2490 val_acc: 0.7627 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.7187 train_acc: 0.9510 val_loss: 1.8803 val_acc: 0.5744 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.7053 train_acc: 0.9553 val_loss: 0.9302 val_acc: 0.8655 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6943 train_acc: 0.9604 val_loss: 1.3927 val_acc: 0.7147 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.6813 train_acc: 0.9659 val_loss: 0.9109 val_acc: 0.8620 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.6725 train_acc: 0.9689 val_loss: 0.8612 val_acc: 0.8801 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6651 train_acc: 0.9722 val_loss: 0.8180 val_acc: 0.9011 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6566 train_acc: 0.9758 val_loss: 1.1807 val_acc: 0.7780 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6527 train_acc: 0.9773 val_loss: 0.9188 val_acc: 0.8659 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.6419 train_acc: 0.9815 val_loss: 1.1932 val_acc: 0.7733 lr: 0.0025\n",
      "Epoch  24 / 200 train_loss: 0.6391 train_acc: 0.9824 val_loss: 0.8297 val_acc: 0.9001 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.6164 train_acc: 0.9910 val_loss: 1.0794 val_acc: 0.8099 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.6084 train_acc: 0.9935 val_loss: 0.7808 val_acc: 0.9177 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.6054 train_acc: 0.9943 val_loss: 1.1754 val_acc: 0.7778 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.6027 train_acc: 0.9948 val_loss: 0.8257 val_acc: 0.9008 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.6013 train_acc: 0.9950 val_loss: 0.8876 val_acc: 0.8748 lr: 0.00125\n",
      "Epoch  30 / 200 train_loss: 0.5978 train_acc: 0.9961 val_loss: 0.7997 val_acc: 0.9095 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5913 train_acc: 0.9976 val_loss: 0.8667 val_acc: 0.8809 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5885 train_acc: 0.9982 val_loss: 0.7794 val_acc: 0.9151 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5868 train_acc: 0.9982 val_loss: 0.8134 val_acc: 0.9053 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.5860 train_acc: 0.9985 val_loss: 0.8338 val_acc: 0.8932 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.5833 train_acc: 0.9988 val_loss: 0.7833 val_acc: 0.9129 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5819 train_acc: 0.9990 val_loss: 0.7959 val_acc: 0.9105 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5814 train_acc: 0.9991 val_loss: 0.7778 val_acc: 0.9135 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5805 train_acc: 0.9991 val_loss: 0.7943 val_acc: 0.9072 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.5792 train_acc: 0.9994 val_loss: 0.7796 val_acc: 0.9162 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.5789 train_acc: 0.9993 val_loss: 0.7779 val_acc: 0.9152 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5782 train_acc: 0.9995 val_loss: 0.7759 val_acc: 0.9153 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5785 train_acc: 0.9992 val_loss: 0.7846 val_acc: 0.9117 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.5776 train_acc: 0.9994 val_loss: 0.7775 val_acc: 0.9145 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.5769 train_acc: 0.9995 val_loss: 0.7789 val_acc: 0.9163 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.5769 train_acc: 0.9995 val_loss: 0.7807 val_acc: 0.9147 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.5767 train_acc: 0.9994 val_loss: 0.7916 val_acc: 0.9119 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.5766 train_acc: 0.9995 val_loss: 0.7826 val_acc: 0.9152 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.5764 train_acc: 0.9994 val_loss: 0.7751 val_acc: 0.9167 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.5762 train_acc: 0.9995 val_loss: 0.7743 val_acc: 0.9146 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.5764 train_acc: 0.9995 val_loss: 0.7762 val_acc: 0.9133 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5764 train_acc: 0.9995 val_loss: 0.7747 val_acc: 0.9170 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5757 train_acc: 0.9995 val_loss: 0.7726 val_acc: 0.9165 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5760 train_acc: 0.9996 val_loss: 0.7865 val_acc: 0.9130 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5758 train_acc: 0.9995 val_loss: 0.7814 val_acc: 0.9147 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5755 train_acc: 0.9995 val_loss: 0.7754 val_acc: 0.9168 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5760 train_acc: 0.9996 val_loss: 0.7740 val_acc: 0.9160 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9126\n",
      "5000: 0.9126222222222222\n",
      " test_acc: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sample_size</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.952322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.940056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.938778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.935567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.912622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_sample_size       acc\n",
       "0              25000  0.952322\n",
       "1              20000  0.940056\n",
       "2              15000  0.938778\n",
       "3              10000  0.935567\n",
       "4               5000  0.912622"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = []\n",
    "# train_sample_sizes = [14000, 12000, 10000, 8000, 6000, 4000]\n",
    "# train_sample_sizes = [4000, 6000, 8000, 10000, 12000, 14000]\n",
    "train_sample_sizes = [25000, 20000, 15000, 10000, 5000]\n",
    "\n",
    "for train_sample_size in train_sample_sizes:\n",
    "    acc = train_pipeline(train_objs, test_objs, labels, y_code_dict, all_peps, train_name=f'clean_data_ds_{train_sample_size}', train_sample_size=train_sample_size)\n",
    "    acc_df.append([train_sample_size, acc])\n",
    "acc_df =pd.DataFrame(acc_df)\n",
    "acc_df.columns = ['train_sample_size', 'acc']\n",
    "acc_df.to_csv(\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/clean/acc.csv\")\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2db0e8-3bce-4b3c-b3b5-bf303553ec88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
