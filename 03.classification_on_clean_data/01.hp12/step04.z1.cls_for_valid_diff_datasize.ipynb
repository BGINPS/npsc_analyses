{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3319fb-caac-402e-8aef-65ce018aa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.sans-serif'] = \"Arial\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '/Data/user/panhailin/git_lab/npspy')\n",
    "import npspy as nps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea64e7f-45b8-46db-b049-23707b3a8ea7",
   "metadata": {},
   "source": [
    "# 全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc2f31f-983e-4dd5-9390-596504420e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({np.str_('hp1_1'): 0,\n",
       "  np.str_('hp1_2'): 1,\n",
       "  np.str_('hp1_3'): 2,\n",
       "  np.str_('hp1_4'): 3,\n",
       "  np.str_('hp1_5'): 4,\n",
       "  np.str_('hp1_6'): 5,\n",
       "  np.str_('hp1_7'): 6,\n",
       "  np.str_('hp1_8'): 7,\n",
       "  np.str_('hp1_9'): 8,\n",
       "  np.str_('hp2_1'): 9,\n",
       "  np.str_('hp2_2'): 10,\n",
       "  np.str_('hp2_3'): 11,\n",
       "  np.str_('hp2_4'): 12,\n",
       "  np.str_('hp2_5'): 13,\n",
       "  np.str_('hp2_6'): 14},\n",
       " {0: np.str_('hp1_1'),\n",
       "  1: np.str_('hp1_2'),\n",
       "  2: np.str_('hp1_3'),\n",
       "  3: np.str_('hp1_4'),\n",
       "  4: np.str_('hp1_5'),\n",
       "  5: np.str_('hp1_6'),\n",
       "  6: np.str_('hp1_7'),\n",
       "  7: np.str_('hp1_8'),\n",
       "  8: np.str_('hp1_9'),\n",
       "  9: np.str_('hp2_1'),\n",
       "  10: np.str_('hp2_2'),\n",
       "  11: np.str_('hp2_3'),\n",
       "  12: np.str_('hp2_4'),\n",
       "  13: np.str_('hp2_5'),\n",
       "  14: np.str_('hp2_6')})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_num_threads(10)\n",
    "\n",
    "all_peps = [\n",
    "    'hp1_1', 'hp1_2', 'hp1_3', 'hp1_4', 'hp1_5', 'hp1_6', 'hp1_7', 'hp1_8', 'hp1_9',\n",
    "    'hp2_1', 'hp2_2', 'hp2_3', 'hp2_4', 'hp2_5', 'hp2_6',\n",
    "]\n",
    "\n",
    "y_code_dict = nps.ml.set_y_codes_for_classes(np.array(all_peps)[:,None])\n",
    "y_code_dict\n",
    "y_to_label_dict = {v:k for k,v in y_code_dict.items()}\n",
    "y_code_dict, y_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e8a7d-e387-4f28-b79d-9e7a3c7beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs = [f\"../../../00.data/{pep}_valid80.pkl\" for pep in all_peps]\n",
    "test_objs = [f\"../../../00.data/{pep}_valid20.pkl\" for pep in all_peps]\n",
    "labels = all_peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a4b762-6fee-4531-b733-838c7f758d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df, column_name, sample_size=15000, random_state=42):\n",
    "    \"\"\"\n",
    "    对DataFrame按指定列类别分层随机抽样\n",
    "    \n",
    "    参数:\n",
    "        df: 输入DataFrame\n",
    "        column_name: 分层依据的列名\n",
    "        sample_size: 每类抽取样本数(默认15000)\n",
    "        random_state: 随机种子\n",
    "    \n",
    "    返回:\n",
    "        抽样后的新DataFrame\n",
    "    \"\"\"\n",
    "    re_df = df.groupby(column_name, group_keys=True).apply(\n",
    "        lambda x: x.sample(min(len(x), sample_size), \n",
    "                          random_state=random_state),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    re_df[re_df.index.names[0]] =  [i[0] for i in re_df.index]\n",
    "    re_df.index = [i[1] for i in re_df.index]\n",
    "    return re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5218de-adc9-4cb2-8455-4b8b685d68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(train_objs, labels, y_code_dict, all_peps, train_name='valid_data', train_sample_size=14000, seed=42):\n",
    "    # 读取pkl文件，生成readid，X，y组成的df\n",
    "    train_df = nps.ml.get_X_y_from_objs(objs=train_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    train_df = stratified_sample(train_df, 'y', sample_size=train_sample_size, random_state=seed)\n",
    "    train_df, valid_df = train_test_split(train_df, test_size=1/8, random_state=seed, stratify=train_df['y'])\n",
    "    test_df = nps.ml.get_X_y_from_objs(objs=test_objs, labels=labels, y_code_dict=y_code_dict, down_sample_to=1000, att='signal')\n",
    "    test_df = stratified_sample(test_df, 'y', sample_size=6000, random_state=seed)\n",
    "\n",
    "    # 通过data_df构建dataloader\n",
    "    batch_size = 128\n",
    "    train_dl = nps.ml.construct_dataloader_from_data_df(train_df, batch_size=batch_size, augment=False)\n",
    "    valid_dl = nps.ml.construct_dataloader_from_data_df(valid_df, batch_size=batch_size)\n",
    "    test_dl = nps.ml.construct_dataloader_from_data_df(test_df, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train\n",
    "    nps.ml.seed_everything(seed)\n",
    "    clf = nps.ml.Trainer(lr=0.005, num_classes=len(all_peps), epochs=200, device='cuda', lr_scheduler_patience=3, label_smoothing=0.1, model_name='CNN1DL1000')\n",
    "    clf.fit(train_dl, valid_dl, early_stopping_patience=30, name=train_name)\n",
    "\n",
    "    # pred\n",
    "    pred_df = clf.predict(test_dl, name=train_name, y_to_label_dict=y_to_label_dict)\n",
    "    test_all_reads_s = pred_df['true'].value_counts()\n",
    "    cm_df = nps.ml.get_cm(pred_df, label_order=all_peps)\n",
    "    cm_df.to_csv(f\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/valid/{train_name}_cm.csv\")\n",
    "    acc = np.sum(np.diag(cm_df))/len(pred_df)\n",
    "    print(f'{train_sample_size}: {acc}')\n",
    "    pred_proba_df = clf.predict_proba(test_dl, name=train_name)\n",
    "    pred_proba_df.to_csv(f\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/valid/{train_name}_pred_proba.csv\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833c8ae5-cf4d-4ac1-97fb-58b8f9cd6176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.2432 train_acc: 0.7271 val_loss: 1.0308 val_acc: 0.8137 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.9540 train_acc: 0.8509 val_loss: 1.2691 val_acc: 0.7122 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8799 train_acc: 0.8801 val_loss: 1.1615 val_acc: 0.7591 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8370 train_acc: 0.8964 val_loss: 1.2595 val_acc: 0.7271 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8053 train_acc: 0.9082 val_loss: 0.9356 val_acc: 0.8480 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7833 train_acc: 0.9168 val_loss: 0.8729 val_acc: 0.8784 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7631 train_acc: 0.9244 val_loss: 1.3552 val_acc: 0.6829 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7472 train_acc: 0.9309 val_loss: 0.8967 val_acc: 0.8620 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7335 train_acc: 0.9362 val_loss: 0.9961 val_acc: 0.8242 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7210 train_acc: 0.9407 val_loss: 0.8413 val_acc: 0.8878 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7122 train_acc: 0.9442 val_loss: 2.4226 val_acc: 0.4590 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7020 train_acc: 0.9487 val_loss: 1.4785 val_acc: 0.6560 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.6928 train_acc: 0.9523 val_loss: 2.4812 val_acc: 0.4469 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.6863 train_acc: 0.9551 val_loss: 0.7607 val_acc: 0.9233 lr: 0.005\n",
      "Epoch  19 / 200 train_loss: 0.6217 train_acc: 0.9814 val_loss: 0.7593 val_acc: 0.9261 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6160 train_acc: 0.9840 val_loss: 1.0861 val_acc: 0.8231 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6128 train_acc: 0.9849 val_loss: 1.0376 val_acc: 0.8392 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6087 train_acc: 0.9864 val_loss: 0.7849 val_acc: 0.9176 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.6064 train_acc: 0.9870 val_loss: 0.9099 val_acc: 0.8744 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.5940 train_acc: 0.9922 val_loss: 0.7600 val_acc: 0.9279 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.5905 train_acc: 0.9933 val_loss: 0.7403 val_acc: 0.9343 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.5886 train_acc: 0.9939 val_loss: 0.8111 val_acc: 0.9104 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.5868 train_acc: 0.9944 val_loss: 0.7678 val_acc: 0.9244 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.5852 train_acc: 0.9950 val_loss: 0.8546 val_acc: 0.8975 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.5838 train_acc: 0.9953 val_loss: 0.8589 val_acc: 0.8949 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5795 train_acc: 0.9967 val_loss: 0.7442 val_acc: 0.9332 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5779 train_acc: 0.9971 val_loss: 0.8895 val_acc: 0.8883 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5770 train_acc: 0.9974 val_loss: 0.7622 val_acc: 0.9267 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5764 train_acc: 0.9972 val_loss: 0.7425 val_acc: 0.9340 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.5743 train_acc: 0.9981 val_loss: 0.7425 val_acc: 0.9342 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.5737 train_acc: 0.9981 val_loss: 0.7417 val_acc: 0.9348 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5733 train_acc: 0.9982 val_loss: 0.7585 val_acc: 0.9289 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5729 train_acc: 0.9983 val_loss: 0.7440 val_acc: 0.9339 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5726 train_acc: 0.9984 val_loss: 0.7415 val_acc: 0.9355 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5721 train_acc: 0.9984 val_loss: 0.7411 val_acc: 0.9352 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.5721 train_acc: 0.9983 val_loss: 0.7412 val_acc: 0.9355 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.5715 train_acc: 0.9985 val_loss: 0.7444 val_acc: 0.9347 lr: 0.0003125\n",
      "Epoch  42 / 200 train_loss: 0.5712 train_acc: 0.9986 val_loss: 0.7462 val_acc: 0.9336 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5706 train_acc: 0.9987 val_loss: 0.7455 val_acc: 0.9336 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.5702 train_acc: 0.9988 val_loss: 0.7430 val_acc: 0.9343 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.5700 train_acc: 0.9989 val_loss: 0.7518 val_acc: 0.9323 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.5699 train_acc: 0.9987 val_loss: 0.7424 val_acc: 0.9353 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.5696 train_acc: 0.9989 val_loss: 0.7425 val_acc: 0.9346 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5694 train_acc: 0.9989 val_loss: 0.7434 val_acc: 0.9352 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5693 train_acc: 0.9988 val_loss: 0.7423 val_acc: 0.9354 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.5693 train_acc: 0.9989 val_loss: 0.7430 val_acc: 0.9348 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5690 train_acc: 0.9989 val_loss: 0.7430 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5689 train_acc: 0.9990 val_loss: 0.7430 val_acc: 0.9355 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5689 train_acc: 0.9990 val_loss: 0.7428 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5688 train_acc: 0.9989 val_loss: 0.7428 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5688 train_acc: 0.9990 val_loss: 0.7436 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5687 train_acc: 0.9990 val_loss: 0.7438 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5687 train_acc: 0.9989 val_loss: 0.7420 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5686 train_acc: 0.9990 val_loss: 0.7436 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5685 train_acc: 0.9991 val_loss: 0.7432 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5685 train_acc: 0.9990 val_loss: 0.7438 val_acc: 0.9348 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5685 train_acc: 0.9990 val_loss: 0.7443 val_acc: 0.9349 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5687 train_acc: 0.9989 val_loss: 0.7461 val_acc: 0.9342 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5683 train_acc: 0.9991 val_loss: 0.7437 val_acc: 0.9349 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5685 train_acc: 0.9990 val_loss: 0.7450 val_acc: 0.9349 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5683 train_acc: 0.9991 val_loss: 0.7458 val_acc: 0.9342 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5682 train_acc: 0.9991 val_loss: 0.7467 val_acc: 0.9339 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5682 train_acc: 0.9991 val_loss: 0.7445 val_acc: 0.9346 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5682 train_acc: 0.9991 val_loss: 0.7447 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5682 train_acc: 0.9991 val_loss: 0.7436 val_acc: 0.9355 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5681 train_acc: 0.9990 val_loss: 0.7448 val_acc: 0.9348 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5681 train_acc: 0.9991 val_loss: 0.7445 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5680 train_acc: 0.9992 val_loss: 0.7459 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5679 train_acc: 0.9991 val_loss: 0.7445 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5680 train_acc: 0.9991 val_loss: 0.7442 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5678 train_acc: 0.9991 val_loss: 0.7446 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5679 train_acc: 0.9990 val_loss: 0.7455 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.5678 train_acc: 0.9991 val_loss: 0.7453 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.5678 train_acc: 0.9991 val_loss: 0.7443 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.5677 train_acc: 0.9992 val_loss: 0.7451 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.5677 train_acc: 0.9991 val_loss: 0.7450 val_acc: 0.9353 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.5677 train_acc: 0.9991 val_loss: 0.7457 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.5676 train_acc: 0.9992 val_loss: 0.7457 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.5675 train_acc: 0.9992 val_loss: 0.7458 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.5675 train_acc: 0.9992 val_loss: 0.7456 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.5676 train_acc: 0.9992 val_loss: 0.7468 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.5673 train_acc: 0.9992 val_loss: 0.7456 val_acc: 0.9350 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.5675 train_acc: 0.9992 val_loss: 0.7452 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.5674 train_acc: 0.9992 val_loss: 0.7459 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  89 / 200 train_loss: 0.5673 train_acc: 0.9991 val_loss: 0.7459 val_acc: 0.9346 lr: 5e-05\n",
      "Epoch  90 / 200 train_loss: 0.5673 train_acc: 0.9992 val_loss: 0.7463 val_acc: 0.9352 lr: 5e-05\n",
      "Epoch  91 / 200 train_loss: 0.5672 train_acc: 0.9992 val_loss: 0.7459 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  92 / 200 train_loss: 0.5671 train_acc: 0.9993 val_loss: 0.7478 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  93 / 200 train_loss: 0.5673 train_acc: 0.9991 val_loss: 0.7463 val_acc: 0.9345 lr: 5e-05\n",
      "Epoch  94 / 200 train_loss: 0.5672 train_acc: 0.9992 val_loss: 0.7457 val_acc: 0.9349 lr: 5e-05\n",
      "Epoch  95 / 200 train_loss: 0.5670 train_acc: 0.9993 val_loss: 0.7469 val_acc: 0.9343 lr: 5e-05\n",
      "Epoch  96 / 200 train_loss: 0.5669 train_acc: 0.9993 val_loss: 0.7463 val_acc: 0.9349 lr: 5e-05\n",
      "Epoch  97 / 200 train_loss: 0.5670 train_acc: 0.9992 val_loss: 0.7461 val_acc: 0.9347 lr: 5e-05\n",
      "Epoch  98 / 200 train_loss: 0.5670 train_acc: 0.9992 val_loss: 0.7463 val_acc: 0.9351 lr: 5e-05\n",
      "Epoch  99 / 200 train_loss: 0.5670 train_acc: 0.9992 val_loss: 0.7471 val_acc: 0.9347 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9342\n",
      "25000: 0.9342222222222222\n",
      " test_acc: 0.9342\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.2665 train_acc: 0.7173 val_loss: 1.2831 val_acc: 0.7106 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 0.9650 train_acc: 0.8464 val_loss: 1.2790 val_acc: 0.7119 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.8953 train_acc: 0.8740 val_loss: 1.3176 val_acc: 0.7004 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8513 train_acc: 0.8913 val_loss: 1.7406 val_acc: 0.5684 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8196 train_acc: 0.9039 val_loss: 1.1033 val_acc: 0.7805 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.7943 train_acc: 0.9144 val_loss: 0.9461 val_acc: 0.8511 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.7750 train_acc: 0.9211 val_loss: 1.9626 val_acc: 0.4686 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7580 train_acc: 0.9280 val_loss: 1.1751 val_acc: 0.7622 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7432 train_acc: 0.9344 val_loss: 0.8329 val_acc: 0.8951 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7299 train_acc: 0.9393 val_loss: 1.2609 val_acc: 0.7291 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7181 train_acc: 0.9439 val_loss: 1.9777 val_acc: 0.5388 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7087 train_acc: 0.9480 val_loss: 1.4095 val_acc: 0.6932 lr: 0.005\n",
      "Epoch  12 / 200 train_loss: 0.6989 train_acc: 0.9516 val_loss: 0.8138 val_acc: 0.9051 lr: 0.005\n",
      "Epoch  13 / 200 train_loss: 0.6888 train_acc: 0.9560 val_loss: 0.9685 val_acc: 0.8411 lr: 0.005\n",
      "Epoch  14 / 200 train_loss: 0.6827 train_acc: 0.9583 val_loss: 1.2829 val_acc: 0.7364 lr: 0.005\n",
      "Epoch  15 / 200 train_loss: 0.6765 train_acc: 0.9608 val_loss: 1.3598 val_acc: 0.7125 lr: 0.005\n",
      "Epoch  16 / 200 train_loss: 0.6699 train_acc: 0.9634 val_loss: 1.2092 val_acc: 0.7700 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6366 train_acc: 0.9776 val_loss: 0.8367 val_acc: 0.8970 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.6277 train_acc: 0.9813 val_loss: 1.1476 val_acc: 0.7913 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.6227 train_acc: 0.9834 val_loss: 0.7782 val_acc: 0.9185 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6178 train_acc: 0.9853 val_loss: 1.2537 val_acc: 0.7608 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6144 train_acc: 0.9865 val_loss: 1.0551 val_acc: 0.8314 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6110 train_acc: 0.9876 val_loss: 1.1723 val_acc: 0.7805 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.6088 train_acc: 0.9883 val_loss: 1.6662 val_acc: 0.6248 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.5962 train_acc: 0.9930 val_loss: 1.0831 val_acc: 0.8199 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.5929 train_acc: 0.9942 val_loss: 0.7737 val_acc: 0.9213 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.5915 train_acc: 0.9944 val_loss: 0.8069 val_acc: 0.9089 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.5893 train_acc: 0.9951 val_loss: 0.7916 val_acc: 0.9166 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.5879 train_acc: 0.9953 val_loss: 0.7797 val_acc: 0.9206 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.5864 train_acc: 0.9958 val_loss: 0.9053 val_acc: 0.8769 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5820 train_acc: 0.9972 val_loss: 0.8268 val_acc: 0.9035 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5808 train_acc: 0.9973 val_loss: 0.7709 val_acc: 0.9230 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5801 train_acc: 0.9975 val_loss: 0.7582 val_acc: 0.9270 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5793 train_acc: 0.9975 val_loss: 0.7546 val_acc: 0.9295 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.5781 train_acc: 0.9980 val_loss: 0.7932 val_acc: 0.9150 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.5777 train_acc: 0.9979 val_loss: 0.7545 val_acc: 0.9281 lr: 0.000625\n",
      "Epoch  36 / 200 train_loss: 0.5772 train_acc: 0.9980 val_loss: 0.7562 val_acc: 0.9279 lr: 0.000625\n",
      "Epoch  37 / 200 train_loss: 0.5765 train_acc: 0.9982 val_loss: 0.7558 val_acc: 0.9277 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5750 train_acc: 0.9984 val_loss: 0.7677 val_acc: 0.9230 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5745 train_acc: 0.9985 val_loss: 0.7550 val_acc: 0.9290 lr: 0.0003125\n",
      "Epoch  40 / 200 train_loss: 0.5739 train_acc: 0.9986 val_loss: 0.7669 val_acc: 0.9242 lr: 0.0003125\n",
      "Epoch  41 / 200 train_loss: 0.5737 train_acc: 0.9987 val_loss: 0.7609 val_acc: 0.9264 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5729 train_acc: 0.9988 val_loss: 0.7584 val_acc: 0.9273 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5728 train_acc: 0.9988 val_loss: 0.7567 val_acc: 0.9279 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.5723 train_acc: 0.9989 val_loss: 0.7557 val_acc: 0.9282 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.5722 train_acc: 0.9989 val_loss: 0.7544 val_acc: 0.9287 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.5720 train_acc: 0.9989 val_loss: 0.7549 val_acc: 0.9288 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.5718 train_acc: 0.9990 val_loss: 0.7563 val_acc: 0.9283 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5715 train_acc: 0.9991 val_loss: 0.7553 val_acc: 0.9288 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5714 train_acc: 0.9990 val_loss: 0.7549 val_acc: 0.9289 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.5714 train_acc: 0.9990 val_loss: 0.7561 val_acc: 0.9285 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5716 train_acc: 0.9990 val_loss: 0.7552 val_acc: 0.9291 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5713 train_acc: 0.9991 val_loss: 0.7545 val_acc: 0.9289 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5712 train_acc: 0.9991 val_loss: 0.7553 val_acc: 0.9282 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5713 train_acc: 0.9990 val_loss: 0.7553 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5710 train_acc: 0.9991 val_loss: 0.7554 val_acc: 0.9290 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5709 train_acc: 0.9991 val_loss: 0.7552 val_acc: 0.9287 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5710 train_acc: 0.9991 val_loss: 0.7551 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5709 train_acc: 0.9992 val_loss: 0.7551 val_acc: 0.9286 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5710 train_acc: 0.9990 val_loss: 0.7569 val_acc: 0.9285 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5709 train_acc: 0.9991 val_loss: 0.7560 val_acc: 0.9287 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5709 train_acc: 0.9991 val_loss: 0.7553 val_acc: 0.9285 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5708 train_acc: 0.9991 val_loss: 0.7554 val_acc: 0.9287 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5706 train_acc: 0.9992 val_loss: 0.7561 val_acc: 0.9284 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9303\n",
      "20000: 0.9302888888888889\n",
      " test_acc: 0.9303\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.3922 train_acc: 0.6625 val_loss: 1.2629 val_acc: 0.7140 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0201 train_acc: 0.8232 val_loss: 1.2518 val_acc: 0.7216 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9382 train_acc: 0.8565 val_loss: 1.3625 val_acc: 0.6878 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.8886 train_acc: 0.8768 val_loss: 0.9091 val_acc: 0.8628 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8534 train_acc: 0.8905 val_loss: 0.9310 val_acc: 0.8563 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8242 train_acc: 0.9029 val_loss: 0.9599 val_acc: 0.8401 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.8015 train_acc: 0.9114 val_loss: 1.5503 val_acc: 0.6055 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.7808 train_acc: 0.9203 val_loss: 2.5612 val_acc: 0.3888 lr: 0.0025\n",
      "Epoch   8 / 200 train_loss: 0.7286 train_acc: 0.9419 val_loss: 1.2317 val_acc: 0.7619 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.7093 train_acc: 0.9506 val_loss: 0.8045 val_acc: 0.9069 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.6958 train_acc: 0.9561 val_loss: 1.1201 val_acc: 0.7899 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.6851 train_acc: 0.9605 val_loss: 1.5341 val_acc: 0.6647 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6754 train_acc: 0.9646 val_loss: 0.7908 val_acc: 0.9138 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6661 train_acc: 0.9686 val_loss: 1.2856 val_acc: 0.7418 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6584 train_acc: 0.9719 val_loss: 1.0698 val_acc: 0.8134 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6528 train_acc: 0.9735 val_loss: 1.0426 val_acc: 0.8221 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6463 train_acc: 0.9767 val_loss: 1.0597 val_acc: 0.8208 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.6251 train_acc: 0.9853 val_loss: 0.7865 val_acc: 0.9150 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.6190 train_acc: 0.9874 val_loss: 0.7736 val_acc: 0.9191 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.6153 train_acc: 0.9888 val_loss: 0.9115 val_acc: 0.8706 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.6117 train_acc: 0.9901 val_loss: 0.7840 val_acc: 0.9168 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.6091 train_acc: 0.9907 val_loss: 0.8957 val_acc: 0.8786 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.6063 train_acc: 0.9919 val_loss: 0.8072 val_acc: 0.9077 lr: 0.000625\n",
      "Epoch  23 / 200 train_loss: 0.5992 train_acc: 0.9940 val_loss: 0.8308 val_acc: 0.8989 lr: 0.000625\n",
      "Epoch  24 / 200 train_loss: 0.5964 train_acc: 0.9948 val_loss: 0.8989 val_acc: 0.8737 lr: 0.000625\n",
      "Epoch  25 / 200 train_loss: 0.5947 train_acc: 0.9953 val_loss: 0.8851 val_acc: 0.8798 lr: 0.000625\n",
      "Epoch  26 / 200 train_loss: 0.5934 train_acc: 0.9957 val_loss: 0.7967 val_acc: 0.9122 lr: 0.0003125\n",
      "Epoch  27 / 200 train_loss: 0.5902 train_acc: 0.9964 val_loss: 0.7913 val_acc: 0.9147 lr: 0.0003125\n",
      "Epoch  28 / 200 train_loss: 0.5891 train_acc: 0.9966 val_loss: 0.7628 val_acc: 0.9237 lr: 0.0003125\n",
      "Epoch  29 / 200 train_loss: 0.5882 train_acc: 0.9971 val_loss: 0.9102 val_acc: 0.8717 lr: 0.0003125\n",
      "Epoch  30 / 200 train_loss: 0.5876 train_acc: 0.9971 val_loss: 0.7914 val_acc: 0.9153 lr: 0.0003125\n",
      "Epoch  31 / 200 train_loss: 0.5865 train_acc: 0.9972 val_loss: 0.7988 val_acc: 0.9098 lr: 0.0003125\n",
      "Epoch  32 / 200 train_loss: 0.5861 train_acc: 0.9976 val_loss: 0.8298 val_acc: 0.9026 lr: 0.00015625\n",
      "Epoch  33 / 200 train_loss: 0.5848 train_acc: 0.9977 val_loss: 0.7683 val_acc: 0.9226 lr: 0.00015625\n",
      "Epoch  34 / 200 train_loss: 0.5843 train_acc: 0.9978 val_loss: 0.7681 val_acc: 0.9219 lr: 0.00015625\n",
      "Epoch  35 / 200 train_loss: 0.5838 train_acc: 0.9978 val_loss: 0.7633 val_acc: 0.9245 lr: 0.00015625\n",
      "Epoch  36 / 200 train_loss: 0.5836 train_acc: 0.9980 val_loss: 0.7823 val_acc: 0.9180 lr: 0.00015625\n",
      "Epoch  37 / 200 train_loss: 0.5830 train_acc: 0.9981 val_loss: 0.7658 val_acc: 0.9234 lr: 0.00015625\n",
      "Epoch  38 / 200 train_loss: 0.5827 train_acc: 0.9980 val_loss: 0.7655 val_acc: 0.9233 lr: 0.00015625\n",
      "Epoch  39 / 200 train_loss: 0.5824 train_acc: 0.9981 val_loss: 0.7761 val_acc: 0.9204 lr: 7.8125e-05\n",
      "Epoch  40 / 200 train_loss: 0.5819 train_acc: 0.9982 val_loss: 0.7654 val_acc: 0.9234 lr: 7.8125e-05\n",
      "Epoch  41 / 200 train_loss: 0.5818 train_acc: 0.9981 val_loss: 0.7727 val_acc: 0.9212 lr: 7.8125e-05\n",
      "Epoch  42 / 200 train_loss: 0.5815 train_acc: 0.9982 val_loss: 0.7686 val_acc: 0.9221 lr: 7.8125e-05\n",
      "Epoch  43 / 200 train_loss: 0.5815 train_acc: 0.9984 val_loss: 0.7686 val_acc: 0.9218 lr: 5e-05\n",
      "Epoch  44 / 200 train_loss: 0.5814 train_acc: 0.9982 val_loss: 0.7657 val_acc: 0.9230 lr: 5e-05\n",
      "Epoch  45 / 200 train_loss: 0.5810 train_acc: 0.9984 val_loss: 0.7647 val_acc: 0.9237 lr: 5e-05\n",
      "Epoch  46 / 200 train_loss: 0.5808 train_acc: 0.9983 val_loss: 0.7702 val_acc: 0.9222 lr: 5e-05\n",
      "Epoch  47 / 200 train_loss: 0.5809 train_acc: 0.9983 val_loss: 0.7652 val_acc: 0.9233 lr: 5e-05\n",
      "Epoch  48 / 200 train_loss: 0.5808 train_acc: 0.9983 val_loss: 0.7642 val_acc: 0.9239 lr: 5e-05\n",
      "Epoch  49 / 200 train_loss: 0.5806 train_acc: 0.9984 val_loss: 0.7650 val_acc: 0.9240 lr: 5e-05\n",
      "Epoch  50 / 200 train_loss: 0.5805 train_acc: 0.9984 val_loss: 0.7647 val_acc: 0.9232 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5802 train_acc: 0.9985 val_loss: 0.7640 val_acc: 0.9239 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5805 train_acc: 0.9984 val_loss: 0.7720 val_acc: 0.9211 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5803 train_acc: 0.9984 val_loss: 0.7716 val_acc: 0.9223 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5802 train_acc: 0.9985 val_loss: 0.7839 val_acc: 0.9181 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5801 train_acc: 0.9984 val_loss: 0.7690 val_acc: 0.9226 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5801 train_acc: 0.9984 val_loss: 0.7665 val_acc: 0.9228 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5799 train_acc: 0.9984 val_loss: 0.7654 val_acc: 0.9232 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5800 train_acc: 0.9983 val_loss: 0.7672 val_acc: 0.9228 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5798 train_acc: 0.9985 val_loss: 0.7684 val_acc: 0.9227 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5798 train_acc: 0.9986 val_loss: 0.7694 val_acc: 0.9219 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5795 train_acc: 0.9986 val_loss: 0.7684 val_acc: 0.9233 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5793 train_acc: 0.9987 val_loss: 0.7669 val_acc: 0.9235 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5795 train_acc: 0.9985 val_loss: 0.7650 val_acc: 0.9238 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5794 train_acc: 0.9985 val_loss: 0.7669 val_acc: 0.9227 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5795 train_acc: 0.9985 val_loss: 0.7654 val_acc: 0.9230 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9231\n",
      "15000: 0.9230777777777778\n",
      " test_acc: 0.9231\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.4504 train_acc: 0.6413 val_loss: 1.3037 val_acc: 0.7020 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.0901 train_acc: 0.7933 val_loss: 1.4247 val_acc: 0.6647 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 0.9916 train_acc: 0.8353 val_loss: 1.6649 val_acc: 0.5774 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 0.9343 train_acc: 0.8601 val_loss: 1.0211 val_acc: 0.8224 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 0.8941 train_acc: 0.8760 val_loss: 1.0715 val_acc: 0.7893 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.8607 train_acc: 0.8896 val_loss: 1.1780 val_acc: 0.7553 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.8360 train_acc: 0.8995 val_loss: 1.4934 val_acc: 0.6456 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.8106 train_acc: 0.9102 val_loss: 0.9310 val_acc: 0.8533 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.7891 train_acc: 0.9186 val_loss: 1.3975 val_acc: 0.6863 lr: 0.005\n",
      "Epoch   9 / 200 train_loss: 0.7700 train_acc: 0.9273 val_loss: 1.6286 val_acc: 0.6268 lr: 0.005\n",
      "Epoch  10 / 200 train_loss: 0.7526 train_acc: 0.9336 val_loss: 1.1387 val_acc: 0.7694 lr: 0.005\n",
      "Epoch  11 / 200 train_loss: 0.7384 train_acc: 0.9401 val_loss: 1.3264 val_acc: 0.7104 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.6881 train_acc: 0.9620 val_loss: 0.8214 val_acc: 0.9024 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.6719 train_acc: 0.9685 val_loss: 0.8755 val_acc: 0.8809 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6627 train_acc: 0.9726 val_loss: 1.2486 val_acc: 0.7583 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6552 train_acc: 0.9759 val_loss: 1.3381 val_acc: 0.7316 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6466 train_acc: 0.9795 val_loss: 1.1303 val_acc: 0.7908 lr: 0.00125\n",
      "Epoch  17 / 200 train_loss: 0.6283 train_acc: 0.9862 val_loss: 0.8068 val_acc: 0.9064 lr: 0.00125\n",
      "Epoch  18 / 200 train_loss: 0.6211 train_acc: 0.9889 val_loss: 0.8329 val_acc: 0.8960 lr: 0.00125\n",
      "Epoch  19 / 200 train_loss: 0.6175 train_acc: 0.9904 val_loss: 0.8511 val_acc: 0.8904 lr: 0.00125\n",
      "Epoch  20 / 200 train_loss: 0.6135 train_acc: 0.9916 val_loss: 0.9734 val_acc: 0.8445 lr: 0.00125\n",
      "Epoch  21 / 200 train_loss: 0.6104 train_acc: 0.9924 val_loss: 0.8091 val_acc: 0.9090 lr: 0.00125\n",
      "Epoch  22 / 200 train_loss: 0.6083 train_acc: 0.9930 val_loss: 0.8274 val_acc: 0.9005 lr: 0.00125\n",
      "Epoch  23 / 200 train_loss: 0.6054 train_acc: 0.9940 val_loss: 0.8327 val_acc: 0.8970 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.6038 train_acc: 0.9942 val_loss: 0.8180 val_acc: 0.9047 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.6017 train_acc: 0.9946 val_loss: 0.7930 val_acc: 0.9157 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.5992 train_acc: 0.9955 val_loss: 0.8077 val_acc: 0.9118 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.5972 train_acc: 0.9958 val_loss: 1.1628 val_acc: 0.7866 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.5958 train_acc: 0.9960 val_loss: 0.9705 val_acc: 0.8442 lr: 0.00125\n",
      "Epoch  29 / 200 train_loss: 0.5945 train_acc: 0.9962 val_loss: 0.9348 val_acc: 0.8638 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5890 train_acc: 0.9978 val_loss: 0.8498 val_acc: 0.8943 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5868 train_acc: 0.9979 val_loss: 0.7866 val_acc: 0.9189 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5857 train_acc: 0.9981 val_loss: 0.7834 val_acc: 0.9182 lr: 0.000625\n",
      "Epoch  33 / 200 train_loss: 0.5849 train_acc: 0.9981 val_loss: 0.7860 val_acc: 0.9179 lr: 0.000625\n",
      "Epoch  34 / 200 train_loss: 0.5841 train_acc: 0.9984 val_loss: 0.9695 val_acc: 0.8519 lr: 0.000625\n",
      "Epoch  35 / 200 train_loss: 0.5833 train_acc: 0.9985 val_loss: 0.8119 val_acc: 0.9050 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5812 train_acc: 0.9988 val_loss: 0.8043 val_acc: 0.9105 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5803 train_acc: 0.9990 val_loss: 0.7819 val_acc: 0.9183 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5797 train_acc: 0.9990 val_loss: 0.7874 val_acc: 0.9166 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5792 train_acc: 0.9991 val_loss: 0.7970 val_acc: 0.9152 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.5784 train_acc: 0.9991 val_loss: 0.7830 val_acc: 0.9182 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5780 train_acc: 0.9992 val_loss: 0.7839 val_acc: 0.9177 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5779 train_acc: 0.9991 val_loss: 0.7838 val_acc: 0.9168 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5774 train_acc: 0.9992 val_loss: 0.7845 val_acc: 0.9175 lr: 7.8125e-05\n",
      "Epoch  44 / 200 train_loss: 0.5770 train_acc: 0.9993 val_loss: 0.7813 val_acc: 0.9178 lr: 7.8125e-05\n",
      "Epoch  45 / 200 train_loss: 0.5767 train_acc: 0.9993 val_loss: 0.7864 val_acc: 0.9163 lr: 7.8125e-05\n",
      "Epoch  46 / 200 train_loss: 0.5768 train_acc: 0.9993 val_loss: 0.7805 val_acc: 0.9193 lr: 7.8125e-05\n",
      "Epoch  47 / 200 train_loss: 0.5766 train_acc: 0.9993 val_loss: 0.7808 val_acc: 0.9186 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5765 train_acc: 0.9992 val_loss: 0.7827 val_acc: 0.9180 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5762 train_acc: 0.9993 val_loss: 0.7816 val_acc: 0.9178 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.5761 train_acc: 0.9994 val_loss: 0.7810 val_acc: 0.9185 lr: 5e-05\n",
      "Epoch  51 / 200 train_loss: 0.5760 train_acc: 0.9994 val_loss: 0.7810 val_acc: 0.9189 lr: 5e-05\n",
      "Epoch  52 / 200 train_loss: 0.5760 train_acc: 0.9994 val_loss: 0.7806 val_acc: 0.9186 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5760 train_acc: 0.9994 val_loss: 0.7823 val_acc: 0.9185 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5757 train_acc: 0.9994 val_loss: 0.7834 val_acc: 0.9182 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5758 train_acc: 0.9994 val_loss: 0.7832 val_acc: 0.9177 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5758 train_acc: 0.9993 val_loss: 0.7808 val_acc: 0.9182 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5756 train_acc: 0.9993 val_loss: 0.7876 val_acc: 0.9174 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5754 train_acc: 0.9994 val_loss: 0.7864 val_acc: 0.9174 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5753 train_acc: 0.9995 val_loss: 0.7817 val_acc: 0.9181 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5754 train_acc: 0.9993 val_loss: 0.7832 val_acc: 0.9191 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5751 train_acc: 0.9994 val_loss: 0.7808 val_acc: 0.9190 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5753 train_acc: 0.9993 val_loss: 0.7815 val_acc: 0.9179 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5749 train_acc: 0.9995 val_loss: 0.7850 val_acc: 0.9178 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5752 train_acc: 0.9994 val_loss: 0.7822 val_acc: 0.9183 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5750 train_acc: 0.9995 val_loss: 0.7820 val_acc: 0.9182 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5747 train_acc: 0.9995 val_loss: 0.7834 val_acc: 0.9171 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5747 train_acc: 0.9996 val_loss: 0.7845 val_acc: 0.9178 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5748 train_acc: 0.9995 val_loss: 0.7812 val_acc: 0.9179 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5748 train_acc: 0.9995 val_loss: 0.7813 val_acc: 0.9184 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5747 train_acc: 0.9994 val_loss: 0.7868 val_acc: 0.9170 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5746 train_acc: 0.9994 val_loss: 0.7828 val_acc: 0.9182 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5746 train_acc: 0.9995 val_loss: 0.7833 val_acc: 0.9181 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5744 train_acc: 0.9996 val_loss: 0.7825 val_acc: 0.9185 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5743 train_acc: 0.9996 val_loss: 0.7835 val_acc: 0.9177 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5745 train_acc: 0.9995 val_loss: 0.7818 val_acc: 0.9180 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5742 train_acc: 0.9995 val_loss: 0.7814 val_acc: 0.9189 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.9166\n",
      "10000: 0.9165666666666666\n",
      " test_acc: 0.9166\n",
      "Model CNN1DL1000 has total parameter number: 6.21 M\n",
      "Epoch   0 / 200 train_loss: 1.7212 train_acc: 0.5191 val_loss: 1.5869 val_acc: 0.5584 lr: 0.005\n",
      "Epoch   1 / 200 train_loss: 1.2735 train_acc: 0.7128 val_loss: 1.3009 val_acc: 0.7066 lr: 0.005\n",
      "Epoch   2 / 200 train_loss: 1.1459 train_acc: 0.7718 val_loss: 1.3624 val_acc: 0.6868 lr: 0.005\n",
      "Epoch   3 / 200 train_loss: 1.0593 train_acc: 0.8087 val_loss: 1.5533 val_acc: 0.6227 lr: 0.005\n",
      "Epoch   4 / 200 train_loss: 1.0028 train_acc: 0.8319 val_loss: 1.0001 val_acc: 0.8295 lr: 0.005\n",
      "Epoch   5 / 200 train_loss: 0.9554 train_acc: 0.8519 val_loss: 1.2273 val_acc: 0.7315 lr: 0.005\n",
      "Epoch   6 / 200 train_loss: 0.9177 train_acc: 0.8670 val_loss: 1.6542 val_acc: 0.5970 lr: 0.005\n",
      "Epoch   7 / 200 train_loss: 0.8822 train_acc: 0.8843 val_loss: 1.0158 val_acc: 0.8212 lr: 0.005\n",
      "Epoch   8 / 200 train_loss: 0.8525 train_acc: 0.8951 val_loss: 1.4766 val_acc: 0.6421 lr: 0.0025\n",
      "Epoch   9 / 200 train_loss: 0.7793 train_acc: 0.9280 val_loss: 1.1679 val_acc: 0.7712 lr: 0.0025\n",
      "Epoch  10 / 200 train_loss: 0.7518 train_acc: 0.9401 val_loss: 1.2255 val_acc: 0.7443 lr: 0.0025\n",
      "Epoch  11 / 200 train_loss: 0.7374 train_acc: 0.9475 val_loss: 0.9810 val_acc: 0.8380 lr: 0.0025\n",
      "Epoch  12 / 200 train_loss: 0.7186 train_acc: 0.9555 val_loss: 1.1013 val_acc: 0.7907 lr: 0.0025\n",
      "Epoch  13 / 200 train_loss: 0.7047 train_acc: 0.9614 val_loss: 1.3208 val_acc: 0.7210 lr: 0.0025\n",
      "Epoch  14 / 200 train_loss: 0.6907 train_acc: 0.9678 val_loss: 0.9343 val_acc: 0.8636 lr: 0.0025\n",
      "Epoch  15 / 200 train_loss: 0.6807 train_acc: 0.9712 val_loss: 0.9133 val_acc: 0.8717 lr: 0.0025\n",
      "Epoch  16 / 200 train_loss: 0.6712 train_acc: 0.9754 val_loss: 1.3672 val_acc: 0.7118 lr: 0.0025\n",
      "Epoch  17 / 200 train_loss: 0.6617 train_acc: 0.9793 val_loss: 1.6086 val_acc: 0.6399 lr: 0.0025\n",
      "Epoch  18 / 200 train_loss: 0.6539 train_acc: 0.9823 val_loss: 1.1251 val_acc: 0.7941 lr: 0.0025\n",
      "Epoch  19 / 200 train_loss: 0.6467 train_acc: 0.9848 val_loss: 0.8810 val_acc: 0.8819 lr: 0.0025\n",
      "Epoch  20 / 200 train_loss: 0.6395 train_acc: 0.9874 val_loss: 1.0512 val_acc: 0.8214 lr: 0.0025\n",
      "Epoch  21 / 200 train_loss: 0.6366 train_acc: 0.9884 val_loss: 1.8862 val_acc: 0.5823 lr: 0.0025\n",
      "Epoch  22 / 200 train_loss: 0.6299 train_acc: 0.9904 val_loss: 1.0544 val_acc: 0.8166 lr: 0.0025\n",
      "Epoch  23 / 200 train_loss: 0.6285 train_acc: 0.9904 val_loss: 1.0741 val_acc: 0.8075 lr: 0.00125\n",
      "Epoch  24 / 200 train_loss: 0.6113 train_acc: 0.9956 val_loss: 0.8500 val_acc: 0.8926 lr: 0.00125\n",
      "Epoch  25 / 200 train_loss: 0.6046 train_acc: 0.9970 val_loss: 0.9421 val_acc: 0.8554 lr: 0.00125\n",
      "Epoch  26 / 200 train_loss: 0.6019 train_acc: 0.9976 val_loss: 0.8934 val_acc: 0.8772 lr: 0.00125\n",
      "Epoch  27 / 200 train_loss: 0.5998 train_acc: 0.9977 val_loss: 0.8858 val_acc: 0.8798 lr: 0.00125\n",
      "Epoch  28 / 200 train_loss: 0.5978 train_acc: 0.9978 val_loss: 0.8838 val_acc: 0.8807 lr: 0.000625\n",
      "Epoch  29 / 200 train_loss: 0.5928 train_acc: 0.9988 val_loss: 0.9348 val_acc: 0.8636 lr: 0.000625\n",
      "Epoch  30 / 200 train_loss: 0.5912 train_acc: 0.9991 val_loss: 1.0425 val_acc: 0.8187 lr: 0.000625\n",
      "Epoch  31 / 200 train_loss: 0.5891 train_acc: 0.9991 val_loss: 1.0545 val_acc: 0.8204 lr: 0.000625\n",
      "Epoch  32 / 200 train_loss: 0.5891 train_acc: 0.9991 val_loss: 0.8474 val_acc: 0.8914 lr: 0.0003125\n",
      "Epoch  33 / 200 train_loss: 0.5863 train_acc: 0.9993 val_loss: 0.8419 val_acc: 0.8932 lr: 0.0003125\n",
      "Epoch  34 / 200 train_loss: 0.5854 train_acc: 0.9995 val_loss: 0.8602 val_acc: 0.8897 lr: 0.0003125\n",
      "Epoch  35 / 200 train_loss: 0.5849 train_acc: 0.9994 val_loss: 0.8385 val_acc: 0.8967 lr: 0.0003125\n",
      "Epoch  36 / 200 train_loss: 0.5841 train_acc: 0.9995 val_loss: 0.8466 val_acc: 0.8918 lr: 0.0003125\n",
      "Epoch  37 / 200 train_loss: 0.5835 train_acc: 0.9996 val_loss: 0.8482 val_acc: 0.8921 lr: 0.0003125\n",
      "Epoch  38 / 200 train_loss: 0.5831 train_acc: 0.9996 val_loss: 0.8684 val_acc: 0.8845 lr: 0.0003125\n",
      "Epoch  39 / 200 train_loss: 0.5829 train_acc: 0.9997 val_loss: 1.0279 val_acc: 0.8253 lr: 0.00015625\n",
      "Epoch  40 / 200 train_loss: 0.5818 train_acc: 0.9996 val_loss: 0.8413 val_acc: 0.8946 lr: 0.00015625\n",
      "Epoch  41 / 200 train_loss: 0.5815 train_acc: 0.9998 val_loss: 0.8520 val_acc: 0.8910 lr: 0.00015625\n",
      "Epoch  42 / 200 train_loss: 0.5810 train_acc: 0.9997 val_loss: 0.8580 val_acc: 0.8884 lr: 0.00015625\n",
      "Epoch  43 / 200 train_loss: 0.5808 train_acc: 0.9997 val_loss: 0.8417 val_acc: 0.8974 lr: 0.00015625\n",
      "Epoch  44 / 200 train_loss: 0.5808 train_acc: 0.9996 val_loss: 0.8704 val_acc: 0.8836 lr: 0.00015625\n",
      "Epoch  45 / 200 train_loss: 0.5799 train_acc: 0.9999 val_loss: 0.8416 val_acc: 0.8953 lr: 0.00015625\n",
      "Epoch  46 / 200 train_loss: 0.5801 train_acc: 0.9999 val_loss: 0.8407 val_acc: 0.8973 lr: 0.00015625\n",
      "Epoch  47 / 200 train_loss: 0.5801 train_acc: 0.9997 val_loss: 0.8384 val_acc: 0.8965 lr: 7.8125e-05\n",
      "Epoch  48 / 200 train_loss: 0.5794 train_acc: 0.9998 val_loss: 0.8379 val_acc: 0.8979 lr: 7.8125e-05\n",
      "Epoch  49 / 200 train_loss: 0.5790 train_acc: 0.9998 val_loss: 0.8381 val_acc: 0.8964 lr: 7.8125e-05\n",
      "Epoch  50 / 200 train_loss: 0.5792 train_acc: 0.9997 val_loss: 0.8393 val_acc: 0.8978 lr: 7.8125e-05\n",
      "Epoch  51 / 200 train_loss: 0.5791 train_acc: 0.9998 val_loss: 0.8390 val_acc: 0.8976 lr: 7.8125e-05\n",
      "Epoch  52 / 200 train_loss: 0.5786 train_acc: 0.9998 val_loss: 0.8386 val_acc: 0.8952 lr: 5e-05\n",
      "Epoch  53 / 200 train_loss: 0.5783 train_acc: 0.9999 val_loss: 0.8382 val_acc: 0.8966 lr: 5e-05\n",
      "Epoch  54 / 200 train_loss: 0.5784 train_acc: 0.9998 val_loss: 0.8394 val_acc: 0.8971 lr: 5e-05\n",
      "Epoch  55 / 200 train_loss: 0.5782 train_acc: 0.9998 val_loss: 0.8367 val_acc: 0.8971 lr: 5e-05\n",
      "Epoch  56 / 200 train_loss: 0.5782 train_acc: 0.9998 val_loss: 0.8429 val_acc: 0.8939 lr: 5e-05\n",
      "Epoch  57 / 200 train_loss: 0.5783 train_acc: 0.9998 val_loss: 0.8406 val_acc: 0.8971 lr: 5e-05\n",
      "Epoch  58 / 200 train_loss: 0.5779 train_acc: 0.9999 val_loss: 0.8379 val_acc: 0.8987 lr: 5e-05\n",
      "Epoch  59 / 200 train_loss: 0.5780 train_acc: 0.9998 val_loss: 0.8407 val_acc: 0.8945 lr: 5e-05\n",
      "Epoch  60 / 200 train_loss: 0.5779 train_acc: 0.9998 val_loss: 0.8392 val_acc: 0.8950 lr: 5e-05\n",
      "Epoch  61 / 200 train_loss: 0.5781 train_acc: 0.9997 val_loss: 0.8379 val_acc: 0.8972 lr: 5e-05\n",
      "Epoch  62 / 200 train_loss: 0.5782 train_acc: 0.9998 val_loss: 0.8396 val_acc: 0.8944 lr: 5e-05\n",
      "Epoch  63 / 200 train_loss: 0.5778 train_acc: 0.9999 val_loss: 0.8397 val_acc: 0.8971 lr: 5e-05\n",
      "Epoch  64 / 200 train_loss: 0.5775 train_acc: 0.9998 val_loss: 0.8355 val_acc: 0.8982 lr: 5e-05\n",
      "Epoch  65 / 200 train_loss: 0.5777 train_acc: 0.9999 val_loss: 0.8394 val_acc: 0.8966 lr: 5e-05\n",
      "Epoch  66 / 200 train_loss: 0.5776 train_acc: 0.9999 val_loss: 0.8357 val_acc: 0.8966 lr: 5e-05\n",
      "Epoch  67 / 200 train_loss: 0.5778 train_acc: 0.9999 val_loss: 0.8377 val_acc: 0.8981 lr: 5e-05\n",
      "Epoch  68 / 200 train_loss: 0.5774 train_acc: 0.9998 val_loss: 0.8480 val_acc: 0.8921 lr: 5e-05\n",
      "Epoch  69 / 200 train_loss: 0.5776 train_acc: 0.9998 val_loss: 0.8382 val_acc: 0.8957 lr: 5e-05\n",
      "Epoch  70 / 200 train_loss: 0.5772 train_acc: 0.9999 val_loss: 0.8376 val_acc: 0.8979 lr: 5e-05\n",
      "Epoch  71 / 200 train_loss: 0.5772 train_acc: 0.9999 val_loss: 0.8388 val_acc: 0.8963 lr: 5e-05\n",
      "Epoch  72 / 200 train_loss: 0.5770 train_acc: 0.9999 val_loss: 0.8365 val_acc: 0.8971 lr: 5e-05\n",
      "Epoch  73 / 200 train_loss: 0.5773 train_acc: 0.9998 val_loss: 0.8396 val_acc: 0.8974 lr: 5e-05\n",
      "Epoch  74 / 200 train_loss: 0.5772 train_acc: 0.9998 val_loss: 0.8373 val_acc: 0.8973 lr: 5e-05\n",
      "Epoch  75 / 200 train_loss: 0.5772 train_acc: 0.9998 val_loss: 0.8378 val_acc: 0.8969 lr: 5e-05\n",
      "Epoch  76 / 200 train_loss: 0.5769 train_acc: 0.9998 val_loss: 0.8374 val_acc: 0.8978 lr: 5e-05\n",
      "Epoch  77 / 200 train_loss: 0.5768 train_acc: 0.9999 val_loss: 0.8384 val_acc: 0.8973 lr: 5e-05\n",
      "Epoch  78 / 200 train_loss: 0.5770 train_acc: 0.9998 val_loss: 0.8371 val_acc: 0.8967 lr: 5e-05\n",
      "Epoch  79 / 200 train_loss: 0.5772 train_acc: 0.9999 val_loss: 0.8419 val_acc: 0.8929 lr: 5e-05\n",
      "Epoch  80 / 200 train_loss: 0.5766 train_acc: 0.9998 val_loss: 0.8370 val_acc: 0.8969 lr: 5e-05\n",
      "Epoch  81 / 200 train_loss: 0.5766 train_acc: 0.9999 val_loss: 0.8372 val_acc: 0.8967 lr: 5e-05\n",
      "Epoch  82 / 200 train_loss: 0.5764 train_acc: 0.9999 val_loss: 0.8372 val_acc: 0.8976 lr: 5e-05\n",
      "Epoch  83 / 200 train_loss: 0.5766 train_acc: 0.9999 val_loss: 0.8372 val_acc: 0.8977 lr: 5e-05\n",
      "Epoch  84 / 200 train_loss: 0.5768 train_acc: 0.9999 val_loss: 0.8430 val_acc: 0.8932 lr: 5e-05\n",
      "Epoch  85 / 200 train_loss: 0.5765 train_acc: 0.9999 val_loss: 0.8358 val_acc: 0.8972 lr: 5e-05\n",
      "Epoch  86 / 200 train_loss: 0.5763 train_acc: 0.9999 val_loss: 0.8369 val_acc: 0.8974 lr: 5e-05\n",
      "Epoch  87 / 200 train_loss: 0.5762 train_acc: 0.9998 val_loss: 0.8381 val_acc: 0.8974 lr: 5e-05\n",
      "Epoch  88 / 200 train_loss: 0.5762 train_acc: 0.9998 val_loss: 0.8368 val_acc: 0.8986 lr: 5e-05\n",
      "Early stopping triggered.\n",
      " test_acc: 0.8965\n",
      "5000: 0.8964888888888889\n",
      " test_acc: 0.8965\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sample_size</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>0.934222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.930289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.923078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.916567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>0.896489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_sample_size       acc\n",
       "0              25000  0.934222\n",
       "1              20000  0.930289\n",
       "2              15000  0.923078\n",
       "3              10000  0.916567\n",
       "4               5000  0.896489"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 42\n",
    "acc_df = []\n",
    "# train_sample_sizes = [14000, 12000, 10000, 8000, 6000, 4000]\n",
    "train_sample_sizes = [25000, 20000, 15000, 10000, 5000]\n",
    "\n",
    "for train_sample_size in train_sample_sizes:\n",
    "    acc = train_pipeline(train_objs, labels, y_code_dict, all_peps, train_name=f'valid_data_ds_{train_sample_size}', train_sample_size=train_sample_size, seed=1)\n",
    "    acc_df.append([train_sample_size, acc])\n",
    "acc_df =pd.DataFrame(acc_df)\n",
    "acc_df.columns = ['train_sample_size', 'acc']\n",
    "acc_df.to_csv(\"../../../03.results/classification_on_clean_data/hp12/diff_data_size/valid/acc.csv\")\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecacf4-be72-478e-844f-4fc9068197de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npspy_env",
   "language": "python",
   "name": "npspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
